<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Asset Enrichment Agent Roadmap - Onelist Roadmap</title>
  <style>
    :root {
      --bg: #0a0a0a;
      --card-bg: #141414;
      --border: #2a2a2a;
      --text: #e0e0e0;
      --text-muted: #888;
      --accent: #3b82f6;
      --accent-hover: #60a5fa;
      --code-bg: #1a1a1a;
    }
    
    * { box-sizing: border-box; margin: 0; padding: 0; }
    
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: var(--bg);
      color: var(--text);
      line-height: 1.7;
      padding: 2rem;
      max-width: 900px;
      margin: 0 auto;
    }
    
    .back-link {
      display: inline-block;
      margin-bottom: 2rem;
      color: var(--accent);
      text-decoration: none;
    }
    .back-link:hover { color: var(--accent-hover); }
    
    h1 { font-size: 2rem; margin-bottom: 0.5rem; }
    h2 { font-size: 1.5rem; margin-top: 2rem; margin-bottom: 1rem; border-bottom: 1px solid var(--border); padding-bottom: 0.5rem; }
    h3 { font-size: 1.25rem; margin-top: 1.5rem; margin-bottom: 0.75rem; }
    h4 { font-size: 1.1rem; margin-top: 1.25rem; margin-bottom: 0.5rem; }
    
    p { margin-bottom: 1rem; }
    
    a { color: var(--accent); }
    a:hover { color: var(--accent-hover); }
    
    code {
      background: var(--code-bg);
      padding: 0.2rem 0.4rem;
      border-radius: 0.25rem;
      font-size: 0.9em;
      font-family: 'SF Mono', Monaco, monospace;
    }
    
    pre {
      background: var(--code-bg);
      padding: 1rem;
      border-radius: 0.5rem;
      overflow-x: auto;
      margin-bottom: 1rem;
    }
    pre code {
      background: none;
      padding: 0;
    }
    
    ul, ol { margin-bottom: 1rem; padding-left: 1.5rem; }
    li { margin-bottom: 0.5rem; }
    
    table {
      width: 100%;
      border-collapse: collapse;
      margin-bottom: 1rem;
    }
    th, td {
      border: 1px solid var(--border);
      padding: 0.5rem 0.75rem;
      text-align: left;
    }
    th { background: var(--card-bg); }
    
    blockquote {
      border-left: 3px solid var(--accent);
      padding-left: 1rem;
      margin: 1rem 0;
      color: var(--text-muted);
    }
    
    hr {
      border: none;
      border-top: 1px solid var(--border);
      margin: 2rem 0;
    }
    
    .meta {
      color: var(--text-muted);
      font-size: 0.875rem;
      margin-bottom: 2rem;
    }
  </style>
</head>
<body>
  <a href="/roadmap/" class="back-link">â† Back to Roadmap Index</a>
  
  <article>
    <h1>Asset Enrichment Agent Roadmap</h1>
<h2>Executive Summary</h2>
<p>The Asset Enrichment Agent transforms raw uploaded assets into richly indexed, searchable, and interconnected knowledge. It's the intelligence layer that makes Onelist's "augmented memory" vision possible.</p>
<h3>What It Does</h3>
<table>
<tr><th>Asset Type</th><th>Key Enrichments</th></tr>
<tr><td><strong>Images</strong></td><td>Deep descriptions, OCR, object/scene detection, face detection, GIF analysis, screenshot sequence reconstruction</td></tr>
<tr><td><strong>Audio</strong></td><td>Transcription, speaker diarization, action item extraction, life recording support, media consumption tracking</td></tr>
<tr><td><strong>Video</strong></td><td>Audio-visual fusion, scene detection, captions, screen recording analysis, long-form chaptering</td></tr>
<tr><td><strong>Documents</strong></td><td>Structure extraction, financial data parsing, contract analysis, meeting notes processing</td></tr>
</table>
<h3>Key Features</h3>
<ul>
<li><strong>Life Steward Integration</strong>: Extracted action items, decisions, and people feed directly into the Life Steward for GTD-style organization</li>
<li><strong>Weighted Tag Suggestions</strong>: Multi-dimensional tags with confidence scores, integrated with Onelist's taxonomy system</li>
<li><strong>Privacy-First Design</strong>: Required risk acknowledgment, sensitive content detection, panic button, flight mode</li>
<li><strong>User-Controlled Costs</strong>: Per-content-type processing tiers, pre-operation cost warnings, budget limits (user's own API keys)</li>
<li><strong>Accessibility Priority</strong>: Alt text, transcripts, and captions run first as Tier 1 enrichments</li>
<h3>Implementation Approach</h3>
<table>
<tr><th>Phase</th><th>Focus</th><th>Key Deliverables</th></tr>
<tr><td>1</td><td>Foundation</td><td>Schema, orchestrator, basic image/audio enrichment</td></tr>
<tr><td>2</td><td>Life Steward</td><td>Action items, decisions, people detection, auto-filing hints</td></tr>
<tr><td>3</td><td>Intelligence</td><td>Weighted tags, embeddings, related entry detection</td></tr>
<tr><td>4</td><td>Deep Analysis</td><td>Video pipeline, advanced documents, quote extraction</td></tr>
<tr><td>5</td><td>Life Recording</td><td>Conversation segmentation, media consumption analysis</td></tr>
<tr><td>6</td><td>Advanced</td><td>Local AI, re-enrichment, custom recipes</td></tr>
</table>
<h3>Philosophy</h3>
<li><strong>Inform, don't block</strong>: User's content, user's choice - we detect and warn, not police</li>
<li><strong>Transparency</strong>: All metrics visible, debug mode available, clear cost tracking</li>
<li><strong>Adaptive</strong>: System learns from corrections, adjusts confidence thresholds automatically</li>
<li><strong>Interoperable</strong>: Integrates with Life Steward, Core Taxonomy, Consume Later, and other agents</li>
<h3>Post-MVP Roadmap</h3>
<li>20+ external data source integrations (music, movies, plants, recipes, etc.)</li>
<li>Local AI processing (Whisper, Ollama) for offline/privacy</li>
<li>Custom enrichment recipes per content type</li>
<li>Compliance mode for regulated industries</li>
<li>Advanced admin dashboard for self-hosted</li>
<h3>Document Contents</h3>
<p>This roadmap covers: enrichment types by asset (~100 total), architecture, 6 implementation phases, Life Steward integration, cost management, and 13 additional considerations (error handling, quality assurance, privacy, accessibility, legal compliance, and more).</p>
<hr>
<h2>Overview</h2>
<p>The Asset Enrichment Agent is an intelligent background processing system that extracts maximum value from uploaded assets (images, audio, video, documents). It automatically analyzes content, generates metadata, creates alternative representations, and recommends tags - transforming raw files into richly indexed, searchable, and interconnected knowledge.</p>
<h2>Core Philosophy</h2>
<li><strong>Zero-effort enrichment</strong>: Users upload assets; the system does the rest</li>
<li><strong>Progressive enhancement</strong>: Basic enrichments run immediately; deeper analysis queued</li>
<li><strong>Provider-agnostic</strong>: Support multiple AI providers (OpenAI, Anthropic, Google, local models)</li>
<li><strong>Cost-aware</strong>: User-configurable enrichment levels based on cost/value tradeoffs</li>
<li><strong>Privacy-respecting</strong>: Optional local-only processing for sensitive content</li>
<hr>
<h2>Enrichment Categories by Asset Type</h2>
<h3>1. Image Enrichments</h3>
<h4>Core Philosophy: Deep Semantic Capture</h4>
<p>The goal is not simple object labeling ("dog") but <strong>comprehensive semantic understanding</strong>:
<li><strong>What</strong> is in the image (objects, subjects, text)</li>
<li><strong>Who</strong> (people, animals, characters - with descriptions)</li>
<li><strong>Where</strong> (setting, environment, location type)</li>
<li><strong>What's happening</strong> (actions, interactions, events)</li>
<li><strong>How</strong> (manner, style, mood, quality)</li>
<li><strong>Attributes</strong> (colors, sizes, breeds, styles, conditions)</li></p>
<strong>Example - Dog catching frisbee image:</strong>
<pre><code class="language-">Description: &quot;A muscular brown pitbull mix is leaping through the air in a
grassy park, mouth open to catch a red frisbee. The dog's front legs are
extended forward, ears flapping back from the motion. Sunny day with trees
visible in the background.&quot;
<p>Generated Tags (with weights):
<li>dog (0.99)</li>
<li>pitbull (0.85)</li>
<li>brown dog (0.92)</li>
<li>frisbee (0.97)</li>
<li>red frisbee (0.94)</li>
<li>dog jumping (0.96)</li>
<li>dog catching (0.93)</li>
<li>dog playing fetch (0.88)</li>
<li>fetch game (0.85)</li>
<li>frisbee catching (0.91)</li>
<li>running (0.72)</li>
<li>jumping (0.95)</li>
<li>leaping (0.93)</li>
<li>outdoors (0.98)</li>
<li>park (0.89)</li>
<li>grass (0.94)</li>
<li>sunny day (0.82)</li>
<li>trees (0.71)</li>
<li>action shot (0.90)</li>
<li>pet photography (0.75)</li>
<li>athletic dog (0.80)</li>
</code></pre></p>
<h4>GIF/Animated Image Special Handling</h4>
<p>GIFs require multi-frame analysis to capture the full narrative:</p>
<p>1. <strong>Frame Sampling</strong>: Analyze key frames (not every frame)
2. <strong>Motion Detection</strong>: Identify what's moving and how
3. <strong>Temporal Description</strong>: Describe the sequence of events
4. <strong>Action Extraction</strong>: Capture verbs/actions across the animation
5. <strong>Loop Analysis</strong>: Understand if/how the animation loops</p>
<strong>Example - GIF of cat knocking item off table:</strong>
<pre><code class="language-">Description: &quot;An orange tabby cat sits on a wooden table next to a glass of
water. The cat extends its paw and deliberately pushes the glass toward the
edge. The glass falls off the table and shatters on the floor. The cat
watches it fall with apparent satisfaction, then looks directly at the camera.&quot;
<p>Generated Tags:
<li>cat (0.99)</li>
<li>orange tabby (0.94)</li>
<li>tabby cat (0.95)</li>
<li>cat pushing (0.97)</li>
<li>cat knocking over (0.96)</li>
<li>glass of water (0.92)</li>
<li>broken glass (0.88)</li>
<li>table (0.90)</li>
<li>wooden table (0.85)</li>
<li>mischievous cat (0.89)</li>
<li>cat behavior (0.87)</li>
<li>funny cat (0.82)</li>
<li>cat meme (0.78)</li>
<li>deliberate action (0.84)</li>
<li>indoor (0.95)</li>
<li>pet antics (0.86)</li>
</code></pre></p>
<h4>Enrichment Types</h4>
<table>
<tr><th>Enrichment</th><th>Output Type</th><th>Description</th></tr>
<tr><td><strong>EXIF Extraction</strong></td><td>Metadata</td><td>Camera, date, GPS, settings extraction</td></tr>
<tr><td><strong>OCR Text Extraction</strong></td><td>Representation (plaintext)</td><td>Extract text from screenshots, documents, signs</td></tr>
<tr><td><strong>Deep Scene Analysis</strong></td><td>Representation (markdown) + Tags</td><td>Full narrative description with all details</td></tr>
<tr><td><strong>Subject Description</strong></td><td>Metadata + Tags</td><td>Detailed descriptions of main subjects (breed, color, clothing, expression)</td></tr>
<tr><td><strong>Action Detection</strong></td><td>Tags</td><td>What's happening (running, jumping, eating, talking, working)</td></tr>
<tr><td><strong>Interaction Detection</strong></td><td>Tags</td><td>Relationships between subjects (playing with, holding, looking at)</td></tr>
<tr><td><strong>Setting Analysis</strong></td><td>Tags</td><td>Environment details (indoor/outdoor, room type, location type, weather)</td></tr>
<tr><td><strong>Object Inventory</strong></td><td>Metadata + Tags</td><td>Complete list of visible objects with attributes</td></tr>
<tr><td><strong>Face Detection</strong></td><td>Metadata</td><td>Count faces, expressions, optional recognition if user opts-in</td></tr>
<tr><td><strong>Color Palette</strong></td><td>Metadata + Tags</td><td>Dominant colors, notable color descriptions</td></tr>
<tr><td><strong>Mood/Atmosphere</strong></td><td>Tags</td><td>Emotional tone (cheerful, moody, dramatic, peaceful)</td></tr>
<tr><td><strong>Style Classification</strong></td><td>Tags</td><td>Photo type (portrait, landscape, macro, candid, staged, meme, screenshot)</td></tr>
<tr><td><strong>Alt Text Generation</strong></td><td>Representation</td><td>Accessibility-ready concise description</td></tr>
<tr><td><strong>Document Classification</strong></td><td>Tags</td><td>Receipt, screenshot, meme, photo, diagram, chart, etc.</td></tr>
<tr><td><strong>Handwriting Recognition</strong></td><td>Representation (plaintext)</td><td>Extract handwritten notes</td></tr>
<tr><td><strong>QR/Barcode Scanning</strong></td><td>Metadata + linked Entry</td><td>Decode and optionally create linked entries</td></tr>
<tr><td><strong>Landmark Recognition</strong></td><td>Tags + Metadata</td><td>Identify famous places/buildings</td></tr>
<tr><td><strong>Aesthetic Scoring</strong></td><td>Metadata</td><td>Quality/composition score for photo curation</td></tr>
<tr><td><strong>Similar Image Detection</strong></td><td>Links</td><td>Find duplicates/near-duplicates in library</td></tr>
<tr><td><strong>Content Moderation</strong></td><td>Metadata</td><td>NSFW/sensitive content flagging</td></tr>
<tr><td><strong>Diagram/Chart Analysis</strong></td><td>Representation</td><td>Extract data from charts, describe diagrams</td></tr>
<tr><td><strong>GIF Narrative Analysis</strong></td><td>Representation (markdown)</td><td>Full description of animated sequence</td></tr>
<tr><td><strong>GIF Action Extraction</strong></td><td>Tags</td><td>All actions/movements in animation</td></tr>
<tr><td><strong>Meme Text Extraction</strong></td><td>Representation + Tags</td><td>Extract and understand meme text/context</td></tr>
</table>
<h4>Screenshot Sequence Reconstruction</h4>
<p>When users capture long conversations (iMessage, WhatsApp, Discord, Slack, Twitter threads, etc.), they often take multiple overlapping screenshots. The agent should intelligently reconstruct the full content.</p>
<strong>Detection Triggers:</strong>
<li>Multiple images uploaded together or within short time window</li>
<li>Similar visual structure/app UI detected</li>
<li>OCR reveals overlapping text content</li>
<li>User explicitly marks images as "sequence" or "group"</li>
<li>Entry Group containing multiple screenshot assets</li>
<strong>Reconstruction Process:</strong>
<pre><code class="language-">Screenshot 1:        Screenshot 2:        Screenshot 3:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Message A   â”‚     â”‚ Message C   â”‚     â”‚ Message E   â”‚
â”‚ Message B   â”‚     â”‚ Message D   â”‚     â”‚ Message F   â”‚
â”‚ Message C â—„â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â–º Message C â”‚     â”‚ Message G   â”‚
â”‚ Message D â—„â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â–º Message D â”‚     â”‚ Message H   â”‚
â”‚             â”‚     â”‚ Message E â—„â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â–º Message E â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
<p>Reconstructed Output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Message A   â”‚
â”‚ Message B   â”‚
â”‚ Message C   â”‚  â† Deduplicated
â”‚ Message D   â”‚  â† Deduplicated
â”‚ Message E   â”‚  â† Deduplicated
â”‚ Message F   â”‚
â”‚ Message G   â”‚
â”‚ Message H   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></p>
<strong>Algorithm:</strong>
1. <strong>OCR each image</strong> individually with position data
2. <strong>Detect platform/app</strong> (iMessage, Discord, etc.) for parsing hints
3. <strong>Extract message blocks</strong> with sender, timestamp if visible, content
4. <strong>Fuzzy match</strong> overlapping sections (handle OCR variations)
5. <strong>Order detection</strong> using timestamps, scroll position, visual cues
6. <strong>Merge and deduplicate</strong> preserving correct sequence
7. <strong>Generate unified representation</strong> as markdown conversation
<strong>Output Representation (markdown):</strong>
<pre><code class="language-markdown">## Conversation Reconstruction
<em>Reconstructed from 3 screenshots</em>
<em>Platform: iMessage (detected)</em>
<em>Participants: John, Sarah</em>
<strong>John</strong> (2:34 PM):
Hey, did you see the game last night?
<strong>Sarah</strong> (2:35 PM):
Yes! That last play was incredible
<strong>John</strong> (2:35 PM):
I couldn't believe they pulled it off
<strong>Sarah</strong> (2:36 PM):
We should go to a game together sometime
<p>... [continues]
</code></pre></p>
<strong>Edge Cases to Handle:</strong>
<li>Screenshots in wrong order (use content flow to reorder)</li>
<li>Gaps in conversation (indicate with "[...gap...]")</li>
<li>Different crop sizes/positions</li>
<li>Partial message visibility at edges</li>
<li>Reactions/emoji overlays</li>
<li>Deleted/edited message indicators</li>
<li>Group chats with multiple participants</li>
<li>Mixed media (images within conversation)</li>
<strong>Related Features:</strong>
<li>Link individual screenshot assets to the reconstructed entry</li>
<li>Allow manual reordering if auto-detection fails</li>
<li>"Add more screenshots" to extend existing reconstruction</li>
<li>Platform-specific parsing (timestamps, read receipts, etc.)</li>
<h3>2. Audio Enrichments</h3>
<h4>Core Philosophy: Complete Audio Intelligence</h4>
<p>Audio content is rich with information beyond just words. The agent should extract:
<li><strong>What was said</strong> (accurate transcription)</li>
<li><strong>Who said it</strong> (speaker identification)</li>
<li><strong>How it was said</strong> (tone, emotion, emphasis)</li>
<li><strong>What it means</strong> (topics, summaries, insights)</li>
<li><strong>What to do</strong> (action items, commitments, decisions)</li>
<li><strong>What's notable</strong> (quotes, key moments, highlights)</li></p>
<h4>Audio Type Detection & Specialized Processing</h4>
<p>Different audio types require different enrichment strategies:</p>
<table>
<tr><th>Audio Type</th><th>Detection Signals</th><th>Special Processing</th></tr>
<tr><td><strong>Conversation/Meeting</strong></td><td>Multiple speakers, turn-taking</td><td>Speaker diarization, action items, decisions</td></tr>
<tr><td><strong>Voice Memo</strong></td><td>Single speaker, informal, short</td><td>Quick summary, todo extraction</td></tr>
<tr><td><strong>Podcast</strong></td><td>Intro/outro music, consistent format</td><td>Chapter detection, show notes generation</td></tr>
<tr><td><strong>Interview</strong></td><td>Q&A pattern, two primary speakers</td><td>Question/answer pairing, quote extraction</td></tr>
<tr><td><strong>Lecture/Presentation</strong></td><td>Single speaker, formal, long-form</td><td>Chapter breakdown, key concepts, study notes</td></tr>
<tr><td><strong>Phone Call</strong></td><td>Two speakers, phone audio quality</td><td>Contact association, follow-up extraction</td></tr>
<tr><td><strong>Audiobook</strong></td><td>Single narrator, literary content</td><td>Chapter detection, character tracking</td></tr>
<tr><td><strong>Music</strong></td><td>Melodic content, rhythm</td><td>Artist/song detection, lyrics extraction, mood</td></tr>
<tr><td><strong>Ambient/Field Recording</strong></td><td>Environmental sounds</td><td>Sound identification, location inference</td></tr>
</table>
<h4>Deep Transcription Features</h4>
<strong>Beyond Basic Speech-to-Text:</strong>
<pre><code class="language-markdown">## Transcription Options
<strong>Verbatim Mode:</strong>
<li>Every word including filler words (um, uh, like, you know)</li>
<li>False starts and self-corrections</li>
<li>Overlapping speech indicated</li>
<li>Useful for: legal, research, exact records</li>
<strong>Clean Mode (Default):</strong>
<li>Filler words removed</li>
<li>Grammar lightly smoothed</li>
<li>Natural paragraph breaks</li>
<li>Useful for: most use cases, readability</li>
<strong>Summary Mode:</strong>
<li>Condensed to key points only</li>
<li>Paraphrased for clarity</li>
<li>Useful for: quick review, busy users</li>
</code></pre>
<strong>Speaker Diarization with Descriptions:</strong>
<pre><code class="language-markdown">## Meeting Transcript
<em>Duration: 47:23 | Speakers: 3 detected</em>
<strong>Speaker 1</strong> (Primary speaker, male voice, American accent):
Let's start with the Q3 results...
<strong>Speaker 2</strong> (Female voice, asks many questions):
Can you clarify the margin numbers?
<strong>Speaker 3</strong> (Male voice, technical terminology):
The infrastructure costs were higher due to...
<hr>
<em>Note: Assign speaker names by clicking [Identify Speakers]</em>
</code></pre>
<strong>Timestamp Granularity:</strong>
<li>Paragraph-level (default): Each speaker turn timestamped</li>
<li>Sentence-level: Every sentence timestamped</li>
<li>Word-level: Every word timestamped (for precise navigation)</li>
<h4>Comprehensive Tag Generation from Audio</h4>
<strong>Example - Product Team Meeting:</strong>
<pre><code class="language-">Audio: 45-minute product planning meeting discussing Q2 roadmap
<p>Generated Tags (with weights):
Topics:
<li>product roadmap (0.96)</li>
<li>Q2 planning (0.94)</li>
<li>feature prioritization (0.91)</li>
<li>user research (0.87)</li>
<li>mobile app (0.89)</li>
<li>API improvements (0.82)</li>
<li>performance optimization (0.78)</li></p>
<p>People Mentioned:
<li>@john-smith (0.95) [linked to contact]</li>
<li>@sarah-chen (0.92) [linked to contact]</li>
<li>competitor-acme-corp (0.85)</li></p>
<p>Organizations:
<li>engineering team (0.93)</li>
<li>design team (0.88)</li>
<li>customer-success (0.79)</li></p>
<p>Actions/Themes:
<li>decision made (0.90)</li>
<li>action items (0.94)</li>
<li>deadline discussed (0.87)</li>
<li>budget mentioned (0.82)</li>
<li>risk identified (0.76)</li></p>
<p>Content Type:
<li>meeting (0.98)</li>
<li>planning session (0.91)</li>
<li>internal discussion (0.95)</li></p>
<p>Sentiment:
<li>productive (0.85)</li>
<li>collaborative (0.82)</li>
<li>some tension (0.45)</li>
</code></pre></p>
<h4>Multiple Output Representations</h4>
<p>From a single audio file, generate multiple useful representations:</p>
<table>
<tr><th>Representation Type</th><th>Format</th><th>Use Case</th></tr>
<tr><td><strong>Full Transcript</strong></td><td>markdown</td><td>Complete record, searchable</td></tr>
<tr><td><strong>Speaker Transcript</strong></td><td>markdown</td><td>Who said what, attributed</td></tr>
<tr><td><strong>Executive Summary</strong></td><td>markdown</td><td>2-3 paragraph overview</td></tr>
<tr><td><strong>Key Points</strong></td><td>markdown (bullets)</td><td>Quick takeaways</td></tr>
<tr><td><strong>Action Items</strong></td><td>markdown (checklist)</td><td>Follow-up tasks with owners</td></tr>
<tr><td><strong>Decisions Log</strong></td><td>markdown</td><td>What was decided and by whom</td></tr>
<tr><td><strong>Questions Raised</strong></td><td>markdown</td><td>Open questions, unresolved items</td></tr>
<tr><td><strong>Notable Quotes</strong></td><td>markdown</td><td>Significant statements with timestamps</td></tr>
<tr><td><strong>Chapter Breakdown</strong></td><td>markdown + metadata</td><td>Navigable sections</td></tr>
<tr><td><strong>Study Notes</strong></td><td>markdown</td><td>For lectures - concepts, definitions</td></tr>
<tr><td><strong>Show Notes</strong></td><td>markdown</td><td>For podcasts - links, references, timestamps</td></tr>
</table>
<strong>Example Multi-Representation Output:</strong>
<pre><code class="language-markdown">## Executive Summary
<em>Auto-generated from 45:23 meeting recording</em>
<p>The product team met to finalize the Q2 roadmap. Key decisions included
prioritizing mobile app performance over new features, and delaying the
API v2 launch to Q3. Sarah will lead user research for the new onboarding
flow. Budget concerns were raised regarding infrastructure costs.</p>
<hr>
<h2>Action Items</h2>
<li>[ ] <strong>Sarah</strong>: Complete user research interviews by March 15</li>
<li>[ ] <strong>John</strong>: Draft API v2 migration guide</li>
<li>[ ] <strong>Team</strong>: Review competitor analysis before next meeting</li>
<li>[ ] <strong>Mike</strong>: Get budget approval for additional servers</li>
<hr>
<h2>Decisions Made</h2>
1. <strong>Mobile performance is P0</strong> - All other mobile work paused (32:15)
2. <strong>API v2 delayed to Q3</strong> - Need more migration support (28:45)
3. <strong>Hire contractor for design work</strong> - Approved by leadership (41:02)
<hr>
<h2>Notable Quotes</h2>
&gt; &quot;We can't keep shipping features if the app crashes every other day&quot;
&gt; â€” Speaker 1 @ 33:42
<p>&gt; &quot;The competitor launched this last week and users are asking for it&quot;
&gt; â€” Speaker 2 @ 18:23</p>
<hr>
<h2>Chapter Breakdown</h2>
<li>00:00 - 05:23: Introductions and agenda</li>
<li>05:24 - 18:45: Q1 retrospective review</li>
<li>18:46 - 32:14: Feature prioritization discussion</li>
<li>32:15 - 41:00: Mobile performance deep-dive</li>
<li>41:01 - 45:23: Action items and wrap-up</li>
</code></pre>
<h4>Multi-Part Audio Reconstruction</h4>
<p>Similar to screenshot sequences, handle split recordings:</p>
<strong>Detection Triggers:</strong>
<li>Multiple audio files uploaded together</li>
<li>Similar audio characteristics (same speakers, room, quality)</li>
<li>Content continuity (mid-sentence splits, continuing topics)</li>
<li>User marks as "continued" or groups in Entry Group</li>
<li>Filename patterns (meeting_part1.m4a, meeting_part2.m4a)</li>
<strong>Reconstruction Process:</strong>
1. Analyze each audio file individually
2. Detect speaker voice signatures across files
3. Identify content overlap or continuation points
4. Merge transcripts with proper ordering
5. Unify speaker labels across parts
6. Generate single cohesive representation
7. Maintain links to original audio segments with timestamps
<strong>Output:</strong>
<pre><code class="language-markdown">## Unified Transcript
<em>Reconstructed from 3 audio files</em>
<em>Total duration: 1:47:23</em>
<em>Files: meeting_part1.m4a (45:00), meeting_part2.m4a (38:12), meeting_part3.m4a (24:11)</em>
<p>[00:00:00 - Part 1]
<strong>Sarah</strong>: Welcome everyone to the quarterly review...</p>
<p>[45:00:00 - Part 2 begins]
<strong>Sarah</strong>: ...continuing from where we left off with the sales numbers...</p>
<p>[1:23:12 - Part 3 begins]
<strong>John</strong>: Let's wrap up with action items...
</code></pre></p>
<h4>Specialized Audio Analysis</h4>
<strong>Conversation Dynamics:</strong>
<li>Talk time per speaker (who dominated?)</li>
<li>Interruption frequency</li>
<li>Question/answer ratio</li>
<li>Silence/pause patterns</li>
<li>Energy level changes throughout</li>
<strong>Meeting Effectiveness Metrics:</strong>
<li>Time spent on each agenda item</li>
<li>Decisions per hour</li>
<li>Action items generated</li>
<li>Off-topic tangent duration</li>
<li>Participation balance</li>
<strong>Podcast Analysis:</strong>
<li>Ad segment detection (skip markers)</li>
<li>Guest vs host identification</li>
<li>Topic timeline</li>
<li>Cross-reference mentions (other episodes, external content)</li>
<strong>Voice Memo Intelligence:</strong>
<li>Intent classification (reminder, idea, vent, note-to-self)</li>
<li>Urgency detection</li>
<li>Auto-link to relevant entries/projects</li>
<li>Suggested entry type (task, note, memory)</li>
<h4>Life Recording / Always-On Wearable Audio</h4>
<p>Support for continuous audio capture devices (Limitless Pendant, Plaud, Humane, custom solutions) that record throughout the day. The agent must intelligently process hours of ambient audio into meaningful, organized entries.</p>
<strong>Core Challenge:</strong>
<pre><code class="language-">Input:  8+ hours of continuous audio
Output: Discrete, tagged, linked entries for each meaningful interaction
</code></pre>
<strong>Conversation Boundary Detection:</strong>
<table>
<tr><th>Signal</th><th>Detection Method</th></tr>
<tr><td><strong>Silence gaps</strong></td><td>Extended silence (>30s configurable) indicates conversation end</td></tr>
<tr><td><strong>Speaker change</strong></td><td>New voices entering, familiar voices leaving</td></tr>
<tr><td><strong>Location change</strong></td><td>Ambient sound shift (office â†’ car â†’ restaurant)</td></tr>
<tr><td><strong>Topic discontinuity</strong></td><td>Major subject change even with same speakers</td></tr>
<tr><td><strong>Time gaps</strong></td><td>Recording pauses/resumes</td></tr>
<tr><td><strong>Audio environment</strong></td><td>Background noise profile change</td></tr>
</table>
<strong>Auto-Segmentation Pipeline:</strong>
<pre><code class="language-">Raw Life Recording (8 hours)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Conversation Detection  â”‚
â”‚ â€¢ Find boundaries       â”‚
â”‚ â€¢ Score importance      â”‚
â”‚ â€¢ Classify type         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Segment 1: 9:15-9:32am  â”‚ â†’ Entry: &quot;Morning standup with engineering team&quot;
â”‚ Type: Meeting           â”‚   Tags: #standup #engineering #work
â”‚ Speakers: 4 detected    â”‚   Links: Project:Backend-Refactor, @john, @sarah
â”‚ Location: Office        â”‚   Action Items: 3 extracted
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Segment 2: 10:45-10:52  â”‚ â†’ Entry: &quot;Quick chat with Sarah about deadline&quot;
â”‚ Type: 1:1 Conversation  â”‚   Tags: #deadline #project-x
â”‚ Speakers: 2 detected    â”‚   Links: @sarah, Task:Submit-Proposal
â”‚ Location: Office        â”‚   Follow-up: Deadline moved to Friday
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Segment 3: 12:30-1:15pm â”‚ â†’ Entry: &quot;Lunch with Mom - discussed vacation plans&quot;
â”‚ Type: Personal          â”‚   Tags: #family #vacation #planning
â”‚ Speakers: 2 detected    â”‚   Links: @mom, Project:Summer-Vacation
â”‚ Location: Restaurant    â”‚   Memory: Mom prefers beach over mountains
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Segment 4: 3:00-3:45pm  â”‚ â†’ Entry: &quot;Client call - Q2 contract negotiation&quot;
â”‚ Type: Business Call     â”‚   Tags: #client #sales #contract #acme-corp
â”‚ Speakers: 3 detected    â”‚   Links: @client-bob, Project:Acme-Deal
â”‚ Location: Office        â”‚   Decisions: 2, Action Items: 4
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<strong>Automatic Linking Intelligence:</strong>
<table>
<tr><th>Link Type</th><th>Detection Method</th></tr>
<tr><td><strong>People</strong></td><td>Voice recognition â†’ known contacts, or "Unknown speaker, male, deep voice"</td></tr>
<tr><td><strong>Projects</strong></td><td>Topic matching to existing projects, keywords, context</td></tr>
<tr><td><strong>Tasks</strong></td><td>Mentioned tasks, deadlines, commitments</td></tr>
<tr><td><strong>Calendar Events</strong></td><td>Time correlation with calendar, meeting titles mentioned</td></tr>
<tr><td><strong>Locations</strong></td><td>GPS (if available), ambient sound, mentioned places</td></tr>
<tr><td><strong>Previous Conversations</strong></td><td>"Following up on what we discussed yesterday"</td></tr>
<tr><td><strong>Entries</strong></td><td>References to notes, documents, shared items</td></tr>
</table>
<strong>Context Inference:</strong>
<pre><code class="language-yaml">Segment Analysis:
  time: 9:15am Tuesday
  location_audio: Office ambient (keyboard, HVAC, distant chatter)
  calendar_correlation: &quot;Engineering Standup&quot; at 9:15am
  speakers:
    <li>Speaker 1: 95% match â†’ John Smith (known contact)</li>
    <li>Speaker 2: 88% match â†’ Sarah Chen (known contact)</li>
    <li>Speaker 3: Unknown, male voice (suggest: &quot;Add to contacts?&quot;)</li>
    <li>Speaker 4: 72% match â†’ Mike Johnson (known contact)</li>
  inferred_context: Work meeting, engineering team, likely standup
<p>Auto-Applied:
  <li>Project: Backend-Refactor (mentioned 4 times)</li>
  <li>Tags: #work #engineering #standup #sprint-planning</li>
  <li>Entry Type: memory (meeting record)</li>
</code></pre></p>
<strong>Importance Scoring:</strong>
<p>Not all conversations are worth saving as detailed entries. Score by:</p>
<table>
<tr><th>Factor</th><th>Weight</th><th>Examples</th></tr>
<tr><td><strong>Action items present</strong></td><td>High</td><td>"I'll send that by Friday"</td></tr>
<tr><td><strong>Decisions made</strong></td><td>High</td><td>"Let's go with option B"</td></tr>
<tr><td><strong>New information</strong></td><td>High</td><td>Learning something new</td></tr>
<tr><td><strong>Known important people</strong></td><td>Medium</td><td>Boss, spouse, key clients</td></tr>
<tr><td><strong>Project relevance</strong></td><td>Medium</td><td>Mentions active projects</td></tr>
<tr><td><strong>Emotional significance</strong></td><td>Medium</td><td>Laughter, excitement, concern</td></tr>
<tr><td><strong>Duration</strong></td><td>Low</td><td>Longer = potentially more important</td></tr>
<tr><td><strong>Routine/repeated</strong></td><td>Negative</td><td>Daily small talk, "how's it going"</td></tr>
</table>
<strong>Output Options:</strong>
<pre><code class="language-yaml">High importance (score &gt; 0.8):
  â†’ Full entry with complete transcript
  â†’ All representations (summary, action items, quotes)
  â†’ All auto-links applied
  â†’ Notification: &quot;New important conversation captured&quot;
<p>Medium importance (0.5-0.8):
  â†’ Entry with summary only
  â†’ Key points extracted
  â†’ Auto-links applied
  â†’ Appears in daily digest</p>
<p>Low importance (0.2-0.5):
  â†’ Minimal entry, title + brief note
  â†’ Searchable but not prominent
  â†’ &quot;12:15pm - Brief chat with coworker about weather&quot;</p>
<p>Very low importance (&lt; 0.2):
  â†’ Log only (no entry created)
  â†’ Retained for X days for search
  â†’ &quot;Background conversation at coffee shop&quot;</p>
<p>User preference: &quot;Save everything&quot; vs &quot;Smart filtering&quot; vs &quot;Important only&quot;
</code></pre></p>
<strong>Privacy Controls:</strong>
<table>
<tr><th>Control</th><th>Function</th></tr>
<tr><td><strong>Location-based pause</strong></td><td>Auto-pause in certain locations (doctor, therapy, etc.)</td></tr>
<tr><td><strong>Keyword triggers</strong></td><td>Pause when certain phrases detected ("off the record")</td></tr>
<tr><td><strong>Speaker-based</strong></td><td>Never record certain people, or always record certain people</td></tr>
<tr><td><strong>Time-based</strong></td><td>Recording hours (work only, always, custom schedule)</td></tr>
<tr><td><strong>Retroactive delete</strong></td><td>"Delete last 5 minutes" voice command</td></tr>
<tr><td><strong>Encryption</strong></td><td>E2EE for sensitive conversations</td></tr>
<tr><td><strong>Consent detection</strong></td><td>Flag when new speakers join (for consent notification)</td></tr>
</table>
<strong>Daily Life Recording Digest:</strong>
<pre><code class="language-markdown">## Daily Audio Digest - Tuesday, March 15
<strong>Conversations Captured:</strong> 12
<strong>Total Audio:</strong> 6h 23m â†’ 47 minutes of actual conversation
<strong>Entries Created:</strong> 8 (4 high importance, 3 medium, 1 low)
<h3>Highlights</h3>
<p>ğŸ¯ <strong>Action Items Extracted (7):</strong>
<li>[ ] Send proposal to Acme Corp by Friday (@you, from Client Call)</li>
<li>[ ] Review John's PR for auth changes (@you, from Standup)</li>
<li>[ ] Book restaurant for anniversary dinner (@you, from Call with spouse)</li>
...</p>
<p>ğŸ’¡ <strong>Decisions Made (3):</strong>
<li>Go with Redis for caching solution (Standup, 9:15am)</li>
<li>Move deadline to next Friday (Chat with Sarah, 10:45am)</li>
<li>Summer vacation will be beach trip (Lunch with Mom, 12:30pm)</li></p>
<p>ğŸ’¬ <strong>Notable Quotes:</strong>
&gt; &quot;This is the best quarter we've had in three years&quot;
&gt; â€” Client Bob, 3:23pm</p>
<p>ğŸ“Š <strong>Time Breakdown:</strong>
<li>Work conversations: 3h 12m (5 entries)</li>
<li>Personal: 1h 45m (2 entries)</li>
<li>Ambient/low-value: 1h 26m (logged only)</li></p>
<h3>Quick Links</h3>
[View all today's entries] [Manage privacy settings] [Retrain voice recognition]
</code></pre>
<h4>Media Consumption Detection & Analysis</h4>
<p>Life recording will capture not just conversations, but also media the user consumes - podcasts, YouTube, TV shows, audiobooks, lectures, etc. This media must be:
1. <strong>Detected and separated</strong> from personal conversations
2. <strong>Identified</strong> (what content is this?)
3. <strong>Analyzed for relevance</strong> to user's goals, projects, and interests
4. <strong>Tracked</strong> for consumption patterns and research interests</p>
<strong>Media Detection Signals:</strong>
<table>
<tr><th>Signal</th><th>Indicates</th></tr>
<tr><td><strong>Audio fingerprinting</strong></td><td>Match against known media databases</td></tr>
<tr><td><strong>Production quality</strong></td><td>Professional audio vs ambient conversation</td></tr>
<tr><td><strong>Single voice, scripted</strong></td><td>Podcast host, audiobook narrator</td></tr>
<tr><td><strong>Music beds/jingles</strong></td><td>Podcast intros, ad breaks</td></tr>
<tr><td><strong>Consistent audio level</strong></td><td>Produced media vs variable conversation</td></tr>
<tr><td><strong>No user voice present</strong></td><td>User is listening, not participating</td></tr>
<tr><td><strong>User voice responding to media</strong></td><td>"That's interesting" - one-sided conversation</td></tr>
<tr><td><strong>Device audio signature</strong></td><td>TV speakers, headphones, car stereo patterns</td></tr>
</table>
<strong>Media Type Classification:</strong>
<pre><code class="language-">Detected Audio Stream
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Media Classification Engine â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â–º Podcast
        â”‚    â”œâ”€â”€ Episode identification (if possible)
        â”‚    â”œâ”€â”€ Show identification
        â”‚    â””â”€â”€ Topic extraction
        â”‚
        â”œâ”€â”€â–º YouTube/Video Audio
        â”‚    â”œâ”€â”€ Creator identification
        â”‚    â”œâ”€â”€ Content type (tutorial, commentary, entertainment)
        â”‚    â””â”€â”€ Topic extraction
        â”‚
        â”œâ”€â”€â–º Television
        â”‚    â”œâ”€â”€ Show identification
        â”‚    â”œâ”€â”€ Episode detection (if dialogue matches)
        â”‚    â””â”€â”€ Genre classification
        â”‚
        â”œâ”€â”€â–º Audiobook
        â”‚    â”œâ”€â”€ Book identification
        â”‚    â”œâ”€â”€ Chapter detection
        â”‚    â””â”€â”€ Reading progress tracking
        â”‚
        â”œâ”€â”€â–º Music
        â”‚    â”œâ”€â”€ Song/artist identification
        â”‚    â”œâ”€â”€ Genre, mood tagging
        â”‚    â””â”€â”€ Listening history
        â”‚
        â”œâ”€â”€â–º Lecture/Course
        â”‚    â”œâ”€â”€ Course/institution identification
        â”‚    â”œâ”€â”€ Topic extraction
        â”‚    â””â”€â”€ Learning progress tracking
        â”‚
        â”œâ”€â”€â–º News/Radio
        â”‚    â”œâ”€â”€ Station/program identification
        â”‚    â”œâ”€â”€ Story extraction
        â”‚    â””â”€â”€ Topic categorization
        â”‚
        â””â”€â”€â–º Unknown Media
             â”œâ”€â”€ Full transcription
             â”œâ”€â”€ Topic extraction
             â””â”€â”€ Flag for user identification
</code></pre>
<strong>Separation from Conversations:</strong>
<pre><code class="language-">Timeline: 6:00 PM - 8:00 PM
<p>â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6:00-6:15   â”‚ 6:15-6:45    â”‚ 6:45-7:00   â”‚ 7:00-7:45    â”‚ 7:45-8:00   â”‚
â”‚             â”‚              â”‚             â”‚              â”‚             â”‚
â”‚ TV: News    â”‚ Conversation â”‚ TV: News    â”‚ TV: Show     â”‚ Conversationâ”‚
â”‚ Background  â”‚ with Spouse  â”‚ Background  â”‚ Active Watch â”‚ with Spouse â”‚
â”‚             â”‚              â”‚             â”‚              â”‚             â”‚
â”‚ [Ambient]   â”‚ [Entry]      â”‚ [Ambient]   â”‚ [Media Entry]â”‚ [Entry]     â”‚
â”‚ Low priorityâ”‚ Full process â”‚ Low priorityâ”‚ Analyze      â”‚ Full processâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</p>
<p>Detection: Conversation starts â†’ Media demoted to &quot;background&quot;
Detection: User actively watching (no conversation, reactions) â†’ Media promoted
</code></pre></p>
<strong>Media Consumption Entry Creation:</strong>
<pre><code class="language-yaml">Entry Type: media_consumption (or memory with media subtype)
<p>Example - Podcast Episode:
  title: &quot;Listened to: Huberman Lab - Sleep Optimization&quot;
  media_type: podcast
  identified:
    show: &quot;Huberman Lab&quot;
    episode: &quot;Episode 127: Master Your Sleep&quot;
    host: &quot;Andrew Huberman&quot;
    duration_consumed: 45:23 of 2:15:00
    completion: 33%</p>
<p>content_analysis:
    topics:
      <li>sleep optimization (0.95)</li>
      <li>circadian rhythm (0.92)</li>
      <li>light exposure (0.88)</li>
      <li>supplements (melatonin, magnesium) (0.85)</li>
      <li>temperature regulation (0.82)</li></p>
<p>key_takeaways:
      <li>&quot;View morning sunlight within 30-60 minutes of waking&quot;</li>
      <li>&quot;Keep bedroom temperature between 65-68Â°F&quot;</li>
      <li>&quot;Avoid bright lights 2 hours before bed&quot;</li>
      <li>&quot;Magnesium threonate may improve sleep quality&quot;</li></p>
<p>mentioned_resources:
      <li>Book: &quot;Why We Sleep&quot; by Matthew Walker</li>
      <li>Study: Stanford sleep research paper</li>
      <li>Product: Eight Sleep mattress</li></p>
<p>actionable_insights:
      <li>[ ] Get morning sunlight exposure</li>
      <li>[ ] Check bedroom temperature</li>
      <li>[ ] Research magnesium threonate</li>
</code></pre></p>
<strong>Goal & Project Relevance Analysis:</strong>
<p>The key value-add: connecting consumed media to what the user cares about.</p>
<pre><code class="language-">User's Active Context:
â”œâ”€â”€ Goals:
â”‚   â”œâ”€â”€ &quot;Improve sleep quality&quot; (Health)
â”‚   â”œâ”€â”€ &quot;Learn Python for data analysis&quot; (Career)
â”‚   â”œâ”€â”€ &quot;Plan kitchen renovation&quot; (Home)
â”‚   â””â”€â”€ &quot;Read more non-fiction&quot; (Personal)
â”‚
â”œâ”€â”€ Active Projects:
â”‚   â”œâ”€â”€ &quot;Q2 Marketing Campaign&quot;
â”‚   â”œâ”€â”€ &quot;Home Automation Setup&quot;
â”‚   â””â”€â”€ &quot;Side Business Launch&quot;
â”‚
â””â”€â”€ Tracked Interests:
    â”œâ”€â”€ AI/Machine Learning
    â”œâ”€â”€ Productivity systems
    â”œâ”€â”€ Cooking/Recipes
    â””â”€â”€ Personal finance
<p>â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</p>
<p>Consumed Media: Huberman Lab - Sleep Optimization</p>
<p>Relevance Analysis:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ¯ HIGH RELEVANCE: Goal &quot;Improve sleep quality&quot;                â”‚
â”‚    Match score: 0.94                                           â”‚
â”‚    Reason: Direct topic match - sleep optimization strategies  â”‚
â”‚    Suggested action: Create tasks from episode takeaways       â”‚
â”‚    Auto-link: Yes                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ“Š MEDIUM RELEVANCE: Interest &quot;Productivity systems&quot;           â”‚
â”‚    Match score: 0.67                                           â”‚
â”‚    Reason: Sleep impacts productivity, tangentially related    â”‚
â”‚    Suggested action: Tag for research reference                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ğŸ’¡ POTENTIAL RELEVANCE: Project &quot;Home Automation Setup&quot;        â”‚
â”‚    Match score: 0.45                                           â”‚
â”‚    Reason: Episode mentions smart lighting for circadian rhythmâ”‚
â”‚    Suggested action: Note idea for bedroom automation          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</p>
<p>Generated Entry Additions:
<li>Link to Goal: &quot;Improve sleep quality&quot;</li>
<li>Tags: #sleep #health #podcast #huberman #research</li>
<li>Tasks created:</li>
  <li>[ ] Get morning sunlight (linked to goal)</li>
  <li>[ ] Research smart bedroom lighting (linked to project)</li>
<li>Research note: &quot;Magnesium threonate mentioned - investigate&quot;</li>
</code></pre></p>
<strong>Interest Tracking & Research Correlation:</strong>
<p>Track media consumption patterns to understand and serve user interests:</p>
<pre><code class="language-yaml">Interest Profile (Auto-Generated):
<p>AI/Machine Learning:
  engagement_level: High
  recent_consumption:
    <li>Lex Fridman #312: Sam Altman (listened 100%)</li>
    <li>YouTube: &quot;Transformers Explained&quot; (watched 75%)</li>
    <li>Podcast: &quot;Practical AI&quot; x 5 episodes</li>
  total_hours_this_month: 12.5
  trending: â†‘ (up 40% from last month)
  related_goals: &quot;Learn Python for data analysis&quot;
  suggested_deep_dives:
    <li>Course: Fast.ai (mentioned 3x in consumed content)</li>
    <li>Book: &quot;Deep Learning&quot; by Goodfellow (referenced)</li></p>
<p>Personal Finance:
  engagement_level: Medium
  recent_consumption:
    <li>YouTube: &quot;Index Fund Investing 2024&quot;</li>
    <li>Podcast: &quot;Rational Reminder&quot; x 2 episodes</li>
  total_hours_this_month: 4.2
  trending: â†’ (stable)
  related_goals: None explicitly set
  suggested: &quot;Create a personal finance goal?&quot;</p>
<p>Cooking/Recipes:
  engagement_level: Passive
  recent_consumption:
    <li>Background: Cooking shows while doing dishes (8 instances)</li>
  total_hours_this_month: 6.0
  note: &quot;Mostly background consumption, not active engagement&quot;
</code></pre></p>
<strong>Media-Triggered Entry Creation:</strong>
<pre><code class="language-">Consumption Event                    â†’  Resulting Action
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Podcast mentions book user owns      â†’  Add to reading queue, link entries
Tutorial matches active project      â†’  Link to project, extract steps
News about company user invests in   â†’  Create note, link to finance tracking
Recipe video while cooking           â†’  Offer to save recipe as entry
Audiobook chapter completed          â†’  Update reading progress
Documentary about user's interest    â†’  Add to research collection
Course lecture watched               â†’  Update learning progress, extract notes
</code></pre>
<strong>Daily Media Consumption Digest:</strong>
<pre><code class="language-markdown">## Media Consumption - Tuesday, March 15
<h3>Summary</h3>
<strong>Total media time:</strong> 3h 47m
<strong>Active consumption:</strong> 2h 15m (podcasts, focused video)
<strong>Background:</strong> 1h 32m (TV while cooking, music)
<h3>High-Value Content Consumed</h3>
<p>ğŸ“š <strong>Audiobook: &quot;Atomic Habits&quot; by James Clear</strong>
<li>Progress: Chapter 4-6 (45 minutes)</li>
<li>Key concept: &quot;Habit stacking&quot;</li>
<li>Relevance: ğŸ¯ Directly relates to Goal: &quot;Build morning routine&quot;</li>
<li>[View extracted notes] [Create habit stack entry]</li></p>
<p>ğŸ§ <strong>Podcast: &quot;How I Built This - Spanx&quot;</strong>
<li>Duration: 52 minutes (complete)</li>
<li>Key insight: &quot;Prototype with minimal resources&quot;</li>
<li>Relevance: ğŸ’¡ May relate to Project: &quot;Side Business Launch&quot;</li>
<li>[Link to project] [Save quote]</li></p>
<p>ğŸ“º <strong>YouTube: &quot;Home Assistant Automation Ideas&quot;</strong>
<li>Duration: 23 minutes (watched 100%)</li>
<li>Relevance: ğŸ¯ Directly relates to Project: &quot;Home Automation Setup&quot;</li>
<li>Extracted: 5 automation ideas</li>
<li>[View ideas] [Create tasks]</li></p>
<h3>Interest Trends This Week</h3>
<li>AI/ML content: â†‘ 2.5 hours (above average)</li>
<li>Productivity: â†’ stable</li>
<li>Cooking: â†“ less than usual</li>
<h3>Suggested Actions</h3>
<li>[ ] Audiobook &quot;Atomic Habits&quot; mentioned 3 concepts - review and apply?</li>
<li>[ ] YouTube creator &quot;TechWithTim&quot; watched 4 videos - subscribe/follow?</li>
<li>[ ] Podcast backlog: 12 episodes queued across 3 shows</li>
<h3>Background Media (Low Priority)</h3>
<li>CNN/News: 45 min (no specific entries created)</li>
<li>Music: 47 min (playlist: &quot;Focus&quot; - logged to listening history)</li>
</code></pre>
<strong>Integration with Consume Later System:</strong>
<p>See <a href="./future_roadmap_consume_later.md">Consume Later Roadmap</a> for full details. Media detected during life recording can:
<li>Auto-mark items as "consumed" if detected</li>
<li>Update progress on partially consumed items</li>
<li>Suggest adding mentioned resources to "consume later" queue</li>
<li>Track completion rates across media types</li></p>
<strong>Integration with Limitless.ai:</strong>
<p>See <a href="./future_roadmap_limitless.md">Limitless Integration Roadmap</a> for specific integration details. The Life Recording features here are designed to be compatible with Limitless data export and can enhance their transcripts with Onelist's linking and organization capabilities.</p>
<h4>Enrichment Types Table</h4>
<table>
<tr><th>Enrichment</th><th>Output Type</th><th>Description</th></tr>
<tr><td><strong>Deep Transcription</strong></td><td>Representation (markdown)</td><td>Full transcript with speaker attribution, timestamps</td></tr>
<tr><td><strong>Audio Type Detection</strong></td><td>Metadata</td><td>Meeting, podcast, voice memo, music, etc.</td></tr>
<tr><td><strong>Speaker Diarization</strong></td><td>Representation + Metadata</td><td>Who said what with voice signatures</td></tr>
<tr><td><strong>Speaker Identification</strong></td><td>Metadata</td><td>Link to known contacts/speakers</td></tr>
<tr><td><strong>Language Detection</strong></td><td>Metadata</td><td>Primary and secondary languages</td></tr>
<tr><td><strong>Multi-Language Transcription</strong></td><td>Representation</td><td>Handle code-switching, translate if needed</td></tr>
<tr><td><strong>Executive Summary</strong></td><td>Representation (markdown)</td><td>2-3 paragraph overview</td></tr>
<tr><td><strong>Key Points Extraction</strong></td><td>Representation (markdown)</td><td>Bullet-point takeaways</td></tr>
<tr><td><strong>Action Items</strong></td><td>Representation + linked Entries</td><td>Tasks with owners and deadlines</td></tr>
<tr><td><strong>Decisions Log</strong></td><td>Representation</td><td>What was decided, by whom, when</td></tr>
<tr><td><strong>Questions Extraction</strong></td><td>Representation</td><td>Open questions, unresolved items</td></tr>
<tr><td><strong>Quote Extraction</strong></td><td>Representation</td><td>Notable quotes with timestamps</td></tr>
<tr><td><strong>Chapter Detection</strong></td><td>Metadata + Representation</td><td>Logical segments with timestamps</td></tr>
<tr><td><strong>Topic Extraction</strong></td><td>Tags</td><td>Main subjects discussed</td></tr>
<tr><td><strong>Named Entity Recognition</strong></td><td>Metadata + Tags</td><td>People, companies, products mentioned</td></tr>
<tr><td><strong>Sentiment Analysis</strong></td><td>Metadata</td><td>Overall and per-segment tone</td></tr>
<tr><td><strong>Emotion Detection</strong></td><td>Metadata</td><td>Emotional undertones in voice</td></tr>
<tr><td><strong>Conversation Dynamics</strong></td><td>Metadata</td><td>Talk time, interruptions, participation</td></tr>
<tr><td><strong>Music Detection</strong></td><td>Metadata + Tags</td><td>Artist, song, genre identification</td></tr>
<tr><td><strong>Lyrics Extraction</strong></td><td>Representation</td><td>For music content</td></tr>
<tr><td><strong>Sound Identification</strong></td><td>Tags</td><td>Non-speech sounds (applause, laughter, background)</td></tr>
<tr><td><strong>Audio Quality Assessment</strong></td><td>Metadata</td><td>Quality score, noise level, issues</td></tr>
<tr><td><strong>Multi-Part Reconstruction</strong></td><td>Representation</td><td>Merge split recordings intelligently</td></tr>
</table>
<h3>3. Video Enrichments</h3>
<h4>Core Philosophy: Multi-Modal Fusion</h4>
<p>Video is unique because it combines visual and audio streams. The agent should:
<li><strong>Extract audio</strong> â†’ Full audio enrichment pipeline (transcription, speakers, analysis)</li>
<li><strong>Analyze visuals</strong> â†’ Frame-by-frame understanding, motion, objects, scenes</li>
<li><strong>Fuse both streams</strong> â†’ Combined understanding greater than sum of parts</li>
<li><strong>Understand narrative</strong> â†’ What's the story being told across time?</li></p>
<strong>The fusion insight:</strong> A video of someone saying "I love this" while frowning tells a different story than audio or video alone.
<h4>Video Type Detection & Specialized Processing</h4>
<table>
<tr><th>Video Type</th><th>Detection Signals</th><th>Special Processing</th></tr>
<tr><td><strong>Screen Recording</strong></td><td>UI elements, cursor movement, static frame regions</td><td>OCR all text, detect app/website, step extraction</td></tr>
<tr><td><strong>Tutorial/How-To</strong></td><td>Screen + voiceover, step-by-step narration</td><td>Step-by-step breakdown, tool/technique tagging</td></tr>
<tr><td><strong>Meeting Recording</strong></td><td>Gallery view, multiple faces, talking heads</td><td>Speaker timeline, face-to-voice matching</td></tr>
<tr><td><strong>Presentation</strong></td><td>Slides + speaker, screen share</td><td>Slide extraction, sync with narration</td></tr>
<tr><td><strong>Vlog/Personal</strong></td><td>Single POV, casual narration, varied scenes</td><td>Story beats, location timeline, activity log</td></tr>
<tr><td><strong>Interview</strong></td><td>Two cameras or split screen, Q&A pattern</td><td>Question/answer pairing, quote extraction</td></tr>
<tr><td><strong>Event Recording</strong></td><td>Crowd, stage, ambient audio, long duration</td><td>Highlight extraction, moment detection</td></tr>
<tr><td><strong>Surveillance/Security</strong></td><td>Fixed camera, motion-triggered, timestamp overlay</td><td>Motion events, person detection, anomaly flagging</td></tr>
<tr><td><strong>Sports/Action</strong></td><td>Fast motion, specific play patterns</td><td>Play-by-play, highlight moments, statistics</td></tr>
<tr><td><strong>Nature/Documentary</strong></td><td>Wildlife, landscapes, narration</td><td>Species identification, location, narration sync</td></tr>
<tr><td><strong>Music Video</strong></td><td>Music audio, artistic visuals, lyrics</td><td>Song ID, lyrics sync, visual theme analysis</td></tr>
<tr><td><strong>Short-form (TikTok/Reels)</strong></td><td>Vertical, <60s, trending audio</td><td>Trend detection, meme classification, viral elements</td></tr>
<tr><td><strong>Home Video</strong></td><td>Casual framing, family events, amateur</td><td>People identification, event type, memory tagging</td></tr>
<tr><td><strong>Dashcam/Body Cam</strong></td><td>Fixed POV, continuous, timestamp</td><td>Incident detection, location tracking, event flagging</td></tr>
</table>
<h4>Visual Analysis Pipeline</h4>
<strong>Frame Sampling Strategy:</strong>
<pre><code class="language-">Video Duration    Sample Rate           Key Frame Detection
&lt; 1 minute        Every 1 second        + Scene changes
1-10 minutes      Every 5 seconds       + Scene changes + Motion peaks
10-60 minutes     Every 15 seconds      + Scene changes + Audio cues
&gt; 60 minutes      Every 30 seconds      + Scene changes + Chapter boundaries
</code></pre>
<strong>Scene Detection & Segmentation:</strong>
<pre><code class="language-">Raw Video
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scene 1 (0:00-2:34)    â”‚ Scene 2 (2:35-5:12)    â”‚ Scene 3...   â”‚
â”‚ Location: Kitchen      â”‚ Location: Living Room  â”‚              â”‚
â”‚ People: 2 detected     â”‚ People: 3 detected     â”‚              â”‚
â”‚ Activity: Cooking      â”‚ Activity: Conversation â”‚              â”‚
â”‚ Objects: stove, pan,   â”‚ Objects: couch, TV,    â”‚              â”‚
â”‚   ingredients          â”‚   coffee table         â”‚              â”‚
â”‚ Mood: Focused          â”‚ Mood: Relaxed, social  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre>
<strong>Deep Visual Understanding Per Scene:</strong>
<table>
<tr><th>Analysis</th><th>Output</th><th>Description</th></tr>
<tr><td><strong>Setting/Location</strong></td><td>Tags + Metadata</td><td>Indoor/outdoor, room type, environment</td></tr>
<tr><td><strong>People Detection</strong></td><td>Metadata</td><td>Count, positions, clothing, approximate age/gender</td></tr>
<tr><td><strong>Face Recognition</strong></td><td>Links</td><td>Match to known contacts (opt-in)</td></tr>
<tr><td><strong>Expression Analysis</strong></td><td>Metadata</td><td>Emotional state over time per person</td></tr>
<tr><td><strong>Activity Recognition</strong></td><td>Tags</td><td>What people are doing (cooking, talking, working, playing)</td></tr>
<tr><td><strong>Object Detection</strong></td><td>Tags</td><td>All visible objects with persistence tracking</td></tr>
<tr><td><strong>Object Interactions</strong></td><td>Tags</td><td>Person using laptop, holding cup, petting dog</td></tr>
<tr><td><strong>Text/OCR</strong></td><td>Representation</td><td>Signs, screens, documents visible in frame</td></tr>
<tr><td><strong>Brand/Logo Detection</strong></td><td>Tags</td><td>Products, companies visible</td></tr>
<tr><td><strong>Motion Analysis</strong></td><td>Metadata</td><td>Movement patterns, action intensity</td></tr>
<tr><td><strong>Camera Work</strong></td><td>Metadata</td><td>Panning, zooming, cuts, handheld vs stable</td></tr>
<tr><td><strong>Visual Style</strong></td><td>Tags</td><td>Aesthetic, color grading, quality, format</td></tr>
</table>
<h4>Audio-Visual Fusion Analysis</h4>
<strong>Speaker-Face Matching:</strong>
<pre><code class="language-markdown">## Meeting Recording Analysis
<p>Detected Speakers (Audio):          Detected Faces (Video):
â”œâ”€â”€ Speaker 1: Female voice    â†â†’   Face A: Woman, blonde, glasses
â”œâ”€â”€ Speaker 2: Male voice      â†â†’   Face B: Man, beard, blue shirt
â”œâ”€â”€ Speaker 3: Male voice      â†â†’   Face C: Man, bald
â””â”€â”€ Unknown speaker            â†â†’   Face D: Person partially visible</p>
<p>Matched Timeline:
00:00 - 02:15: Face A (Speaker 1) presenting, others listening
02:16 - 05:30: Face B (Speaker 2) asking questions
05:31 - 08:45: Discussion between Speakers 1, 2, and 3
08:46 - 10:00: Face D speaks briefly (voice now captured for future matching)
</code></pre></p>
<strong>Sentiment Fusion:</strong>
<pre><code class="language-yaml">Timestamp: 3:45
Audio sentiment: Positive (words: &quot;great&quot;, &quot;love it&quot;, &quot;perfect&quot;)
Visual sentiment:
  <li>Speaker: Smiling, nodding, open posture</li>
  <li>Listeners: Engaged, leaning forward</li>
Fused assessment: Genuine enthusiasm, high engagement
<p>Timestamp: 7:22
Audio sentiment: Positive (words: &quot;sure&quot;, &quot;sounds good&quot;)
Visual sentiment:
  <li>Speaker: Slight frown, arms crossed, looking away</li>
Fused assessment: Possible disagreement or reluctance despite agreeable words
âš ï¸ Flag: Audio/visual mismatch detected
</code></pre></p>
<strong>Context Enhancement:</strong>
<li>Transcript: "Put it over there" â†’ Video shows: pointing at shelf â†’ Enhanced: "Put it on the wooden shelf by the window"</li>
<li>Transcript: "This is amazing" â†’ Video shows: reaction to laptop screen â†’ Enhanced: "Reacting to [website/app visible on screen]"</li>
<li>Transcript: "[laughter]" â†’ Video shows: dog doing trick â†’ Enhanced: "Laughing at dog's failed backflip attempt"</li>
<h4>Screen Recording Special Processing</h4>
<p>Screen recordings are information-dense and deserve specialized treatment:</p>
<strong>Multi-Layer OCR:</strong>
<pre><code class="language-">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Browser: Chrome                                 â”‚ â† App detection
â”‚ URL: github.com/user/repo/pull/123             â”‚ â† URL extraction
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Pull Request #123: Fix auth bug             â”‚â”‚ â† Page title
â”‚ â”‚                                             â”‚â”‚
â”‚ â”‚ Files changed: 3                            â”‚â”‚ â† Key content
â”‚ â”‚ +47 -12 lines                               â”‚â”‚
â”‚ â”‚                                             â”‚â”‚
â”‚ â”‚ src/auth/login.ts                           â”‚â”‚ â† File paths
â”‚ â”‚ </code></pre>                                         â”‚â”‚
â”‚ â”‚ function validateToken(token: string) {    â”‚â”‚ â† Code extraction
â”‚ â”‚   if (!token) throw new Error('...')       â”‚â”‚
â”‚ â”‚ ``<code>                                         â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚ Cursor: clicking "Approve" button              â”‚ â† Action detection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
<pre><code class="language-">
<strong>Tutorial/Walkthrough Extraction:</strong>
</code></pre>markdown
<h2>Tutorial: Setting up Authentication</h2>
<em>Extracted from screen recording (12:34)</em>
<h3>Step 1: Install dependencies (0:00-0:45)</h3>
<pre><code class="language-bash">npm install passport passport-local bcrypt
</code></pre>
<em>Narrator: "First we need to install our authentication packages"</em>
<h3>Step 2: Create user model (0:46-2:30)</h3>
<em>File: src/models/user.ts</em>
<pre><code class="language-typescript">interface User {
  id: string;
  email: string;
  passwordHash: string;
}
</code></pre>
<em>Narrator: "Now let's create our user model with the fields we need"</em>
<h3>Step 3: Configure passport (2:31-5:15)</h3>
<em>File: src/config/passport.ts</em>
...
<h3>Tools & Technologies Used:</h3>
<li>VS Code (editor)</li>
<li>Terminal (iTerm2)</li>
<li>Browser (Chrome)</li>
<li>Node.js, TypeScript, Passport.js</li>
<h3>Generated Tags:</h3>
#tutorial #authentication #nodejs #typescript #passport #coding #programming
<pre><code class="language-">
<strong>Screen Recording Timeline:</strong>
</code></pre>
00:00 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 12:34
â”‚     â”‚         â”‚              â”‚          â”‚         â”‚
â”‚ VS Code      Chrome        Terminal   VS Code    Chrome
â”‚ Editing    Docs lookup    npm install  More     Testing
â”‚ user.ts    passport.js               editing    in browser
<p>Apps used: VS Code (65%), Chrome (25%), Terminal (10%)
Files edited: src/models/user.ts, src/config/passport.ts, src/routes/auth.ts
URLs visited: passportjs.org/docs, stackoverflow.com/questions/...
Commands run: npm install ..., npm run dev, npm test
<pre><code class="language-">
<h4>Comprehensive Tag Generation from Video</h4></p>
<strong>Example - Home cooking video (8 minutes):</strong>
</code></pre>
Video: Person cooking pasta dish in home kitchen
<p>Generated Tags (with weights):</p>
<p>Content Type:
<li>cooking video (0.97)</li>
<li>home video (0.89)</li>
<li>tutorial style (0.72)</li>
<li>POV cooking (0.85)</li></p>
<p>Setting:
<li>kitchen (0.98)</li>
<li>home kitchen (0.94)</li>
<li>indoor (0.99)</li>
<li>modern kitchen (0.78)</li>
<li>granite countertops (0.71)</li></p>
<p>Food & Ingredients:
<li>pasta (0.96)</li>
<li>spaghetti (0.89)</li>
<li>tomato sauce (0.94)</li>
<li>garlic (0.88)</li>
<li>olive oil (0.91)</li>
<li>parmesan cheese (0.85)</li>
<li>basil (0.82)</li>
<li>italian cooking (0.90)</li></p>
<p>Actions:
<li>cooking (0.98)</li>
<li>boiling water (0.92)</li>
<li>chopping (0.88)</li>
<li>sautÃ©ing (0.85)</li>
<li>stirring (0.91)</li>
<li>plating (0.79)</li>
<li>garnishing (0.75)</li></p>
<p>Equipment:
<li>stove (0.95)</li>
<li>pot (0.93)</li>
<li>pan (0.91)</li>
<li>cutting board (0.88)</li>
<li>knife (0.86)</li>
<li>wooden spoon (0.82)</li></p>
<p>Audio-derived:
<li>recipe explanation (0.88)</li>
<li>cooking tips (0.76)</li>
<li>timing instructions (0.81)</li></p>
<p>Inferred:
<li>dinner preparation (0.84)</li>
<li>weeknight meal (0.65)</li>
<li>beginner-friendly (0.71)</li>
<li>30-minute meal (0.68)</li>
<pre><code class="language-">
<h4>Multiple Output Representations from Video</h4></p>
<table>
<tr><th>Representation</th><th>Format</th><th>Use Case</th></tr>
<tr><td><strong>Full Transcript</strong></td><td>markdown</td><td>Searchable text, what was said</td></tr>
<tr><td><strong>Visual Summary</strong></td><td>markdown</td><td>What was shown, scene by scene</td></tr>
<tr><td><strong>Fused Summary</strong></td><td>markdown</td><td>Combined audio+visual narrative</td></tr>
<tr><td><strong>Chapter Breakdown</strong></td><td>markdown + metadata</td><td>Navigable sections with thumbnails</td></tr>
<tr><td><strong>Key Moments</strong></td><td>metadata (timestamps)</td><td>Highlights, important points</td></tr>
<tr><td><strong>Tutorial Steps</strong></td><td>markdown</td><td>For how-to content</td></tr>
<tr><td><strong>Slide Deck</strong></td><td>PDF/images</td><td>Extracted presentation slides</td></tr>
<tr><td><strong>Quote Reel</strong></td><td>metadata (timestamps)</td><td>Notable quotes with speaker faces</td></tr>
<tr><td><strong>Action Items</strong></td><td>markdown</td><td>Extracted from meeting content</td></tr>
<tr><td><strong>Thumbnail Set</strong></td><td>images</td><td>Multiple options, not just first frame</td></tr>
<tr><td><strong>GIF Highlights</strong></td><td>GIF assets</td><td>Auto-generated highlight clips</td></tr>
<tr><td><strong>Subtitles</strong></td><td>SRT/VTT</td><td>For accessibility and search</td></tr>
<tr><td><strong>Shot List</strong></td><td>markdown</td><td>Scene-by-scene breakdown</td></tr>
</table>
<strong>Example Multi-Representation Output:</strong>
</code></pre>markdown
<h2>Video Summary: Team Retrospective Meeting</h2>
<em>Duration: 45:23 | Participants: 4 | Type: Meeting Recording</em>
<h3>Visual Summary</h3>
The meeting takes place in a conference room with a whiteboard visible.
Four participants are seated around a table. Sarah (blonde, glasses)
leads the meeting, frequently gesturing to the whiteboard where sprint
metrics are written. The mood appears collaborative with frequent nodding
and engaged postures. At 23:45, John (beard) draws a diagram on the
whiteboard illustrating the proposed process change.
<h3>Fused Narrative Summary</h3>
The team conducted their bi-weekly retrospective focusing on the
completed authentication sprint. Sarah presented metrics showing 15%
improvement in velocity. Discussion became animated around 12:00 when
Mike raised concerns about technical debt - his crossed arms and furrowed
brow suggested stronger feelings than his measured words indicated. The
team reached consensus on allocating 20% of next sprint to debt reduction,
with visible relief from Mike (shoulders relaxed, smiled for first time).
<h3>Chapter Breakdown</h3>
<table>
<tr><th>Chapter</th><th>Time</th><th>Thumbnail</th><th>Summary</th></tr>
<tr><td>Opening</td><td>0:00-2:15</td><td>[thumb1]</td><td>Sarah welcomes team, sets agenda</td></tr>
<tr><td>Metrics Review</td><td>2:16-10:30</td><td>[thumb2]</td><td>Sprint velocity, bugs fixed, PRs merged</td></tr>
<tr><td>What Went Well</td><td>10:31-18:45</td><td>[thumb3]</td><td>Auth launch success, team collaboration</td></tr>
<tr><td>Challenges</td><td>18:46-30:20</td><td>[thumb4]</td><td>Tech debt discussion, deployment issues</td></tr>
<tr><td>Action Items</td><td>30:21-40:00</td><td>[thumb5]</td><td>Next sprint planning, assignments</td></tr>
<tr><td>Wrap-up</td><td>40:01-45:23</td><td>[thumb6]</td><td>Summary, next meeting scheduled</td></tr>
</table>
<h3>Key Moments</h3>
<li><strong>12:15</strong> - Mike raises tech debt concern (tension visible)</li>
<li><strong>23:45</strong> - John's whiteboard diagram explaining new process</li>
<li><strong>35:30</strong> - Team votes on sprint allocation (unanimous)</li>
<li><strong>42:00</strong> - Laughter when Sarah jokes about "deployment Fridays"</li>
<h3>Action Items Extracted</h3>
<li>[ ] <strong>John</strong>: Create tech debt tracking board by Monday</li>
<li>[ ] <strong>Mike</strong>: Document deployment checklist</li>
<li>[ ] <strong>Sarah</strong>: Schedule deep-dive on caching issues</li>
<li>[ ] <strong>Team</strong>: Review and estimate debt items before planning</li>
<h3>Whiteboard Content Extracted</h3>
<pre><code class="language-">Sprint 14 Metrics:
<li>Velocity: 47 pts (â†‘15%)</li>
<li>Bugs: 12 fixed, 3 new</li>
<li>PRs: 28 merged</li>
<li>Tech Debt: 23 items (â†‘5)</li>
<p>Proposed Allocation:
Features: 60%
Bugs: 20%
Tech Debt: 20%
</code></pre></p>
<h3>Generated Subtitles (SRT)</h3>
[Attached: retrospective-meeting.srt]
<pre><code class="language-">
<h4>Intelligent Thumbnail Selection</h4>
<p>Don't just use the first frame - select thumbnails that:</p>
<strong>Selection Criteria:</strong>
<table>
<tr><th>Factor</th><th>Weight</th><th>Description</th></tr>
<tr><td><strong>Face visibility</strong></td><td>High</td><td>Frames with clear, well-lit faces</td></tr>
<tr><td><strong>Action peak</strong></td><td>High</td><td>Most dynamic moment of motion</td></tr>
<tr><td><strong>Visual clarity</strong></td><td>High</td><td>Sharp focus, good exposure</td></tr>
<tr><td><strong>Representative</strong></td><td>Medium</td><td>Captures video's main subject/theme</td></tr>
<tr><td><strong>Emotional peak</strong></td><td>Medium</td><td>Expressions, reactions</td></tr>
<tr><td><strong>Text clarity</strong></td><td>Medium</td><td>For tutorials, when important text visible</td></tr>
<tr><td><strong>Avoid</strong></td><td>Negative</td><td>Blurry frames, mid-blink, transitions</td></tr>
</table>
<strong>Output:</strong>
</code></pre>yaml
Thumbnail Options:
  <li>primary: frame_1847 (3:45) - Main subject smiling, good lighting</li>
  <li>action: frame_3621 (7:12) - Peak action moment</li>
  <li>group: frame_892 (1:23) - All participants visible</li>
  <li>content: frame_2103 (4:22) - Whiteboard clearly visible</li>
<pre><code class="language-">
<h4>Multi-Part Video Handling</h4>
<p>Similar to audio, handle split recordings:</p>
<strong>Detection:</strong>
<li>Sequential filenames (video_part1.mp4, video_part2.mp4)</li>
<li>Continuous timecodes</li>
<li>Same visual environment</li>
<li>Matching audio characteristics</li>
<li>Content continuity</li>
<strong>Reconstruction:</strong>
<li>Unified transcript across parts</li>
<li>Continuous chapter numbering</li>
<li>Merged speaker timelines</li>
<li>Single cohesive summary</li>
<li>Links maintained to original segments</li>
<h4>Long-Form Video Processing (1+ hours)</h4>
<p>For lengthy videos (lectures, conferences, livestreams):</p>
<strong>Processing Strategy:</strong>
</code></pre>
Input: 3-hour conference recording
<p>Tier 1 (Immediate):
â”œâ”€â”€ Audio extraction
â”œâ”€â”€ Sparse frame sampling (every 60s)
â”œâ”€â”€ Basic scene detection
â””â”€â”€ Quick content classification</p>
<p>Tier 2 (Background):
â”œâ”€â”€ Full transcription
â”œâ”€â”€ Dense frame analysis at scene boundaries
â”œâ”€â”€ Speaker identification
â””â”€â”€ Chapter detection</p>
<p>Tier 3 (On-Demand):
â”œâ”€â”€ Detailed analysis of flagged sections
â”œâ”€â”€ Deep visual analysis of key moments
â””â”€â”€ Full motion analysis for action segments</p>
<p>Output:
â”œâ”€â”€ Quick summary available in ~5 minutes
â”œâ”€â”€ Full transcript in ~30 minutes
â”œâ”€â”€ Complete analysis in ~2 hours
â””â”€â”€ User notified at each stage
<pre><code class="language-">
<strong>Chaptering for Long Videos:</strong>
</code></pre>markdown
<h2>Conference Keynote: The Future of AI</h2>
<em>Duration: 2:34:17 | Auto-chaptered into 12 sections</em></p>
<h3>Chapters</h3>
1. <strong>Introduction & Welcome</strong> (0:00-8:45)
   <li>Speaker: Conference MC</li>
   <li>Summary: Event opening, sponsor acknowledgments</li>
<p>2. <strong>Keynote: AI in 2025</strong> (8:46-45:30)
   <li>Speaker: Dr. Jane Smith (CEO, AITech)</li>
   <li>Key topics: GPT-5 capabilities, enterprise adoption</li>
   <li>Notable quote @ 23:15: "AI won't replace jobs, it will replace tasks"</li></p>
<p>3. <strong>Live Demo: New Product</strong> (45:31-1:02:15)
   <li>Speakers: Product team</li>
   <li>Demo of: Real-time translation feature</li>
   <li>Audience reaction: Applause at 58:30</li></p>
<p>4. <strong>Panel Discussion</strong> (1:02:16-1:45:00)
   <li>Panelists: 4 industry experts</li>
   <li>Topics: Ethics, regulation, workforce impact</li></p>
<p>...</p>
<h3>Quick Navigation</h3>
[Jump to Demo] [Jump to Q&A] [View All Quotes] [Download Slides]
<pre><code class="language-">
<h4>Enrichment Types Table</h4>
<table>
<tr><th>Enrichment</th><th>Output Type</th><th>Description</th></tr>
<tr><td><strong>Audio Extraction</strong></td><td>Asset + Pipeline</td><td>Separate audio â†’ full audio enrichment</td></tr>
<tr><td><strong>Video Type Detection</strong></td><td>Metadata</td><td>Screen recording, meeting, vlog, tutorial, etc.</td></tr>
<tr><td><strong>Scene Segmentation</strong></td><td>Metadata + Representation</td><td>Logical scene boundaries with descriptions</td></tr>
<tr><td><strong>Visual Summary</strong></td><td>Representation (markdown)</td><td>What's shown, scene by scene</td></tr>
<tr><td><strong>Fused Summary</strong></td><td>Representation (markdown)</td><td>Combined audio+visual narrative</td></tr>
<tr><td><strong>Transcription</strong></td><td>Representation (markdown)</td><td>Full transcript synced to video</td></tr>
<tr><td><strong>Subtitle Generation</strong></td><td>Asset (SRT/VTT)</td><td>Accessibility captions</td></tr>
<tr><td><strong>Chapter Detection</strong></td><td>Metadata + Representation</td><td>Navigable sections with thumbnails</td></tr>
<tr><td><strong>Intelligent Thumbnails</strong></td><td>Assets (images)</td><td>Multiple quality thumbnail options</td></tr>
<tr><td><strong>Key Moments</strong></td><td>Metadata</td><td>Highlights, important timestamps</td></tr>
<tr><td><strong>GIF Extraction</strong></td><td>Assets (GIFs)</td><td>Auto-generated highlight clips</td></tr>
<tr><td><strong>Speaker-Face Matching</strong></td><td>Metadata</td><td>Who said what with visual confirmation</td></tr>
<tr><td><strong>Face Timeline</strong></td><td>Metadata</td><td>When each person appears</td></tr>
<tr><td><strong>Expression Analysis</strong></td><td>Metadata</td><td>Emotional states over time</td></tr>
<tr><td><strong>Activity Recognition</strong></td><td>Tags</td><td>What people are doing</td></tr>
<tr><td><strong>Object Detection</strong></td><td>Tags + Metadata</td><td>Items visible throughout</td></tr>
<tr><td><strong>Setting Analysis</strong></td><td>Tags</td><td>Location, environment, room type</td></tr>
<tr><td><strong>Screen Content Extraction</strong></td><td>Representation</td><td>OCR, app detection, URLs, code</td></tr>
<tr><td><strong>Tutorial Step Extraction</strong></td><td>Representation (markdown)</td><td>Step-by-step for how-to videos</td></tr>
<tr><td><strong>Slide Extraction</strong></td><td>Assets (images/PDF)</td><td>Presentation slides from video</td></tr>
<tr><td><strong>Motion Analysis</strong></td><td>Metadata</td><td>Action intensity, movement patterns</td></tr>
<tr><td><strong>Audio-Visual Sentiment</strong></td><td>Metadata</td><td>Fused emotional analysis</td></tr>
<tr><td><strong>Whiteboard/Document OCR</strong></td><td>Representation</td><td>Text from physical surfaces in video</td></tr>
<tr><td><strong>Brand/Logo Detection</strong></td><td>Tags</td><td>Products and companies visible</td></tr>
<tr><td><strong>Multi-Part Reconstruction</strong></td><td>Representation</td><td>Merge split video recordings</td></tr>
<tr><td><strong>Long-Form Chaptering</strong></td><td>Metadata + Representation</td><td>Auto-chapter 1+ hour videos</td></tr>
</table>
<h3>4. Document Enrichments (PDF, Office docs, Text files)</h3>
<h4>Core Philosophy: Structured Intelligence Extraction</h4>
<p>Documents are dense with structured information. The agent should:
<li><strong>Preserve structure</strong> (headings, sections, tables, lists)</li>
<li><strong>Extract key data</strong> (dates, amounts, names, entities)</li>
<li><strong>Classify document type</strong> for specialized processing</li>
<li><strong>Identify actionable content</strong> (deadlines, commitments, decisions)</li>
<li><strong>Detect relationships</strong> to existing entries, projects, people</li>
<li><strong>Assess confidentiality</strong> (PII, financial, legal sensitivity)</li></p>
<h4>Document Type Detection &amp; Specialized Processing</h4>
<table>
<tr><th>Document Type</th><th>Detection Signals</th><th>Special Processing</th></tr>
<tr><td><strong>Invoice/Bill</strong></td><td>&quot;Invoice&quot;, amounts, due dates, line items</td><td>Amount extraction, due date â†’ Waiting For, vendor linking</td></tr>
<tr><td><strong>Receipt</strong></td><td>Store name, items, total, date</td><td>Expense categorization, Shopping bucket completion</td></tr>
<tr><td><strong>Contract/Agreement</strong></td><td>Signature lines, legal language, parties</td><td>Key dates extraction, obligation tracking, party linking</td></tr>
<tr><td><strong>Resume/CV</strong></td><td>Education, experience sections, skills</td><td>Contact creation, skill tagging</td></tr>
<tr><td><strong>Meeting Notes</strong></td><td>Attendees, date, agenda, action items</td><td>Action item â†’ Next Actions, attendee linking</td></tr>
<tr><td><strong>Academic Paper</strong></td><td>Abstract, citations, methodology</td><td>Citation extraction, research linking, topic tagging</td></tr>
<tr><td><strong>Manual/Documentation</strong></td><td>TOC, procedures, technical content</td><td>Section indexing, product/project linking</td></tr>
<tr><td><strong>Letter/Correspondence</strong></td><td>Salutation, date, signature</td><td>Sender/recipient linking, response tracking</td></tr>
<tr><td><strong>Form (Filled)</strong></td><td>Field labels, filled values</td><td>Key-value extraction, form type classification</td></tr>
<tr><td><strong>Form (Blank)</strong></td><td>Field labels, empty fields</td><td>Template detection, form purpose</td></tr>
<tr><td><strong>Presentation Slides</strong></td><td>Slide structure, bullet points</td><td>Slide-by-slide extraction, speaker notes</td></tr>
<tr><td><strong>Spreadsheet</strong></td><td>Rows, columns, formulas</td><td>Table extraction, data summarization</td></tr>
<tr><td><strong>Legal Document</strong></td><td>Legal terminology, clauses, whereas</td><td>Clause extraction, obligation tracking</td></tr>
<tr><td><strong>Medical Record</strong></td><td>Patient info, diagnoses, medications</td><td>PHI detection, health tracking (with consent)</td></tr>
<tr><td><strong>Financial Statement</strong></td><td>Account balances, transactions</td><td>Financial data extraction, account linking</td></tr>
<tr><td><strong>Email (exported)</strong></td><td>Headers, thread structure</td><td>Sender/recipient linking, thread reconstruction</td></tr>
<tr><td><strong>Handwritten Notes (scanned)</strong></td><td>Handwriting patterns</td><td>Handwriting OCR, structure inference</td></tr>
<tr><td><strong>Book/Ebook</strong></td><td>Chapters, ISBN, author</td><td>Chapter indexing, reading progress</td></tr>
<tr><td><strong>Code/Technical Doc</strong></td><td>Code blocks, API references</td><td>Language detection, function extraction</td></tr>
</table>
<h4>Deep Structure Extraction</h4>
<strong>Beyond Plain Text - Preserve Document Intelligence:</strong>
</code></pre>
Original Document Structure:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUARTERLY BUSINESS REVIEW                                     Page 1 of 12  â”‚
â”‚ Acme Corporation                                                            â”‚
â”‚ Q4 2025                                                                     â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                             â”‚
â”‚ 1. Executive Summary                                                        â”‚
â”‚    Revenue increased 15% YoY to $2.3M. Key wins include...                 â”‚
â”‚                                                                             â”‚
â”‚ 2. Financial Highlights                                                     â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚    â”‚ Metric       â”‚ Q4 2025  â”‚ Q4 2024  â”‚ Change   â”‚                       â”‚
â”‚    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                       â”‚
â”‚    â”‚ Revenue      â”‚ $2.3M    â”‚ $2.0M    â”‚ +15%     â”‚                       â”‚
â”‚    â”‚ Expenses     â”‚ $1.8M    â”‚ $1.7M    â”‚ +6%      â”‚                       â”‚
â”‚    â”‚ Net Income   â”‚ $500K    â”‚ $300K    â”‚ +67%     â”‚                       â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                                                                             â”‚
â”‚ 3. Key Initiatives                                                          â”‚
â”‚    3.1 Product Launch                                                       â”‚
â”‚        â€¢ Launched v2.0 in October                                          â”‚
â”‚        â€¢ 500 new customers acquired                                         â”‚
â”‚    3.2 Market Expansion                                                     â”‚
â”‚        â€¢ Entered European market                                            â”‚
â”‚        â€¢ Partnership with DistributorCo signed                             â”‚
â”‚                                                                             â”‚
â”‚ 4. Action Items for Q1 2026                                                â”‚
â”‚    â–¡ Finalize Series B funding (Due: Feb 15)                               â”‚
â”‚    â–¡ Hire VP of Sales (Due: Jan 31)                                        â”‚
â”‚    â–¡ Launch mobile app beta (Due: Mar 1)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
<p>Extracted Structure:
â”œâ”€â”€ Document Metadata
â”‚   â”œâ”€â”€ Type: Business Report (Quarterly Review)
â”‚   â”œâ”€â”€ Organization: Acme Corporation
â”‚   â”œâ”€â”€ Period: Q4 2025
â”‚   â””â”€â”€ Pages: 12
â”‚
â”œâ”€â”€ Section Hierarchy
â”‚   â”œâ”€â”€ 1. Executive Summary
â”‚   â”œâ”€â”€ 2. Financial Highlights
â”‚   â”‚   â””â”€â”€ [TABLE: Financial Metrics Comparison]
â”‚   â”œâ”€â”€ 3. Key Initiatives
â”‚   â”‚   â”œâ”€â”€ 3.1 Product Launch
â”‚   â”‚   â””â”€â”€ 3.2 Market Expansion
â”‚   â””â”€â”€ 4. Action Items for Q1 2026
â”‚       â””â”€â”€ [CHECKLIST: 3 items with due dates]
â”‚
â”œâ”€â”€ Extracted Tables
â”‚   â””â”€â”€ Table 1: Financial Metrics
â”‚       â”œâ”€â”€ Revenue: $2.3M (+15% YoY)
â”‚       â”œâ”€â”€ Expenses: $1.8M (+6% YoY)
â”‚       â””â”€â”€ Net Income: $500K (+67% YoY)
â”‚
â”œâ”€â”€ Extracted Action Items
â”‚   â”œâ”€â”€ "Finalize Series B funding" - Due: 2026-02-15
â”‚   â”œâ”€â”€ "Hire VP of Sales" - Due: 2026-01-31
â”‚   â””â”€â”€ "Launch mobile app beta" - Due: 2026-03-01
â”‚
â”œâ”€â”€ Named Entities
â”‚   â”œâ”€â”€ Organizations: Acme Corporation, DistributorCo
â”‚   â”œâ”€â”€ Products: v2.0, mobile app
â”‚   â”œâ”€â”€ Amounts: $2.3M, $1.8M, $500K, $2.0M, $1.7M, $300K
â”‚   â””â”€â”€ Dates: Q4 2025, Q4 2024, October, Feb 15, Jan 31, Mar 1
â”‚
â””â”€â”€ Key Metrics
    â”œâ”€â”€ Revenue Growth: +15%
    â”œâ”€â”€ Customer Acquisition: 500 new
    â””â”€â”€ Markets: Entered European market
<pre><code class="language-">
<h4>Invoice/Receipt Special Processing</h4></p>
<p>Financial documents get specialized extraction:</p>
</code></pre>yaml
<h1>Invoice Detection & Extraction</h1>
Document Type: Invoice (confidence: 0.96)
<p>Extracted Fields:
  vendor:
    name: "CloudHost Inc"
    address: "123 Tech Street, San Francisco, CA 94102"
    contact_match: "vendor-cloudhost-uuid" (existing contact)</p>
<p>invoice_details:
    invoice_number: "INV-2026-0142"
    invoice_date: "2026-01-15"
    due_date: "2026-02-15"
    payment_terms: "Net 30"</p>
<p>line_items:
    <li>description: "Monthly hosting - Pro Plan"</li>
      quantity: 1
      unit_price: 299.00
      total: 299.00
    <li>description: "Additional bandwidth (50GB)"</li>
      quantity: 50
      unit_price: 0.10
      total: 5.00
    <li>description: "SSL Certificate renewal"</li>
      quantity: 1
      unit_price: 49.00
      total: 49.00</p>
<p>totals:
    subtotal: 353.00
    tax: 31.77
    total_due: 384.77
    currency: "USD"</p>
<h1>Life Steward Integration</h1>
life_steward_data:
  suggested_domain: "Business: Acme Inc"
  suggested_scope: "Finance"
  suggested_bucket: "reference"
<p># Create Waiting For if payment needed
  waiting_for:
    <li>text: "Pay CloudHost invoice INV-2026-0142"</li>
      amount: "$384.77"
      due_date: "2026-02-15"
      vendor: "vendor-cloudhost-uuid"</p>
<p># Financial tracking
  financial_data:
    type: "expense"
    category: "hosting"
    amount: 384.77
    recurring: true (detected from "Monthly")
    project_hint: "infrastructure costs"
<pre><code class="language-">
</code></pre>yaml
<h1>Receipt Detection & Extraction</h1>
Document Type: Receipt (confidence: 0.94)</p>
<p>Extracted Fields:
  merchant:
    name: "Home Depot"
    store_number: "4521"
    address: "456 Builder Way, Austin, TX"</p>
<p>transaction:
    date: "2026-01-27"
    time: "14:32"
    register: "12"
    transaction_id: "4521-0127-1432-8847"</p>
<p>items:
    <li>description: "Subway Tile White 3x6 (case)"</li>
      sku: "HDX-4421"
      quantity: 4
      price: 32.99
      total: 131.96
    <li>description: "Tile Adhesive 25lb"</li>
      sku: "HDX-8812"
      quantity: 2
      price: 24.99
      total: 49.98
    <li>description: "Grout - Frost White"</li>
      sku: "HDX-8834"
      quantity: 1
      price: 18.99
      total: 18.99</p>
<p>totals:
    subtotal: 200.93
    tax: 16.58
    total: 217.51
    payment_method: "Visa <strong>**4242"</p>
<h1>Life Steward Integration</h1>
life_steward_data:
  suggested_domain: "Personal"
  suggested_scope: "Home"
  suggested_project: "Kitchen Remodel" (matched from items)
  suggested_bucket: "reference"
<p># Complete shopping items
  completes_shopping:
    <li>item: "Tile samples from Home Depot"</li>
      status: "purchased"
      actual_cost: 131.96</p>
<p># Expense tracking
  financial_data:
    type: "expense"
    category: "home improvement"
    project: "kitchen-remodel-uuid"
    amount: 217.51
    deductible: false</p>
<p># Auto-tags
  tags:
    <li>"receipt"</li>
    <li>"home depot"</li>
    <li>"kitchen remodel"</li>
    <li>"tile"</li>
    <li>"materials"</li>
<pre><code class="language-">
<h4>Contract/Agreement Special Processing</h4></p>
<p>Legal documents require careful extraction:</p>
</code></pre>yaml
Document Type: Contract (confidence: 0.91)
Subtype: Service Agreement
<p>Extracted Structure:
  parties:
    <li>role: "Client"</li>
      name: "Acme Corporation"
      contact_match: "business-acme-uuid"
    <li>role: "Service Provider"</li>
      name: "DesignStudio LLC"
      contact_match: null (suggest creating)</p>
<p>key_dates:
    <li>type: "effective_date"</li>
      date: "2026-02-01"
      description: "Agreement becomes effective"
    <li>type: "end_date"</li>
      date: "2026-07-31"
      description: "Initial term ends"
    <li>type: "renewal_notice"</li>
      date: "2026-06-30"
      description: "30 days notice required for non-renewal"
    <li>type: "milestone"</li>
      date: "2026-03-15"
      description: "First deliverable due"
    <li>type: "milestone"</li>
      date: "2026-05-01"
      description: "Second deliverable due"</p>
<p>financial_terms:
    total_value: 45000.00
    currency: "USD"
    payment_schedule:
      <li>amount: 15000.00</li>
        due: "2026-02-01"
        description: "Upon signing"
      <li>amount: 15000.00</li>
        due: "2026-03-15"
        description: "First milestone"
      <li>amount: 15000.00</li>
        due: "2026-05-01"
        description: "Final delivery"</p>
<p>key_obligations:
    client:
      <li>"Provide brand guidelines within 5 business days"</li>
      <li>"Feedback within 3 business days of deliverable"</li>
      <li>"Final approval within 5 business days"</li>
    provider:
      <li>"Weekly progress reports"</li>
      <li>"Deliverables per schedule"</li>
      <li>"2 rounds of revisions included"</li></p>
<p>termination_clauses:
    <li>"Either party may terminate with 30 days written notice"</li>
    <li>"Immediate termination for material breach"</li></p>
<p>confidentiality: true
  non_compete: false
  ip_ownership: "Client owns all deliverables"</p>
<h1>Life Steward Integration</h1>
life_steward_data:
  suggested_domain: "Business: Acme Inc"
  suggested_scope: "Operations"
  suggested_project: "Website Redesign" (inferred from "design" context)
  suggested_bucket: "reference"
<p># Critical dates â†’ Calendar/Timeline
  timeline_data:
    <li>date: "2026-02-01"</li>
      type: "project_start"
      description: "DesignStudio contract begins"
    <li>date: "2026-03-15"</li>
      type: "milestone"
      description: "First design deliverable due"
    <li>date: "2026-05-01"</li>
      type: "milestone"
      description: "Final design delivery"
    <li>date: "2026-06-30"</li>
      type: "deadline"
      description: "Contract renewal notice deadline"</p>
<p># Payment obligations â†’ Waiting For / Next Actions
  action_items:
    <li>text: "Pay DesignStudio signing fee"</li>
      amount: "$15,000"
      due_date: "2026-02-01"
      type: "payment"</p>
<p>waiting_for:
    <li>text: "First deliverable from DesignStudio"</li>
      due_date: "2026-03-15"
      from: "DesignStudio LLC"</p>
<p># Obligations â†’ Tasks
  obligations:
    <li>text: "Provide brand guidelines to DesignStudio"</li>
      due_date: "2026-02-06" (5 business days from effective)
      context: "@computer"</p>
<p># Create contact if needed
  suggest_contact:
    name: "DesignStudio LLC"
    type: "vendor"
    relationship: "service provider"
<pre><code class="language-">
<h4>Meeting Notes / Minutes Special Processing</h4></p>
<p>Meeting documents are rich with actionable content:</p>
</code></pre>yaml
Document Type: Meeting Notes (confidence: 0.93)
<p>Extracted Structure:
  meeting_metadata:
    title: "Product Roadmap Planning"
    date: "2026-01-27"
    time: "10:00 AM - 11:30 AM"
    location: "Conference Room B / Zoom"
    organizer: "Sarah Chen"</p>
<p>attendees:
    present:
      <li>name: "Sarah Chen"</li>
        role: "Product Manager"
        contact_match: "contact-sarah-uuid"
      <li>name: "John Smith"</li>
        role: "Engineering Lead"
        contact_match: "contact-john-uuid"
      <li>name: "Mike Johnson"</li>
        role: "Designer"
        contact_match: "contact-mike-uuid"
    absent:
      <li>name: "Lisa Wang"</li>
        reason: "Out sick"</p>
<p>agenda_items:
    <li>topic: "Q1 Feature Prioritization"</li>
      discussion: "Reviewed 12 feature requests, narrowed to top 5..."
      decisions:
        <li>"Mobile app notifications is P1"</li>
        <li>"Dashboard redesign moved to Q2"</li>
      action_items:
        <li>assignee: "John"</li>
          task: "Create technical spec for notifications"
          due: "2026-02-03"
        <li>assignee: "Mike"</li>
          task: "Wireframes for notification UI"
          due: "2026-02-05"</p>
<li>topic: "Resource Allocation"</li>
      discussion: "Need additional frontend developer..."
      decisions:
        <li>"Approved headcount for 1 senior frontend dev"</li>
      action_items:
        <li>assignee: "Sarah"</li>
          task: "Open job posting"
          due: "2026-01-31"
        <li>assignee: "Sarah"</li>
          task: "Contact recruiting agency"
          due: "2026-01-29"
<li>topic: "Timeline Review"</li>
      discussion: "Current timeline at risk due to API delays..."
      decisions:
        <li>"Push launch from March 1 to March 15"</li>
      action_items:
        <li>assignee: "Sarah"</li>
          task: "Update stakeholders on new timeline"
          due: "2026-01-28"
        <li>assignee: "John"</li>
          task: "Revised engineering schedule"
          due: "2026-02-01"
<p>parking_lot:
    <li>"Discuss analytics integration in future meeting"</li>
    <li>"Review competitor's new feature"</li></p>
<p>next_meeting: "2026-02-03 10:00 AM"</p>
<h1>Life Steward Integration</h1>
life_steward_data:
  suggested_domain: "Business: Acme Inc"
  suggested_scope: "Product"
  suggested_project: "Q1 Product Launch"
  suggested_bucket: "reference"
<p># All action items â†’ Next Actions
  action_items:
    <li>text: "Create technical spec for notifications"</li>
      assignee: "contact-john-uuid"
      due_date: "2026-02-03"
      context: "@computer"
      source: "Product Roadmap Planning meeting"
    <li>text: "Wireframes for notification UI"</li>
      assignee: "contact-mike-uuid"
      due_date: "2026-02-05"
    <li>text: "Open job posting"</li>
      assignee: "contact-sarah-uuid"
      due_date: "2026-01-31"
    <li>text: "Contact recruiting agency"</li>
      assignee: "contact-sarah-uuid"
      due_date: "2026-01-29"
    <li>text: "Update stakeholders on new timeline"</li>
      assignee: "contact-sarah-uuid"
      due_date: "2026-01-28"
      priority: "high" (immediate)
    <li>text: "Revised engineering schedule"</li>
      assignee: "contact-john-uuid"
      due_date: "2026-02-01"</p>
<p># Decisions â†’ Decisions bucket
  decisions:
    <li>text: "Mobile app notifications is P1 for Q1"</li>
      date: "2026-01-27"
      participants: ["Sarah", "John", "Mike"]
    <li>text: "Dashboard redesign moved to Q2"</li>
      date: "2026-01-27"
    <li>text: "Approved headcount for senior frontend dev"</li>
      date: "2026-01-27"
    <li>text: "Launch date moved from March 1 to March 15"</li>
      date: "2026-01-27"
      impacts_timeline: true</p>
<p># Timeline impacts
  timeline_data:
    <li>type: "deadline_change"</li>
      project: "Q1 Product Launch"
      old_date: "2026-03-01"
      new_date: "2026-03-15"
      reason: "API delays"</p>
<p># Parking lot â†’ Someday/Maybe
  someday_maybe:
    <li>"Discuss analytics integration"</li>
    <li>"Review competitor's new feature"</li></p>
<p># @agenda items for attendees
  agenda_items:
    <li>person: "contact-john-uuid"</li>
      topic: "Follow up on technical spec"
    <li>person: "contact-mike-uuid"</li>
      topic: "Review wireframe progress"</p>
<p># People linking
  people_detected:
    <li>contact_id: "contact-sarah-uuid"</li>
      role: "organizer"
      action_items_assigned: 3
    <li>contact_id: "contact-john-uuid"</li>
      role: "participant"
      action_items_assigned: 2
    <li>contact_id: "contact-mike-uuid"</li>
      role: "participant"
      action_items_assigned: 1
<pre><code class="language-">
<h4>Academic Paper / Research Document Processing</h4></p>
</code></pre>yaml
Document Type: Academic Paper (confidence: 0.94)
<p>Extracted Structure:
  bibliographic:
    title: "Machine Learning Approaches to Natural Language Understanding"
    authors:
      <li>name: "Jane Doe, PhD"</li>
        affiliation: "Stanford University"
      <li>name: "John Smith, PhD"</li>
        affiliation: "MIT"
    publication:
      journal: "Nature Machine Intelligence"
      volume: 8
      issue: 3
      pages: "234-251"
      date: "2026-01"
      doi: "10.1038/s42256-026-0142-5"
    keywords: ["NLP", "transformers", "language models", "deep learning"]</p>
<p>abstract: |
    This paper presents a comprehensive survey of machine learning
    approaches to natural language understanding, with focus on
    transformer architectures and their applications...</p>
<p>sections:
    <li>title: "1. Introduction"</li>
      summary: "Overview of NLU challenges and ML approaches"
    <li>title: "2. Background"</li>
      subsections:
        <li>"2.1 Traditional NLP Methods"</li>
        <li>"2.2 Neural Network Approaches"</li>
        <li>"2.3 Transformer Architecture"</li>
    <li>title: "3. Methodology"</li>
      summary: "Comparative analysis framework..."
    <li>title: "4. Results"</li>
      key_findings:
        <li>"Transformer models outperform LSTM by 15% on benchmark"</li>
        <li>"Fine-tuning requires 10x less data than training from scratch"</li>
    <li>title: "5. Discussion"</li>
    <li>title: "6. Conclusion"</li>
      summary: "Transformers represent state-of-the-art for NLU tasks..."</p>
<p>citations:
    total: 87
    key_references:
      <li>"Vaswani et al. (2017) - Attention Is All You Need"</li>
      <li>"Devlin et al. (2019) - BERT"</li>
      <li>"Brown et al. (2020) - GPT-3"</li></p>
<p>figures_tables:
    <li>type: "figure"</li>
      number: 1
      caption: "Transformer architecture overview"
    <li>type: "table"</li>
      number: 1
      caption: "Benchmark comparison results"
      extracted_data: {...}</p>
<p>key_contributions:
    <li>"Novel evaluation framework for NLU systems"</li>
    <li>"Comprehensive benchmark of 15 transformer variants"</li>
    <li>"Open-source implementation released"</li></p>
<h1>Life Steward Integration</h1>
life_steward_data:
  suggested_domain: "Personal"
  suggested_scope: "Learning & Education"
  suggested_project: "AI/ML Research" (if exists)
  suggested_bucket: "reference"
<p># Research tracking
  research_data:
    field: "Machine Learning / NLP"
    relevance_to_interests: ["AI/Machine Learning"] (from interest profile)
    relevance_score: 0.92</p>
<p># Key takeaways for learning
  key_takeaways:
    <li>"Transformers outperform LSTM by 15% on NLU benchmarks"</li>
    <li>"Fine-tuning needs 10x less data than full training"</li></p>
<p># Potential action items
  suggested_actions:
    <li>text: "Read referenced paper: Attention Is All You Need"</li>
      bucket: "someday_maybe"
    <li>text: "Try open-source implementation from paper"</li>
      bucket: "someday_maybe"</p>
<p># Tags
  tags:
    <li>"research paper"</li>
    <li>"machine learning"</li>
    <li>"NLP"</li>
    <li>"transformers"</li>
    <li>"academic"</li>
    <li>"Stanford"</li>
    <li>"MIT"</li></p>
<p># Citation for future reference
  citation_formatted:
    apa: "Doe, J., & Smith, J. (2026). Machine Learning Approaches..."
    bibtex: "@article{doe2026ml, ...}"
<pre><code class="language-">
<h4>Spreadsheet Special Processing</h4></p>
<p>Spreadsheets contain structured data that needs intelligent extraction:</p>
</code></pre>yaml
Document Type: Spreadsheet (confidence: 0.97)
Subtype: Budget Tracker
<p>Detected Structure:
  sheets:
    <li>name: "Monthly Budget"</li>
      type: "financial_tracker"
      structure:
        headers: ["Category", "Budget", "Actual", "Variance"]
        rows: 15
        has_totals: true
        has_formulas: true</p>
<li>name: "Transactions"</li>
      type: "transaction_log"
      structure:
        headers: ["Date", "Description", "Category", "Amount"]
        rows: 147
        date_range: "2026-01-01 to 2026-01-27"
<li>name: "Summary"</li>
      type: "dashboard"
      contains_charts: true
<p>Extracted Data:
  monthly_budget:
    period: "January 2026"
    categories:
      <li>category: "Housing"</li>
        budget: 2000.00
        actual: 2000.00
        variance: 0.00
      <li>category: "Food"</li>
        budget: 600.00
        actual: 523.45
        variance: 76.55
      <li>category: "Transportation"</li>
        budget: 400.00
        actual: 487.23
        variance: -87.23  # over budget
      <li>category: "Entertainment"</li>
        budget: 200.00
        actual: 312.00
        variance: -112.00  # over budget
    totals:
      total_budget: 4500.00
      total_actual: 4198.67
      net_variance: 301.33</p>
<p>insights:
    over_budget_categories: ["Transportation", "Entertainment"]
    under_budget_categories: ["Food", "Utilities"]
    spending_trend: "5% higher than last month"</p>
<h1>Life Steward Integration</h1>
life_steward_data:
  suggested_domain: "Personal"
  suggested_scope: "Finance"
  suggested_bucket: "reference"
<p># Financial insights â†’ potential tasks
  insights:
    <li>type: "over_budget_alert"</li>
      category: "Transportation"
      amount_over: 87.23
      suggested_action: "Review transportation expenses"
    <li>type: "over_budget_alert"</li>
      category: "Entertainment"
      amount_over: 112.00
      suggested_action: "Review entertainment spending"</p>
<p># Goal tracking (if budget goal exists)
  goal_relevance:
    <li>goal: "Stick to monthly budget"</li>
      status: "at_risk"
      reason: "2 categories over budget"</p>
<p># Recurring tracking
  financial_summary:
    month: "January 2026"
    total_spent: 4198.67
    vs_budget: "+301.33 under"
    categories_tracked: 8
<pre><code class="language-">
<h4>Scanned Document / Handwritten Notes Processing</h4></p>
<p>Scanned documents require OCR + layout analysis:</p>
</code></pre>yaml
Document Type: Scanned Handwritten Notes (confidence: 0.88)
<p>OCR Analysis:
  quality: "good" (85% confidence)
  language: "English"
  handwriting_style: "cursive, consistent"</p>
<p>Layout Detection:
  pages: 2
  structure:
    page_1:
      <li>type: "title"</li>
        text: "Meeting Notes - Project Kickoff"
        position: "top center"
      <li>type: "date"</li>
        text: "1/27/26"
        position: "top right"
      <li>type: "list"</li>
        items:
          <li>"Goals for Q1:"</li>
          <li>"  â€¢ Launch beta by March"</li>
          <li>"  â€¢ 100 users target"</li>
          <li>"  â€¢ Gather feedback"</li>
      <li>type: "diagram"</li>
        description: "Simple flowchart showing user journey"
        elements: ["Sign up", "Onboard", "Use", "Feedback"]
      <li>type: "note"</li>
        text: "Talk to Sarah about design timeline!"
        style: "circled, emphasized"</p>
<p>page_2:
      <li>type: "list"</li>
        heading: "Action Items"
        items:
          <li>"â–¡ Set up dev environment - John"</li>
          <li>"â–¡ Create wireframes - Mike"</li>
          <li>"â˜‘ Book conference room - Done"</li>
          <li>"â–¡ Send agenda to team"</li>
      <li>type: "note"</li>
        text: "Budget: ~$50K approved"
        style: "boxed"</p>
<p>Extracted Content:
  title: "Meeting Notes - Project Kickoff"
  date: "2026-01-27"</p>
<p>goals:
    <li>"Launch beta by March"</li>
    <li>"100 users target"</li>
    <li>"Gather feedback"</li></p>
<p>action_items:
    <li>task: "Set up dev environment"</li>
      assignee: "John"
      status: "pending"
    <li>task: "Create wireframes"</li>
      assignee: "Mike"
      status: "pending"
    <li>task: "Book conference room"</li>
      status: "completed"
    <li>task: "Send agenda to team"</li>
      assignee: "user" (inferred)
      status: "pending"</p>
<p>important_notes:
    <li>text: "Talk to Sarah about design timeline!"</li>
      emphasis: "high" (circled)
    <li>text: "Budget: ~$50K approved"</li>
      type: "financial"</p>
<p>diagrams:
    <li>description: "User journey flowchart: Sign up â†’ Onboard â†’ Use â†’ Feedback"</li></p>
<h1>Life Steward Integration</h1>
life_steward_data:
  suggested_project: "Project Kickoff" (new or match existing)
  suggested_bucket: "reference"
<p>action_items:
    <li>text: "Set up dev environment"</li>
      assignee: "contact-john-uuid"
      source: "handwritten notes"
    <li>text: "Create wireframes"</li>
      assignee: "contact-mike-uuid"
    <li>text: "Send agenda to team"</li>
      context: "@computer"</p>
<p># High-emphasis note â†’ immediate attention
  urgent_items:
    <li>text: "Talk to Sarah about design timeline"</li>
      add_to_agenda: "contact-sarah-uuid"
      priority: "high"</p>
<p># Financial note
  financial_data:
    type: "budget_approval"
    amount: 50000
    project: "Project Kickoff"
<pre><code class="language-">
<h4>Email Document Processing</h4></p>
<p>Exported emails (PDF, EML, MSG) get specialized treatment:</p>
</code></pre>yaml
Document Type: Email (confidence: 0.96)
<p>Extracted Structure:
  headers:
    from:
      name: "Bob Johnson"
      email: "bob@acmeclient.com"
      contact_match: "contact-bob-uuid"
    to:
      <li>name: "User"</li>
        email: "user@company.com"
    cc:
      <li>name: "Sarah Chen"</li>
        email: "sarah@company.com"
        contact_match: "contact-sarah-uuid"
    date: "2026-01-27 14:32:00"
    subject: "RE: Q1 Contract Renewal"</p>
<p>thread_context:
    is_reply: true
    thread_length: 4 messages
    original_subject: "Q1 Contract Renewal"</p>
<p>body:
    greeting: "Hi,"
    content: |
      Thanks for sending over the proposal. We've reviewed it internally
      and have a few questions:</p>
<p>1. Can we extend the payment terms to Net 45?
      2. Is there flexibility on the support hours?
      3. When would you need our final decision?</p>
<p>Also, just FYI - our legal team will need at least 2 weeks to
      review once we have the final version.</p>
<p>Looking forward to your response.
    signature:
      name: "Bob Johnson"
      title: "Director of Operations"
      company: "Acme Client Corp"
      phone: "555-123-4567"</p>
<p>attachments:
    <li>name: "Q1_Proposal_v2_redlines.pdf"</li>
      type: "application/pdf"
      size: "245 KB"</p>
<p>extracted_questions:
    <li>"Can we extend the payment terms to Net 45?"</li>
    <li>"Is there flexibility on the support hours?"</li>
    <li>"When would you need our final decision?"</li></p>
<p>extracted_info:
    <li>type: "timeline"</li>
      text: "Legal team needs 2 weeks for review"
    <li>type: "constraint"</li>
      text: "Net 45 payment terms requested"</p>
<p>sentiment: "professional, collaborative"
  urgency: "medium" (waiting for response)</p>
<h1>Life Steward Integration</h1>
life_steward_data:
  suggested_domain: "Business: Company"
  suggested_scope: "Sales"
  suggested_project: "Acme Client Contract"
  suggested_bucket: "reference"
<p># Response needed â†’ Next Action
  action_items:
    <li>text: "Respond to Bob's questions on Q1 contract"</li>
      context: "@computer"
      priority: "medium"
      questions_to_address:
        <li>"Net 45 payment terms"</li>
        <li>"Support hours flexibility"</li>
        <li>"Decision timeline"</li></p>
<p># Timeline constraint â†’ project planning
  timeline_data:
    <li>type: "external_constraint"</li>
      description: "Acme legal needs 2 weeks for review"
      factor_into: "contract finalization timeline"</p>
<p># People linking
  people_detected:
    <li>contact_id: "contact-bob-uuid"</li>
      role: "sender"
      relationship: "client contact"
    <li>contact_id: "contact-sarah-uuid"</li>
      role: "cc'd"</p>
<p># Thread tracking
  thread_tracking:
    thread_id: "thread-q1-contract-uuid"
    status: "awaiting_user_response"
    last_message_from: "external"
<pre><code class="language-">
<h4>Multi-Document Package Processing</h4></p>
<p>Sometimes multiple documents form a logical package:</p>
</code></pre>yaml
Package Detection:
  trigger: "Multiple documents uploaded together"
  documents:
    <li>"Contract_AcmeDeal_2026.pdf"</li>
    <li>"Exhibit_A_SOW.pdf"</li>
    <li>"Exhibit_B_Pricing.pdf"</li>
    <li>"Exhibit_C_Terms.pdf"</li>
<p>analysis:
    relationship: "Contract with exhibits"
    primary_document: "Contract_AcmeDeal_2026.pdf"
    supporting_documents:
      <li>file: "Exhibit_A_SOW.pdf"</li>
        type: "Statement of Work"
        referenced_in_primary: true
      <li>file: "Exhibit_B_Pricing.pdf"</li>
        type: "Pricing Schedule"
        referenced_in_primary: true
      <li>file: "Exhibit_C_Terms.pdf"</li>
        type: "Terms and Conditions"
        referenced_in_primary: true</p>
<p>unified_extraction:
    # Combine intelligence from all documents
    all_dates: [...]
    all_obligations: [...]
    all_financial_terms: [...]
    total_contract_value: 125000.00</p>
<p># Create single unified entry with linked assets
  output:
    primary_entry:
      title: "Acme Deal Contract Package"
      assets: [all 4 documents linked]
      content: "Unified summary of contract..."
<pre><code class="language-">
<h4>Comprehensive Tag Generation from Documents</h4></p>
</code></pre>yaml
Example - Contract Document:
<p>Generated Tags (with weights):
  Document Type:
    <li>contract (0.96)</li>
    <li>service agreement (0.91)</li>
    <li>legal document (0.88)</li></p>
<p>Parties:
    <li>acme corporation (0.97)</li>
    <li>designstudio llc (0.95)</li>
    <li>vendor agreement (0.82)</li></p>
<p>Financial:
    <li>$45,000 contract (0.94)</li>
    <li>payment schedule (0.89)</li>
    <li>net 30 terms (0.85)</li></p>
<p>Dates/Timeline:
    <li>6 month term (0.91)</li>
    <li>q1 2026 (0.87)</li>
    <li>february start (0.85)</li></p>
<p>Subject Matter:
    <li>design services (0.93)</li>
    <li>website redesign (0.88)</li>
    <li>branding (0.76)</li></p>
<p>Obligations:
    <li>deliverables (0.90)</li>
    <li>milestones (0.88)</li>
    <li>revision rounds (0.82)</li></p>
<p>Legal Terms:
    <li>confidentiality (0.89)</li>
    <li>ip ownership (0.87)</li>
    <li>termination clause (0.84)</li></p>
<p>Status:
    <li>pending signature (0.91)</li>
    <li>needs review (0.78)</li>
<pre><code class="language-">
<h4>Enrichment Types Table</h4></p>
<table>
<tr><th>Enrichment</th><th>Output Type</th><th>Description</th></tr>
<tr><td></strong>Document Type Detection<strong></td><td>Metadata</td><td>Invoice, contract, receipt, meeting notes, etc.</td></tr>
<tr><td></strong>Text Extraction<strong></td><td>Representation (markdown)</td><td>Full text with structure preserved</td></tr>
<tr><td></strong>Structure Analysis<strong></td><td>Metadata + Representation</td><td>Headings, sections, hierarchy</td></tr>
<tr><td></strong>Table Extraction<strong></td><td>Representation (markdown/JSON)</td><td>Structured data from tables</td></tr>
<tr><td></strong>Form Field Extraction<strong></td><td>Metadata</td><td>Key-value pairs from forms</td></tr>
<tr><td></strong>Named Entity Recognition<strong></td><td>Metadata + Tags</td><td>People, organizations, dates, amounts</td></tr>
<tr><td></strong>Date/Deadline Extraction<strong></td><td>Metadata</td><td>All dates with context and type</td></tr>
<tr><td></strong>Financial Data Extraction<strong></td><td>Metadata</td><td>Amounts, line items, totals</td></tr>
<tr><td></strong>Action Item Extraction<strong></td><td>Representation + Life Steward</td><td>Tasks, to-dos, assignments</td></tr>
<tr><td></strong>Decision Extraction<strong></td><td>Representation + Life Steward</td><td>Choices made with context</td></tr>
<tr><td></strong>Question Extraction<strong></td><td>Metadata</td><td>Questions requiring answers</td></tr>
<tr><td></strong>AI Summary<strong></td><td>Representation (markdown)</td><td>Executive summary at multiple lengths</td></tr>
<tr><td></strong>Key Points Extraction<strong></td><td>Representation</td><td>Bullet-point takeaways</td></tr>
<tr><td></strong>Citation Extraction<strong></td><td>Metadata + linked Entries</td><td>References and sources</td></tr>
<tr><td></strong>Contact Information<strong></td><td>Metadata</td><td>Emails, phones, addresses</td></tr>
<tr><td></strong>Signature Detection<strong></td><td>Metadata</td><td>Signature presence and status</td></tr>
<tr><td></strong>Handwriting Recognition<strong></td><td>Representation</td><td>For scanned handwritten docs</td></tr>
<tr><td></strong>Diagram/Chart Analysis<strong></td><td>Representation</td><td>Describe visual elements</td></tr>
<tr><td></strong>Language Detection<strong></td><td>Metadata</td><td>Document language(s)</td></tr>
<tr><td></strong>Confidentiality Assessment<strong></td><td>Metadata</td><td>PII, financial, legal sensitivity</td></tr>
<tr><td></strong>Duplicate Detection<strong></td><td>Links</td><td>Find similar/duplicate documents</td></tr>
<tr><td></strong>Multi-Doc Package Analysis<strong></td><td>Links + Representation</td><td>Unify related documents</td></tr>
<tr><td></strong>Obligation Extraction<strong></td><td>Metadata + Life Steward</td><td>Commitments, requirements</td></tr>
<tr><td></strong>Email Thread Analysis<strong></td><td>Metadata</td><td>Thread context, response needs</td></tr>
<tr><td></strong>Spreadsheet Analysis<strong></td><td>Metadata + Representation</td><td>Data summarization, insights</td></tr>
<tr><td></strong>Research Paper Analysis<strong></td><td>Metadata + Representation</td><td>Citations, methodology, findings</td></tr>
</table>
<hr>
<h2>Cross-Cutting Enrichments (All Asset Types)</h2>
<h3>Semantic Analysis</h3>
<li></strong>Vector Embeddings<strong>: Generate embeddings for semantic search via pgvector</li>
<li></strong>Related Entry Detection<strong>: Find similar content in user's library</li>
<li></strong>Auto-Linking<strong>: Suggest connections to existing entries</li>
<li></strong>Duplicate Detection<strong>: Identify exact and near-duplicates</li>
<h3>Tag Intelligence</h3>
<h4>Multi-Dimensional Tag Extraction</h4>
<p>Tags should be extracted across multiple dimensions from the rich descriptions:</p>
<table>
<tr><th>Dimension</th><th>Examples</th></tr>
<tr><td></strong>Subjects<strong></td><td>dog, cat, person, car, building</td></tr>
<tr><td></strong>Subject Attributes<strong></td><td>brown dog, tall woman, red car, old building</td></tr>
<tr><td></strong>Subject Types<strong></td><td>pitbull, tabby cat, businessman, sports car, Victorian house</td></tr>
<tr><td></strong>Actions (gerunds)<strong></td><td>running, jumping, eating, sleeping, driving</td></tr>
<tr><td></strong>Action Phrases<strong></td><td>dog jumping, cat sleeping, person typing</td></tr>
<tr><td></strong>Interactions<strong></td><td>playing fetch, petting dog, shaking hands</td></tr>
<tr><td></strong>Settings<strong></td><td>outdoors, park, office, kitchen, beach</td></tr>
<tr><td></strong>Setting Details<strong></td><td>grassy park, modern office, sunny beach</td></tr>
<tr><td></strong>Objects<strong></td><td>frisbee, laptop, coffee cup, chair</td></tr>
<tr><td></strong>Object Attributes<strong></td><td>red frisbee, silver laptop, white coffee cup</td></tr>
<tr><td></strong>Weather/Time<strong></td><td>sunny, rainy, night, sunset, winter</td></tr>
<tr><td></strong>Mood/Tone<strong></td><td>happy, peaceful, dramatic, funny, sad</td></tr>
<tr><td></strong>Style/Type<strong></td><td>portrait, landscape, meme, screenshot, candid</td></tr>
<tr><td></strong>Colors<strong></td><td>red, blue, colorful, monochrome, pastel</td></tr>
<tr><td></strong>Quantities<strong></td><td>group, crowd, solo, pair, three dogs</td></tr>
</table>
<h4>Confidence Weight Factors</h4>
<p>Tag confidence (0.0-1.0) should consider:</p>
<p>1. </strong>Visibility/Prominence<strong>: Is this the main subject or background detail?
2. </strong>Certainty<strong>: How confident is the AI in this classification?
3. </strong>Specificity Penalty<strong>: More specific tags get slightly lower base confidence
4. </strong>User History Boost<strong>: Tags the user frequently uses get boosted
5. </strong>Existing Tag Match<strong>: Exact matches to existing tags get boosted</p>
</code></pre>
Example weight calculation:
<li>"dog" (main subject, high certainty) â†’ 0.99</li>
<li>"pitbull" (specific breed, moderate certainty) â†’ 0.85</li>
<li>"trees" (background element) â†’ 0.71</li>
<li>"sunny day" (inferred from lighting) â†’ 0.82</li>
<pre><code class="language-">
<h4>Hierarchical Tag Inference</h4>
<li>If &quot;golden retriever&quot; detected â†’ also suggest &quot;dog&quot;, &quot;pet&quot;, &quot;animal&quot;</li>
<li>If &quot;laptop&quot; detected â†’ also suggest &quot;computer&quot;, &quot;electronics&quot;, &quot;technology&quot;</li>
<li>If &quot;Eiffel Tower&quot; detected â†’ also suggest &quot;Paris&quot;, &quot;France&quot;, &quot;landmark&quot;, &quot;travel&quot;</li>
<h4>User Pattern Learning</h4>
<li>Weight suggestions based on user's tagging history</li>
<li>Learn tag co-occurrence patterns (user always tags beach photos with &quot;vacation&quot;)</li>
<li>Respect user's tag vocabulary (prefer their existing tags over synonyms)</li>
<h4>Anti-Tag Detection</h4>
<li>Identify when content does NOT match certain tags</li>
<li>Useful for filtering/exclusion searches</li>
<h4>Tag Cluster Analysis</h4>
<li>Group related suggested tags for easier review</li>
<li>Present as: &quot;Subjects: dog, pitbull | Actions: jumping, catching | Setting: park, outdoors&quot;</li>
<h3>Representation Generation</h3>
<li></strong>Automatic Summaries<strong>: Generate summaries at multiple lengths (tweet, paragraph, page)</li>
<li></strong>Searchable Text<strong>: Ensure all content is full-text searchable</li>
<li></strong>Display-Ready Formats<strong>: Pre-render display HTML for fast retrieval</li>
<hr>
<h2>Architecture</h2>
<h3>Database Schema Additions</h3>
</code></pre>elixir
<h1>New fields on assets table</h1>
:enrichment_status  # pending | processing | completed | failed | partial
:enrichment_config  # JSON - which enrichments to run
:enrichment_results # JSON - raw results before processing into representations
:enriched_at        # timestamp of last enrichment run
:enrichment_version # track model versions for re-enrichment
<h1>New table: asset_enrichment_jobs</h1>
:id
:asset_id
:enrichment_type    # "ocr", "transcription", "object_detection", etc.
:status             # pending | processing | completed | failed
:provider           # "openai", "anthropic", "google", "local"
:result             # JSON
:error              # error message if failed
:cost_cents         # track costs
:processing_time_ms
:model_version
:inserted_at
:updated_at
<h1>New table: tag_suggestions</h1>
:id
:entry_id
:tag_id             # suggested tag (may not exist yet)
:tag_name           # for tags that don't exist
:confidence         # 0.0-1.0
:source             # which enrichment suggested it
:accepted           # null (pending), true, false
:inserted_at
<pre><code class="language-">
<h3>Oban Worker Structure</h3>
</code></pre>
lib/onelist/workers/enrichment/
â”œâ”€â”€ enrichment_orchestrator_worker.ex  # Coordinates enrichments for an asset
â”œâ”€â”€ image_enrichment_worker.ex
â”œâ”€â”€ audio_enrichment_worker.ex
â”œâ”€â”€ video_enrichment_worker.ex
â”œâ”€â”€ document_enrichment_worker.ex
â”œâ”€â”€ transcription_worker.ex
â”œâ”€â”€ embedding_worker.ex
â””â”€â”€ tag_suggestion_worker.ex
<pre><code class="language-">
<h3>Processing Pipeline</h3>
</code></pre>
Asset Upload
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Enrichment Orchestrator â”‚ â—„â”€â”€ Reads user preferences & asset type
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â”€â–º Tier 1 (Immediate, Free/Cheap)
    â”‚    â”œâ”€â”€ EXIF extraction
    â”‚    â”œâ”€â”€ File metadata
    â”‚    â”œâ”€â”€ Basic hash (dedup)
    â”‚    â””â”€â”€ Thumbnail generation
    â”‚
    â”œâ”€â”€â–º Tier 2 (Fast, Low Cost)
    â”‚    â”œâ”€â”€ OCR
    â”‚    â”œâ”€â”€ Object detection
    â”‚    â”œâ”€â”€ Language detection
    â”‚    â””â”€â”€ Basic classification
    â”‚
    â”œâ”€â”€â–º Tier 3 (Slower, Medium Cost)
    â”‚    â”œâ”€â”€ Full transcription
    â”‚    â”œâ”€â”€ AI descriptions
    â”‚    â”œâ”€â”€ Detailed analysis
    â”‚    â””â”€â”€ Vector embeddings
    â”‚
    â””â”€â”€â–º Tier 4 (Expensive, On-Demand)
         â”œâ”€â”€ Video scene analysis
         â”œâ”€â”€ Deep content understanding
         â”œâ”€â”€ Multi-modal fusion
         â””â”€â”€ Custom model analysis
<pre><code class="language-">
<h3>Provider Abstraction</h3>
</code></pre>elixir
defmodule Onelist.Enrichment.Provider do
  @callback transcribe(binary(), opts :: keyword()) :: {:ok, transcript} | {:error, term()}
  @callback analyze_image(binary(), opts :: keyword()) :: {:ok, analysis} | {:error, term()}
  @callback generate_embedding(text :: String.t()) :: {:ok, vector} | {:error, term()}
  # ... etc
end
<h1>Implementations</h1>
Onelist.Enrichment.Providers.OpenAI
Onelist.Enrichment.Providers.Anthropic
Onelist.Enrichment.Providers.Google
Onelist.Enrichment.Providers.Whisper  # Local
Onelist.Enrichment.Providers.Ollama   # Local LLMs
<pre><code class="language-">
<hr>
<h2>User Configuration Options</h2>
<h3>Global Preferences</h3>
<li></strong>Enrichment Level<strong>: None | Basic | Standard | Maximum</li>
<li></strong>Cost Limit<strong>: Monthly budget for AI enrichments</li>
<li></strong>Provider Preferences<strong>: Preferred providers, fallback order</li>
<li></strong>Privacy Mode<strong>: Local-only processing option</li>
<li></strong>Auto-Tagging<strong>: Enable/disable automatic tag application</li>
<h3>Per-Asset Overrides</h3>
<li>Skip enrichment for specific assets</li>
<li>Force re-enrichment with newer models</li>
<li>Select specific enrichments to run</li>
<h3>Tag Suggestion Settings</h3>
<li></strong>Auto-Apply Threshold<strong>: Confidence level to auto-apply tags (e.g., &gt;0.9)</li>
<li></strong>Suggestion Threshold<strong>: Minimum confidence to show suggestions (e.g., &gt;0.5)</li>
<li></strong>Require Review<strong>: Always review before applying</li>
<li></strong>Learn from Decisions<strong>: Improve suggestions based on accept/reject</li>
<hr>
<h2>Implementation Phases</h2>
<h3>Phase 1: Foundation (MVP)</h3>
<li>[ ] Database schema for enrichment tracking</li>
<li>[ ] Enrichment orchestrator worker</li>
<li>[ ] Provider abstraction layer</li>
<li>[ ] Basic image enrichment (EXIF, basic AI description)</li>
<li>[ ] Basic audio enrichment (transcription via Whisper API)</li>
<li>[ ] Store results as representations</li>
<li>[ ] Simple tag suggestions (no weights yet)</li>
<li>[ ] </strong>Life Steward<strong>: Basic </code>life_steward_data<code> metadata structure</li>
<h3>Phase 2: Life Steward Integration (Priority)</h3>
<li>[ ] </strong>Action item extraction<strong> from audio/video â†’ Next Actions</li>
<li>[ ] </strong>Commitment detection<strong> (others' commitments) â†’ Waiting For</li>
<li>[ ] </strong>Decision extraction<strong> â†’ Decisions bucket</li>
<li>[ ] </strong>People detection<strong> with contact linking</li>
<li>[ ] </strong>Date/timeline extraction<strong> for project timeline updates</li>
<li>[ ] </strong>Project/goal reference detection<strong> for auto-filing hints</li>
<li>[ ] </strong>Classification confidence scoring<strong> for Life Steward auto-filing</li>
<li>[ ] Feedback loop: Life Steward corrections improve enrichment</li>
<h3>Phase 3: Intelligence</h3>
<li>[ ] Tag suggestions with confidence weights</li>
<li>[ ] User preference learning for tags</li>
<li>[ ] Vector embeddings for semantic search</li>
<li>[ ] Related entry detection</li>
<li>[ ] Duplicate/similar content detection</li>
<li>[ ] Cost tracking and limits</li>
<li>[ ] </strong>Life Steward<strong>: Urgency/importance signal detection</li>
<h3>Phase 4: Deep Analysis</h3>
<li>[ ] Video processing pipeline</li>
<li>[ ] Multi-provider support</li>
<li>[ ] Scene detection and chapters</li>
<li>[ ] Advanced document analysis</li>
<li>[ ] Quote extraction</li>
<li>[ ] </strong>Life Steward<strong>: @agenda item extraction from conversations</li>
<h3>Phase 5: Life Recording</h3>
<li>[ ] Conversation boundary detection</li>
<li>[ ] Auto-segmentation into discrete entries</li>
<li>[ ] Media consumption detection and separation</li>
<li>[ ] Media relevance analysis (goals, projects, interests)</li>
<li>[ ] Context detection (location, activity, device)</li>
<li>[ ] Importance scoring for filtering</li>
<li>[ ] Privacy controls</li>
<li>[ ] Daily digest generation</li>
<li>[ ] </strong>Life Steward<strong>: Full GTD context inference from life recording</li>
<h3>Phase 6: Advanced Features</h3>
<li>[ ] Local processing options (Whisper, Ollama)</li>
<li>[ ] Re-enrichment when models improve</li>
<li>[ ] Batch enrichment for existing libraries</li>
<li>[ ] Custom enrichment plugins</li>
<li>[ ] Cross-entry analysis (detect recurring themes)</li>
<li>[ ] Screenshot sequence reconstruction</li>
<li>[ ] Multi-part audio/video reconstruction</li>
<li>[ ] </strong>Life Steward<strong>: Predictive classification from patterns</li>
<hr>
<h2>Integration Points</h2>
<h3>With Existing Systems</h3>
<li></strong>Representations<strong>: Store extracted text, summaries as representations</li>
<li></strong>Tags<strong>: Integrate with existing tag system, respect taxonomy</li>
<li></strong>Search<strong>: Feed extracted text to full-text search</li>
<li></strong>pgvector<strong>: Store embeddings for semantic search</li>
<li></strong>Oban<strong>: All processing via background jobs</li>
<li></strong>Assets<strong>: Link generated assets (thumbnails, extracted audio)</li>
<h3>With Future Systems</h3>
<li></strong>Core Taxonomy<strong>: Apply taxonomy terms, not just flat tags</li>
<li></strong>AI Chat Pane<strong>: Use enrichment data for context</li>
<li></strong>Entry Groups<strong>: Auto-organize by enrichment metadata</li>
<li></strong>Consume Later<strong>: Track progress through enriched content</li>
<h3>Life Steward Agent Integration</h3>
<p>The Asset Enrichment Agent serves as a critical preprocessor for the Life Steward Agent, extracting structured intelligence from raw assets that enables automatic classification, filing, and life management.</p>
<h4>Intelligence Flow: Asset Enrichment â†’ Life Steward</h4>
</code></pre>
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ASSET ENRICHMENT â†’ LIFE STEWARD PIPELINE                  â”‚
â”‚                                                                              â”‚
â”‚  Raw Asset                                                                   â”‚
â”‚  (audio, video, image, document)                                            â”‚
â”‚         â”‚                                                                    â”‚
â”‚         â–¼                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                    ASSET ENRICHMENT AGENT                            â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Extracts:                                                           â”‚   â”‚
â”‚  â”‚  â€¢ Transcripts, summaries, descriptions                             â”‚   â”‚
â”‚  â”‚  â€¢ People mentioned/present (with voice/face signatures)            â”‚   â”‚
â”‚  â”‚  â€¢ Action items, commitments, decisions                             â”‚   â”‚
â”‚  â”‚  â€¢ Dates, deadlines, time references                                â”‚   â”‚
â”‚  â”‚  â€¢ Topics, projects, goals mentioned                                â”‚   â”‚
â”‚  â”‚  â€¢ Context (location, activity, media type)                         â”‚   â”‚
â”‚  â”‚  â€¢ Sentiment, urgency, importance signals                           â”‚   â”‚
â”‚  â”‚  â€¢ Relationships to existing entries                                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                                                                    â”‚
â”‚         â”‚ Structured enrichment metadata                                    â”‚
â”‚         â–¼                                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      LIFE STEWARD AGENT                              â”‚   â”‚
â”‚  â”‚                                                                      â”‚   â”‚
â”‚  â”‚  Uses enrichment to:                                                 â”‚   â”‚
â”‚  â”‚  â€¢ Auto-classify into Domain/Scope/Project                          â”‚   â”‚
â”‚  â”‚  â€¢ Create Next Actions from extracted action items                  â”‚   â”‚
â”‚  â”‚  â€¢ Create Waiting For entries from others' commitments              â”‚   â”‚
â”‚  â”‚  â€¢ Log Decisions with context and reasoning                         â”‚   â”‚
â”‚  â”‚  â€¢ Link to Person domains via detected people                       â”‚   â”‚
â”‚  â”‚  â€¢ Populate @agenda:Person with discussion topics                   â”‚   â”‚
â”‚  â”‚  â€¢ Update project timelines from mentioned dates                    â”‚   â”‚
â”‚  â”‚  â€¢ Track goal progress from media consumption relevance             â”‚   â”‚
â”‚  â”‚  â€¢ Detect context for GTD context filtering                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
<pre><code class="language-">
<h4>Specific Enrichment â†’ Life Steward Mappings</h4>
<p>##### 1. Action Item Extraction â†’ Next Actions</p>
<p>Asset Enrichment detects commitments made by the user in conversations:</p>
</code></pre>yaml
<h1>Enrichment Output</h1>
extracted_action_items:
  <li>speaker: "user"</li>
    statement: "I'll send that proposal by Friday"
    timestamp: "00:23:45"
    deadline_mentioned: "Friday"
    confidence: 0.92
<h1>Life Steward Processing</h1>
creates:
  entry_type: "task"
  bucket: "next_actions"
  title: "Send proposal"
  metadata:
    due_date: "2026-02-07"  # computed from "Friday"
    source_entry_id: "meeting-recording-uuid"
    source_timestamp: "00:23:45"
    gtd_context: "@computer"  # inferred from "send"
    extracted_confidence: 0.92
<pre><code class="language-">
##### 2. Others' Commitments â†’ Waiting For
<p>Asset Enrichment detects commitments made by others:</p>
</code></pre>yaml
<h1>Enrichment Output</h1>
extracted_commitments:
  <li>speaker: "John" (identified)</li>
    statement: "I'll have the designs to you by Tuesday"
    commitment_to: "user"
    deadline_mentioned: "Tuesday"
    confidence: 0.89
<h1>Life Steward Processing</h1>
creates:
  entry_type: "task"
  bucket: "waiting_for"
  title: "Waiting: Designs from John"
  metadata:
    expected_date: "2026-02-04"
    waiting_on_person: "contact-john-uuid"
    source_entry_id: "meeting-recording-uuid"
    asked_date: "2026-01-28"
<pre><code class="language-">
##### 3. Decision Extraction â†’ Decisions Bucket
<p>Asset Enrichment detects decisions made in conversations:</p>
</code></pre>yaml
<h1>Enrichment Output</h1>
extracted_decisions:
  <li>statement: "Let's go with the blue theme for the website"</li>
    participants: ["user", "Sarah", "John"]
    timestamp: "00:45:12"
    context: "After discussing three color options"
    confidence: 0.87
<h1>Life Steward Processing</h1>
creates:
  entry_type: "note"
  bucket: "decisions"
  title: "Decision: Blue theme for website"
  content: "Decided to go with blue theme after discussing three options with Sarah and John"
  metadata:
    decision_date: "2026-01-28"
    participants: ["contact-sarah-uuid", "contact-john-uuid"]
    source_entry_id: "meeting-recording-uuid"
    related_project: "website-redesign-uuid"  # matched from context
<pre><code class="language-">
##### 4. People Detection â†’ Person Domains &amp; @agenda
<p>Asset Enrichment identifies people in content:</p>
</code></pre>yaml
<h1>Enrichment Output</h1>
detected_people:
  <li>name: "Sarah"</li>
    voice_match: "contact-sarah-uuid" (95% confidence)
    talk_time: "12:34"
    topics_discussed:
      <li>"website redesign"</li>
      <li>"vacation plans"</li>
      <li>"Mom's birthday party"</li>
<h1>Life Steward Processing</h1>
actions:
  # If Sarah has Person domain
  <li>link_entry_to: "Person: Sarah" domain</li>
<p># Extract @agenda items for next conversation
  <li>add_to_agenda:</li>
      person: "Sarah"
      items:
        <li>"Follow up on website redesign status"</li>
        <li>"Confirm vacation dates"</li>
      source: "conversation-2026-01-28"</p>
<p># Update interaction tracking
  <li>log_interaction:</li>
      person: "contact-sarah-uuid"
      date: "2026-01-28"
      duration: "45:00"
      topics: ["website", "vacation", "family"]
<pre><code class="language-">
##### 5. Project/Goal References â†’ Auto-Filing &amp; Linking</p>
<p>Asset Enrichment detects mentions of projects and goals:</p>
</code></pre>yaml
<h1>Enrichment Output</h1>
detected_references:
  <li>type: "project"</li>
    mention: "kitchen remodel"
    matched_entry: "kitchen-remodel-project-uuid"
    confidence: 0.94
    context: "Discussing contractor timeline"
<li>type: "goal"</li>
    mention: "getting healthier"
    matched_entry: "health-goal-uuid"
    confidence: 0.78
    context: "Mentioned joining gym"
<h1>Life Steward Processing</h1>
actions:
  <li>file_entry:</li>
      primary_location: "Personal / Home / Kitchen Remodel"
      confidence: 0.94
<li>link_entry:</li>
      to: "health-goal-uuid"
      relationship: "relates"
<li>update_project:</li>
      project: "kitchen-remodel-project-uuid"
      add_note: "Contractor timeline discussed in meeting"
<pre><code class="language-">
##### 6. Date/Timeline Extraction â†’ Project Timeline Updates
<p>Asset Enrichment extracts temporal information:</p>
</code></pre>yaml
<h1>Enrichment Output</h1>
extracted_dates:
  <li>statement: "The contractor said they can start March 1st"</li>
    date: "2026-03-01"
    type: "external_dependency"
    related_to: "kitchen remodel"
    confidence: 0.91
<li>statement: "We need to have the designs done two weeks before that"</li>
    date: "2026-02-15"  # computed relative date
    type: "derived_deadline"
    dependency: "before contractor start"
    confidence: 0.85
<h1>Life Steward Processing</h1>
actions:
  <li>update_project_timeline:</li>
      project: "kitchen-remodel-uuid"
      updates:
        <li>task: "Contractor start date"</li>
          new_date: "2026-03-01"
          source: "conversation"
        <li>task: "Finalize designs"</li>
          deadline: "2026-02-15"
          reason: "Must be done 2 weeks before contractor starts"
<li>check_critical_path:</li>
      alert_if: timeline_risk
<pre><code class="language-">
##### 7. Context Detection â†’ GTD Context Filtering
<p>Asset Enrichment (especially life recording) detects context:</p>
</code></pre>yaml
<h1>Enrichment Output (Life Recording)</h1>
detected_context:
  location: "office" (from ambient audio)
  time: "10:30 AM"
  activity: "meeting"
  device_context: "laptop nearby" (keyboard sounds)
  calendar_correlation: "Weekly Team Sync"
<h1>Life Steward Processing</h1>
actions:
  <li>auto_tag_context:</li>
      inferred_contexts: ["@office", "@computer"]
<li>suggest_tasks:</li>
      available_contexts: ["@office", "@computer", "@phone"]
<li>log_time_spent:</li>
      domain: "Business: Acme Inc"
      activity: "meeting"
      duration: "45:00"
<pre><code class="language-">
##### 8. Media Consumption â†’ Goal Progress &amp; Learning Tracking
<p>Asset Enrichment's media analysis feeds Life Steward's goal tracking:</p>
</code></pre>yaml
<h1>Enrichment Output (Media Consumption)</h1>
media_analysis:
  type: "podcast"
  show: "Huberman Lab"
  topic: "Sleep Optimization"
  relevance:
    <li>goal: "Improve sleep quality"</li>
      match_score: 0.94
    <li>goal: "Build morning routine"</li>
      match_score: 0.72
  key_takeaways:
    <li>"Morning sunlight within 30 minutes"</li>
    <li>"Keep bedroom at 65-68Â°F"</li>
<h1>Life Steward Processing</h1>
actions:
  <li>update_goal_progress:</li>
      goal: "Improve sleep quality"
      activity: "Research/learning"
      entry: "podcast-huberman-sleep-uuid"
<li>create_suggested_tasks:</li>
      from_takeaways: true
      suggestions:
        <li>title: "Get morning sunlight after waking"</li>
          goal: "Improve sleep quality"
          habit_candidate: true
        <li>title: "Check bedroom temperature"</li>
          goal: "Improve sleep quality"
          bucket: "next_actions"
<li>track_learning:</li>
      topic: "sleep optimization"
      time_spent: "45:00"
      domain: "Personal / Health"
<pre><code class="language-">
##### 9. Urgency/Importance Signals â†’ Prioritization
<p>Asset Enrichment detects urgency and importance:</p>
</code></pre>yaml
<h1>Enrichment Output</h1>
urgency_signals:
  <li>statement: "This is really critical, we need it ASAP"</li>
    speaker: "boss" (identified)
    urgency_level: "high"
    importance_level: "high"
    related_topic: "client presentation"
    timestamp: "00:12:34"
<p>sentiment_analysis:
  overall: "stressed"
  boss_tone: "urgent, concerned"
  user_tone: "responsive"</p>
<h1>Life Steward Processing</h1>
actions:
  <li>flag_entry:</li>
      priority: "high"
      source: "detected urgency in conversation"
<li>extract_task:</li>
      title: "Client presentation preparation"
      priority: "P1"
      inferred_deadline: "ASAP"
<li>notify_user:</li>
      message: "High-urgency item detected from conversation with boss"
      action: "Review and prioritize"
<pre><code class="language-">
##### 10. Relationship Detection â†’ Entry Connections
<p>Asset Enrichment detects relationships to existing content:</p>
</code></pre>yaml
<h1>Enrichment Output</h1>
detected_relationships:
  <li>type: "follow_up"</li>
    statement: "Following up on what we discussed last week about the API"
    related_entry: "meeting-2026-01-21-uuid" (semantic match)
    confidence: 0.88
<li>type: "answers"</li>
    statement: "I decided to go with PostgreSQL for the database"
    related_entry: "question-database-choice-uuid"
    confidence: 0.91
<li>type: "progresses"</li>
    statement: "I finished the first draft of the proposal"
    related_entry: "task-write-proposal-uuid"
    confidence: 0.94
<h1>Life Steward Processing</h1>
actions:
  <li>create_connections:</li>
      <li>from: current_entry</li>
        to: "meeting-2026-01-21-uuid"
        type: "follows_up"
<li>from: current_entry</li>
        to: "question-database-choice-uuid"
        type: "answers"
<li>update_task:</li>
      task: "task-write-proposal-uuid"
      status: "completed" (or progress update)
      completion_source: "detected in conversation"
<pre><code class="language-">
<h4>Enrichment Metadata Schema for Life Steward</h4>
<p>Asset Enrichment should output a standardized metadata structure that Life Steward can consume:</p>
</code></pre>elixir
<h1>Enrichment output metadata for Life Steward consumption</h1>
metadata: %{
  "life_steward_data" => %{
    # Classification hints
    "suggested_domain" => "Business: Acme Inc",
    "suggested_scope" => "Marketing",
    "suggested_project" => "Q1 Campaign",
    "suggested_bucket" => "reference",
    "classification_confidence" => 0.87,
    "classification_signals" => ["topic_match", "person_match", "project_mention"],
<p># Extracted actionable items
    "action_items" => [
      %{
        "text" => "Send proposal by Friday",
        "speaker" => "user",
        "deadline" => "2026-02-07",
        "context" => "@computer",
        "confidence" => 0.92,
        "timestamp" => "00:23:45"
      }
    ],</p>
<p># Waiting for items
    "waiting_for" => [
      %{
        "text" => "Designs from John",
        "from_person" => "contact-john-uuid",
        "expected_date" => "2026-02-04",
        "confidence" => 0.89
      }
    ],</p>
<p># Decisions made
    "decisions" => [
      %{
        "text" => "Go with blue theme",
        "context" => "Website color discussion",
        "participants" => ["user", "contact-sarah-uuid"],
        "timestamp" => "00:45:12"
      }
    ],</p>
<p># People involved
    "people_detected" => [
      %{
        "name" => "Sarah",
        "contact_id" => "contact-sarah-uuid",
        "confidence" => 0.95,
        "role" => "participant",
        "talk_time" => "12:34"
      }
    ],</p>
<p># Agenda items for future conversations
    "agenda_items" => [
      %{
        "person" => "contact-sarah-uuid",
        "topic" => "Follow up on website status",
        "source_timestamp" => "00:52:00"
      }
    ],</p>
<p># Related entries
    "related_entries" => [
      %{
        "entry_id" => "project-uuid",
        "relationship" => "relates",
        "confidence" => 0.94,
        "reason" => "project mentioned directly"
      }
    ],</p>
<p># Timeline information
    "timeline_data" => [
      %{
        "date" => "2026-03-01",
        "type" => "deadline",
        "context" => "Contractor start date",
        "related_project" => "kitchen-remodel-uuid"
      }
    ],</p>
<p># Context signals
    "context_signals" => %{
      "location" => "office",
      "activity" => "meeting",
      "gtd_contexts" => ["@office", "@computer"],
      "calendar_event" => "team-sync-uuid"
    },</p>
<p># Goal/project relevance (for media consumption)
    "goal_relevance" => [
      %{
        "goal_id" => "health-goal-uuid",
        "match_score" => 0.94,
        "reason" => "topic directly matches goal"
      }
    ],</p>
<p># Urgency/importance
    "priority_signals" => %{
      "urgency" => "high",
      "importance" => "high",
      "source" => "explicit statement by boss",
      "keywords" => ["critical", "ASAP"]
    }
  }
}
<pre><code class="language-">
<h4>Processing Priority</h4></p>
<p>Asset Enrichment should prioritize extractions that Life Steward needs:</p>
</code></pre>
Priority 1 (Extract First - Immediate Life Steward Value):
â”œâ”€â”€ Action items (user's commitments)
â”œâ”€â”€ Waiting for items (others' commitments)
â”œâ”€â”€ People present/mentioned
â”œâ”€â”€ Project/goal references
â””â”€â”€ Urgency/importance signals
<p>Priority 2 (Extract Second - Classification):
â”œâ”€â”€ Topic/subject analysis
â”œâ”€â”€ Domain/scope hints
â”œâ”€â”€ Related entry detection
â””â”€â”€ Context signals</p>
<p>Priority 3 (Extract Third - Enrichment):
â”œâ”€â”€ Full transcription
â”œâ”€â”€ Detailed summaries
â”œâ”€â”€ Sentiment analysis
â”œâ”€â”€ Timeline/date extraction
â””â”€â”€ Decision logging</p>
<p>Priority 4 (Background - Future Reference):
â”œâ”€â”€ Quote extraction
â”œâ”€â”€ Complete tag generation
â”œâ”€â”€ Detailed metadata
â””â”€â”€ Visual descriptions (for video/image)
<pre><code class="language-">
<h4>Feedback Loop: Life Steward â†’ Asset Enrichment</h4></p>
<p>Life Steward can improve Asset Enrichment over time:</p>
</code></pre>yaml
<h1>Life Steward sends back to Asset Enrichment:</h1>
<p>classification_corrections:
  <li>entry_id: "uuid"</li>
    enrichment_suggested: "Personal / Finance"
    user_selected: "Business: Acme Inc / Operations"
    signal: "Improve domain classification for expense-related content"</p>
<p>person_identifications:
  <li>voice_signature: "hash"</li>
    enrichment_guess: "Unknown male speaker"
    user_identified: "contact-john-uuid"
    signal: "Learn John's voice for future recognition"</p>
<p>action_item_feedback:
  <li>extracted: "Send proposal by Friday"</li>
    user_action: "accepted and created task"
    signal: "Good extraction, reinforce pattern"</p>
<li>extracted: "Think about the budget"</li>
    user_action: "rejected - too vague"
    signal: "Avoid vague/thinking statements as action items"
<p>project_associations:
  <li>mentioned: "website project"</li>
    enrichment_matched: null
    user_linked: "website-redesign-uuid"
    signal: "Learn 'website project' = Website Redesign project"
<pre><code class="language-">
<hr></p>
<h2>Cost Considerations</h2>
<h3>Estimated Costs per Asset (2024 pricing)</h3>
<table>
<tr><th>Operation</th><th>Approximate Cost</th></tr>
<tr><td>Image description (GPT-4V)</td><td>$0.01-0.03</td></tr>
<tr><td>Audio transcription (Whisper)</td><td>$0.006/min</td></tr>
<tr><td>Text embedding (ada-002)</td><td>$0.0001/1K tokens</td></tr>
<tr><td>Video analysis (per frame)</td><td>$0.01-0.03/frame</td></tr>
</table>
<h3>Cost Control Strategies</h3>
<li>Tiered enrichment levels</li>
<li>Monthly budget caps</li>
<li>Queue low-priority enrichments</li>
<li>Cache embeddings</li>
<li>Batch similar requests</li>
<li>Use cheaper models for initial pass</li>
<hr>
<h2>Additional Considerations</h2>
<h3>Error Handling &amp; Recovery</h3>
</strong>Graceful Degradation:<strong>
</code></pre>yaml
Enrichment Failure Handling:
  total_failure:
    <li>Log error with details</li>
    <li>Mark asset as "enrichment_failed"</li>
    <li>Retry with exponential backoff</li>
      <li>Standard assets: 3 attempts</li>
      <li>High-value assets (user-starred, large files, linked to active projects): 5 attempts</li>
    <li>Fall back to basic metadata only</li>
    <li>Notify user if critical enrichment fails</li>
<p>partial_failure:
    <li>Save successful enrichments</li>
    <li>Mark specific enrichment types as failed</li>
    <li>Continue with other enrichment types</li>
    <li>Queue failed types for retry</li>
    <li>Show user "Enrichment Incomplete" badge on asset</li>
      <li>Click to see: which enrichments failed and why</li>
      <li>Recommendations for resolving (e.g., "File may be corrupted", "Provider temporarily unavailable - will retry")</li>
      <li>Manual retry option</li>
      <li>Option to dismiss/ignore</li></p>
<p>provider_failure:
    <li>Try fallback provider if configured</li>
    <li>Use cached results if available</li>
    <li>Degrade to cheaper/simpler model</li>
    <li>Queue for retry when provider recovers</li></p>
<p>timeout_handling:
    <li>Long-running enrichments get checkpointed</li>
    <li>Resume from checkpoint on retry</li>
    <li>User notified of extended processing time</li>
<pre><code class="language-">
</strong>Corrupt/Problematic Files:<strong>
<table>
<tr><th>Issue</th><th>Detection</th><th>Handling</th></tr>
<tr><td>Corrupted file</td><td>File parsing fails</td><td>Log, notify user, mark as unprocessable</td></tr>
<tr><td>Password protected</td><td>Encryption detected</td><td>See password handling below</td></tr>
<tr><td>Extremely large</td><td>Size &gt; threshold</td><td>Chunk processing, warn about time/cost</td></tr>
<tr><td>Unsupported format</td><td>Unknown mime type</td><td>Attempt generic extraction, flag for review</td></tr>
<tr><td>Truncated file</td><td>Incomplete data</td><td>Process available data, note incompleteness</td></tr>
<tr><td>Malformed metadata</td><td>Parse errors</td><td>Skip bad sections, extract what's possible</td></tr>
</table></p>
</strong>Password-Protected File Handling:<strong>
</code></pre>yaml
Default Behavior: Prompt per file
  <li>"This file is password protected. Enter password to enrich, or skip."</li>
  <li>Options: [Enter Password] [Skip] [Always Skip Protected Files]</li>
<p>User Settings:
  password_handling: "prompt" | "always_store" | "never_store"</p>
<p>prompt (default):
    <li>Ask each time a protected file is encountered</li>
    <li>Offer to remember password for this file</li>
    <li>"Remember for this file?" checkbox</li></p>
<p>always_store:
    <li>Prompt for password once</li>
    <li>Store encrypted (user's encryption key)</li>
    <li>Auto-use for re-enrichment</li>
    <li>Can view/delete stored passwords in settings</li></p>
<p>never_store:
    <li>Always prompt, never offer to save</li>
    <li>For high-security users</li></p>
<p>Password Storage:
  <li>Encrypted with user's master key</li>
  <li>Stored per-asset (not global)</li>
  <li>Deletable anytime</li>
  <li>Never sent to external providers (decrypt locally first)</li>
<pre><code class="language-">
<h3>Quality Assurance &amp; Confidence</h3></p>
</strong>Confidence Calibration:<strong>
</code></pre>yaml
Enrichment Confidence Levels:
  very_high (0.95+):
    <li>Auto-apply without review</li>
    <li>Examples: EXIF extraction, file type detection, basic OCR</li>
<p>high (0.85-0.94):
    <li>Auto-apply with easy undo</li>
    <li>Examples: Object detection, speaker diarization, document classification</li></p>
<p>medium (0.70-0.84):
    <li>Apply but flag for review</li>
    <li>Examples: Action item extraction, person identification, sentiment</li></p>
<p>low (0.50-0.69):
    <li>Suggest but don't apply</li>
    <li>Examples: Vague tag suggestions, uncertain classifications</li></p>
<p>very_low (<0.50):
    <li>Don't suggest, log for improvement</li>
    <li>Use for training data collection</li>
<pre><code class="language-">
</strong>Adaptive Confidence Thresholds:<strong>
</code></pre>yaml
Self-Adjusting Thresholds:
  The agent analyzes its own performance and adjusts thresholds per enrichment type:</p>
<p>monitoring:
    <li>Track correction rate per enrichment type</li>
    <li>Track "user moved to different location" rate</li>
    <li>Track "user deleted/rejected" rate</li>
    <li>Track time-to-correction (immediate = clear error)</li></p>
<p>adjustment_logic:
    # If users frequently correct a specific enrichment type, raise its threshold
    <li>correction_rate > 15% for 100+ items â†’ raise threshold by 0.03</li>
    <li>correction_rate > 25% for 50+ items â†’ raise threshold by 0.05</li>
    <li>correction_rate < 5% for 200+ items â†’ lower threshold by 0.02 (more trust)</li></p>
<p>per_enrichment_thresholds:
    # Each enrichment type can have its own learned threshold
    image_description: 0.92  # adjusted from 0.90 due to 18% correction rate
    action_item_extraction: 0.88  # lowered from 0.90 due to 3% correction rate
    speaker_identification: 0.94  # raised due to frequent misidentification</p>
<p>bounds:
    <li>Never auto-apply below 0.85 regardless of learning</li>
    <li>Never require above 0.97 (nothing would pass)</li>
<pre><code class="language-">
</strong>User Global Confidence Setting:<strong>
</code></pre>yaml
Auto-Apply Threshold Setting:
  description: "How confident should the AI be before automatically applying enrichments?"</p>
<p>range: 0.85 - 0.97
  default: 0.92</p>
<p>slider_labels:
    0.85: "Trust AI more (faster, occasional corrections needed)"
    0.92: "Balanced (recommended)"
    0.97: "More cautious (review more items manually)"</p>
<p>behavior:
    <li>User setting acts as a modifier on top of adaptive thresholds</li>
    <li>If user sets 0.95, and adaptive threshold for tags is 0.90:</li>
      <li>Effective threshold = max(0.95, 0.90) = 0.95</li>
    <li>User cannot go below system's adaptive floor for risky enrichment types</li>
<pre><code class="language-">
</strong>Human-in-the-Loop Verification:<strong>
<li>Batch review interface for low-confidence items</li>
<li>Quick accept/reject/correct workflow</li>
<li>Corrections feed back to improve future enrichments</li>
<li>&quot;Verify this&quot; queue in daily review</li></p>
</strong>Quality Metrics (User-Visible Dashboard):<strong>
</code></pre>yaml
Transparency Dashboard:
  overall_accuracy:
    enrichments_applied: 4,523
    user_corrections: 187 (4.1%)
    auto_apply_success_rate: 95.9%
<p>by_enrichment_type:
    image_descriptions:
      applied: 892
      corrections: 23 (2.6%)
      current_threshold: 0.91
      trend: "improving"</p>
<p>action_item_extraction:
      applied: 234
      corrections: 41 (17.5%)
      current_threshold: 0.94 (raised)
      trend: "threshold adjusted, monitoring"</p>
<p>tag_suggestions:
      suggested: 3,456
      accepted: 2,891 (83.6%)
      rejected: 565
      trend: "stable"</p>
<p>recent_corrections:
    <li>"Changed tag 'dog' to 'wolf' on image_123" (2 hours ago)</li>
    <li>"Moved entry from Personal to Business" (yesterday)</li>
    <li>"Rejected action item extraction" (yesterday)</li></p>
<p>agent_learning:
    "Based on your corrections, I've raised the threshold for action item
     extraction from 0.90 to 0.94. I'll be more careful with those."
<pre><code class="language-">
</strong>Pattern-Based Learning:<strong>
</code></pre>yaml
Correction Pattern Detection:
  single_correction:
    weight: low
    action: Log, no immediate adjustment</p>
<p>repeated_pattern (3-5 occurrences):
    weight: medium
    action: Flag for threshold review, start adjusting</p>
<p>strong_pattern (10+ occurrences):
    weight: high
    action: Significant threshold adjustment, notify user of learning</p>
<p>examples:
    <li>User corrects "meeting" â†’ "standup" 8 times</li>
      â†’ Learn: "standup" tag preference, suggest "standup" over "meeting"</p>
<li>User rejects action items from casual conversations 12 times</li>
      â†’ Learn: Raise threshold for action items in "casual" audio context
<li>User always moves receipts from Personal to "Business: Acme"</li>
      â†’ Learn: Receipts from certain vendors â†’ auto-file to Business
<pre><code class="language-">
<h3>Performance &amp; Scalability</h3>
</strong>Processing Tiers:<strong>
</code></pre>
Tier 0 (Immediate, <1s):
â”œâ”€â”€ File type detection
â”œâ”€â”€ Basic metadata extraction
â”œâ”€â”€ Hash calculation (dedup)
â””â”€â”€ Thumbnail generation (images)
<p>Tier 1 (Fast, <30s):
â”œâ”€â”€ EXIF extraction
â”œâ”€â”€ Basic OCR
â”œâ”€â”€ Language detection
â””â”€â”€ Quick classification</p>
<p>Tier 2 (Standard, <5min):
â”œâ”€â”€ Full transcription
â”œâ”€â”€ AI descriptions
â”œâ”€â”€ Tag generation
â””â”€â”€ Document structure analysis</p>
<p>Tier 3 (Deep, <30min):
â”œâ”€â”€ Video scene analysis
â”œâ”€â”€ Multi-page document analysis
â”œâ”€â”€ Complex relationship detection
â””â”€â”€ Vector embeddings</p>
<p>Tier 4 (Background, hours):
â”œâ”€â”€ Batch re-enrichment
â”œâ”€â”€ Cross-asset analysis
â”œâ”€â”€ Full library deduplication
â””â”€â”€ Interest profile updates
<pre><code class="language-">
</strong>Auto-Processing by Content Type:<strong>
</code></pre>yaml
User-Configurable Default Tiers:
  # Users set which tier runs automatically per content type
  # Higher tiers require manual queue action</p>
<p>settings:
    images:
      auto_tier: 3  # Full processing by default (cheap, fast)
      reason: "Images are quick and inexpensive to process"</p>
<p>gifs:
      auto_tier: 3  # Full processing including animation analysis
      reason: "Similar cost to images"</p>
<p>audio:
      auto_tier: 2  # Transcription by default
      manual_for: [3, 4]  # Deep analysis requires manual trigger
      reason: "Transcription is core value; deep analysis is expensive"</p>
<p>video:
      auto_tier: 0  # Only basic metadata by default
      manual_for: [1, 2, 3, 4]  # Everything else requires manual trigger
      reason: "Video processing is slow and expensive"</p>
<p>documents:
      auto_tier: 2  # Structure and summary by default
      manual_for: [3, 4]
      reason: "Good balance of value and cost"</p>
<p>life_recording:
      auto_tier: 2  # Transcription and segmentation
      manual_for: [3, 4]
      reason: "Core value is transcription; deep analysis on-demand"</p>
<p>user_interface:
    <li>Settings page with sliders per content type</li>
    <li>"Process deeper" button on individual assets</li>
    <li>Bulk action: "Process all videos to Tier 2"</li>
    <li>Estimated cost shown before manual processing</li></p>
<p>examples:
    User uploads 4K video:
      â†’ Tier 0 runs automatically (thumbnail, metadata, duration)
      â†’ User sees: "Basic processing complete. [Enrich Further] for transcription and analysis"
      â†’ User clicks [Enrich Further], selects Tier 2
      â†’ Shows estimated cost and time, user confirms
      â†’ Tier 1 and 2 processing runs</p>
<p>User uploads screenshot:
      â†’ Tier 0, 1, 2, 3 run automatically
      â†’ OCR, description, tags, embeddings all generated
      â†’ No user action needed
<pre><code class="language-">
</strong>Queue Management:<strong>
</code></pre>yaml
Priority Factors:
  <li>User-initiated (manual "Enrich Further"): +100 priority</li>
  <li>Recently uploaded: +50 priority</li>
  <li>Part of active project: +30 priority</li>
  <li>Waiting for enrichment to file: +20 priority</li>
  <li>Re-enrichment request: +10 priority</li>
  <li>Batch/bulk processing: base priority</li></p>
<p>Rate Limiting:
  <li>Per-provider limits respected</li>
  <li>Cost budget limits enforced</li>
  <li>Burst allowance for user-initiated</li>
  <li>Graceful queuing when limits hit</li></p>
<p>Parallelization:
  <li>Independent enrichments run in parallel</li>
  <li>Dependent enrichments sequenced (audio extract â†’ transcribe)</li>
  <li>Worker pool scales with queue depth</li>
  <li>Resource-aware scheduling (GPU for video, CPU for text)</li>
<pre><code class="language-">
</strong>Self-Hosted Worker Configuration:<strong>
</code></pre>yaml
Worker Pool Settings (for self-hosted installations):
  worker_count:
    default: auto  # Based on CPU cores
    configurable: true
    range: 1 - 32</p>
<p>gpu_workers:
    enabled: false  # Default off
    count: 0
    types: ["video_processing", "local_whisper", "local_llm"]</p>
<p>memory_limit_per_worker:
    default: "2GB"
    configurable: true</p>
<p>concurrent_provider_calls:
    openai: 5  # Respect rate limits
    anthropic: 3
    local: unlimited</p>
<p>queue_settings:
    max_queue_depth: 10000
    stale_job_timeout: "24h"
    retry_failed_after: "1h"</p>
<p>resource_profiles:
    minimal:
      workers: 2
      memory: "1GB each"
      note: "For low-resource environments"</p>
<p>standard:
      workers: 4
      memory: "2GB each"
      note: "Recommended for most installations"</p>
<p>performance:
      workers: 8
      memory: "4GB each"
      gpu_enabled: true
      note: "For dedicated servers with GPU"
<pre><code class="language-">
</strong>Caching Strategy:<strong>
<li>Cache embeddings (expensive to regenerate)</li>
<li>Cache provider responses for identical inputs</li>
<li>Cache intermediate results (extracted audio from video)</li>
<li>TTL based on likelihood of change</li>
<li>Invalidate on source asset modification</li></p>
<h3>Privacy &amp; Security</h3>
<h4>Philosophy: Informed Consent, Not Paternalism</h4>
</code></pre>yaml
Our Approach:
  <li>It's the user's data - they're adults</li>
  <li>We inform, we don't block</li>
  <li>We document all risks clearly upfront</li>
  <li>We require acknowledgment before enabling the agent</li>
  <li>We provide controls, then get out of the way</li>
  <li>We don't make moral judgments about content</li>
  <li>We don't report anything to anyone</li>
  <li>We remember user choices and don't nag</li>
<p>What We Don't Do:
  <li>Block any content type</li>
  <li>Auto-delete anything</li>
  <li>Report to authorities</li>
  <li>Require justification for choices</li>
  <li>Make it complicated</li>
<pre><code class="language-">
<h4>Required Risk Acknowledgment (Before Agent Activation)</h4></p>
</strong>Users cannot enable the Asset Enrichment Agent without reviewing and accepting:<strong>
</code></pre>yaml
Risk Acknowledgment Flow:
  step_1: "Before You Begin"
    <li>Explanation of what the agent does</li>
    <li>Clear statement: "This agent sends your content to third-party AI providers for analysis"</li>
<p>step_2: "Understanding the Risks"
    <li>Data goes to external providers (OpenAI, Anthropic, Google, etc.)</li>
    <li>Provider employees may see content during review/debugging</li>
    <li>Data breaches at providers could expose your content</li>
    <li>Enrichment results stored in your account (encrypted)</li>
    <li>Legal discovery could access your enriched content</li></p>
<p>step_3: "Sensitive Content Scenarios"
    # User must scroll through / acknowledge each category</p>
<p>financial_identity:
      title: "Financial & Identity Documents"
      examples: "Tax returns, bank statements, passport scans, SSN"
      risk: "Identity theft if breached, financial fraud"
      our_default: "Warn before first external processing, then remember your choice"
      your_control: "Can set to always local, always external, or always ask"</p>
<p>medical_health:
      title: "Medical & Health Information"
      examples: "Medical records, prescriptions, therapy notes, health data"
      risk: "Insurance discrimination, employment issues, personal embarrassment"
      our_default: "Warn before first external processing"
      your_control: "Can set to never process externally"
      note: "HIPAA applies to healthcare providers, not personal storage, but consider the sensitivity"</p>
<p>intimate_sexual:
      title: "Intimate & Sexual Content"
      examples: "Nude photos, intimate videos, sexual content, dating conversations"
      risk: "Revenge porn, blackmail, relationship damage, personal devastation"
      our_default: "NSFW detection warns before external processing (if detection enabled)"
      your_control: "Can disable NSFW detection entirely, or set to never process externally"
      note: "Videos require manual enrichment trigger by default, providing natural protection"</p>
<p>ex_partner_content:
      title: "Ex-Partner Intimate Content"
      examples: "Intimate photos/videos of former partners"
      risk: "Legal liability in many jurisdictions, revenge porn laws may apply"
      our_default: "No special detection - treated as intimate content"
      your_control: "Same as intimate content"
      warning: "Possessing and processing intimate content of others without consent may be illegal in your jurisdiction. You are responsible for ensuring you have the right to possess and process this content."</p>
<p>children:
      title: "Content Involving Children"
      examples: "Photos of your kids, school records, medical records"
      risk: "Children's faces in third-party systems, long-horizon identity theft"
      our_default: "No special detection by default"
      your_control: "Face recognition off by default, can enable selectively"
      note: "You are responsible for your children's privacy decisions"</p>
<p>professional_confidential:
      title: "Professional & Confidential Work"
      examples: "Client data, trade secrets, NDA-covered material, job search"
      risk: "Termination, lawsuits, NDA violation, loss of business"
      our_default: "No special detection - processed like other documents"
      your_control: "Tag-based rules can mark work content as local-only"
      warning: "You are responsible for complying with your professional obligations"</p>
<p>legal_privileged:
      title: "Legal & Attorney-Privileged Content"
      examples: "Communications with lawyers, evidence in disputes, court documents"
      risk: "External processing may waive attorney-client privilege"
      our_default: "Detect legal document markers, warn before processing"
      your_control: "Can set to never process legal docs externally"
      warning: "Consult your attorney about privilege implications"</p>
<p>substance_illegal:
      title: "Substance Use & Potentially Illegal Content"
      examples: "Drug use documentation, illegal activity evidence"
      risk: "Legal consequences if subpoenaed or breached"
      our_default: "No special detection or blocking"
      your_control: "You decide what to store and process"
      warning: "We don't report anything, but this data exists and could be discovered"</p>
<p>step_4: "Your Responsibilities"
    acknowledgments:
      <li>"I understand my content will be sent to third-party AI providers"</li>
      <li>"I understand data breaches could expose my content"</li>
      <li>"I understand I am responsible for the content I choose to process"</li>
      <li>"I understand legal discovery could access my stored content"</li>
      <li>"I have read and understand the sensitive content scenarios above"</li></p>
<p>step_5: "Enable Agent"
    <li>User checks acknowledgment boxes</li>
    <li>User clicks "I Understand, Enable Asset Enrichment"</li>
    <li>Agent becomes active</li>
    <li>User can disable anytime</li>
<pre><code class="language-">
<h4>Sensitive Content Detection &amp; Handling</h4></p>
</strong>Detection runs locally first, warns before external processing:<strong>
</code></pre>yaml
Detection Categories:
  financial:
    signals: SSN patterns, "W-2", "1099", "tax return", account number patterns
    detection: Local pattern matching (no external processing for detection)
    action: Warning before first external processing
    message: "This appears to be a financial document. Process with external AI?"
    options:
      <li>"Process externally"</li>
      <li>"Process locally only"</li>
      <li>"Skip enrichment"</li>
      <li>"Always process financial docs externally" (remembers)</li>
      <li>"Never process financial docs externally" (remembers)</li>
<p>medical:
    signals: "diagnosis", "prescription", "patient", medical terminology, HIPAA markers
    detection: Local pattern matching
    action: Warning before first external processing
    same_options: true</p>
<p>identity_documents:
    signals: "passport", "driver license", ID layouts, SSN
    detection: Local pattern matching
    action: Warning
    same_options: true</p>
<p>nsfw:
    signals: Nudity detection model
    detection: Local ML model (runs on device, nothing sent externally)
    user_setting: Can disable NSFW detection entirely
    action_if_enabled: Warning before external processing
    message: "This image may contain nudity. Process with external AI?"
    options:
      <li>"Process externally"</li>
      <li>"Skip AI enrichment"</li>
      <li>"Always process NSFW externally" (remembers)</li>
      <li>"Never process NSFW externally" (remembers)</li></p>
<p>legal_privileged:
    signals: "attorney", "privileged", "confidential", court document layouts
    detection: Local pattern matching
    action: Warning with specific privilege note
    message: "This may be attorney-privileged. External processing could waive privilege."
    same_options: true
<pre><code class="language-">
</strong>NSFW Detection Setting:<strong>
</code></pre>yaml
NSFW Detection:
  setting_location: Privacy Settings
  options:
    enabled (default):
      <li>Local nudity detection runs on images</li>
      <li>Warns before external processing if nudity detected</li>
      <li>Detection model runs entirely on device</li></p>
<p>disabled:
      <li>No nudity scanning at all</li>
      <li>Images processed like any other content</li>
      <li>User's choice if they don't want scanning</li></p>
<p>note: "Even when enabled, detection runs locally - no images sent anywhere for NSFW detection"
<pre><code class="language-">
<h4>Emergency Controls</h4></p>
</strong>Panic Button - Immediately Disable External Processing:<strong>
</code></pre>yaml
Panic Button:
  location: Prominent in settings, also in quick-access menu
  label: "ğŸ›‘ Stop All External Processing"
<p>action_when_pressed:
    immediate:
      <li>Cancel all queued external API calls</li>
      <li>Cancel all in-flight requests (where possible)</li>
      <li>Disable external processing flag</li>
      <li>All future enrichments go local-only</li></p>
<p>confirmation:
      message: "External processing stopped. All enrichments will now use local processing only (reduced quality). Resume external processing?"
      options:
        <li>"Keep external processing disabled"</li>
        <li>"Resume in 1 hour"</li>
        <li>"Resume in 24 hours"</li>
        <li>"Resume now"</li></p>
<p>use_cases:
    <li>User realizes they uploaded something sensitive</li>
    <li>User gets nervous about recent uploads</li>
    <li>User is entering sensitive time period (legal proceedings, etc.)</li>
    <li>User just wants to pause and think</li></p>
<p>resuming:
    <li>Can resume anytime from settings</li>
    <li>Queued items will ask again before processing</li>
    <li>No automatic resume without user action</li>
<pre><code class="language-">
<h4>User Privacy Settings</h4></p>
</code></pre>yaml
Privacy Settings:
<p>global_controls:
    external_processing:
      enabled: true (default)
      panic_button: "Stop All External Processing"</p>
<p>local_only_mode:
      enabled: false
      description: "Never send anything to external AI providers. Reduced quality."</p>
<p>remember_my_choices:
      enabled: true (default)
      description: "Remember my decisions for each content category"</p>
<p>detection_toggles:
    nsfw_detection:
      enabled: true (default)
      description: "Scan images locally for nudity, warn before external processing"</p>
<p>financial_detection:
      enabled: true (default)</p>
<p>medical_detection:
      enabled: true (default)</p>
<p>legal_detection:
      enabled: true (default)</p>
<p>per_category_rules:
    # For each detected category, user sets default behavior</p>
<p>financial_documents:
      options:
        <li>"Always process externally"</li>
        <li>"Always process locally only"</li>
        <li>"Always skip enrichment"</li>
        <li>"Ask each time"</li>
      default: "Ask each time" (then remembers choice)</p>
<p>medical_documents:
      same_options: true
      default: "Ask each time"</p>
<p>nsfw_content:
      same_options: true
      default: "Ask each time"</p>
<p>identity_documents:
      same_options: true
      default: "Ask each time"</p>
<p>legal_documents:
      same_options: true
      default: "Ask each time"</p>
<p>tag_based_rules:
    description: "Apply rules based on tags"
    examples:
      <li>tag: "private"</li>
        rule: "Always local only"
      <li>tag: "work-confidential"</li>
        rule: "Never process"
      <li>tag: "safe-to-process"</li>
        rule: "Always external"</p>
<p>per_asset_controls:
    # Available on any asset
    options:
      <li>"Process externally" (override category rule)</li>
      <li>"Process locally only"</li>
      <li>"Skip all enrichment"</li>
      <li>"Delete all enrichments"</li>
      <li>"Mark as sensitive" (applies local-only)</li></p>
<p>biometric_controls:
    face_recognition:
      enabled: false (default)
      description: "Learn and recognize faces across photos"
      if_enabled:
        <li>Can enable/disable per person</li>
        <li>Can delete all face data anytime</li></p>
<p>voice_recognition:
      enabled: false (default)
      description: "Learn and recognize voices in audio"
      if_enabled:
        <li>Can enable/disable per person</li>
        <li>Can delete all voice data anytime</li>
<pre><code class="language-">
<h4>Data Retention &amp; Deletion</h4></p>
</code></pre>yaml
Data Retention:
  raw_provider_responses: 30 days (for debugging/quality improvement)
  processed_enrichments: As long as asset exists
  failed_enrichment_logs: 90 days
  correction_history: 1 year
  face_embeddings: Until user deletes or disables
  voice_embeddings: Until user deletes or disables
<p>Deletion Options:
  delete_asset:
    <li>Asset file deleted</li>
    <li>All enrichments deleted</li>
    <li>All provider response logs deleted</li>
    <li>References in other entries remain (as broken links)</li></p>
<p>delete_enrichments_only:
    <li>Keep asset file</li>
    <li>Delete all AI-generated enrichments</li>
    <li>Can re-enrich later if desired</li></p>
<p>delete_biometric_data:
    <li>"Forget this face" - delete face embeddings for one person</li>
    <li>"Forget all faces" - delete all face recognition data</li>
    <li>"Forget this voice" - delete voice embeddings for one person</li>
    <li>"Forget all voices" - delete all voice recognition data</li></p>
<p>delete_account:
    <li>All assets deleted</li>
    <li>All enrichments deleted</li>
    <li>All biometric data deleted</li>
    <li>All provider logs deleted</li>
    <li>Confirmation required</li>
    <li>Cannot be undone</li>
<pre><code class="language-">
<h3>Cost Management</h3></p>
</strong>Pre-Operation Cost Warnings:<strong>
</code></pre>yaml
Cost Warning Threshold:
  user_setting:
    name: "Warn me before expensive enrichments"
    description: "Show cost estimate and confirm before processing"
    default: $1.00
    range: $0.10 - $10.00
<p>behavior:
    if estimated_cost > threshold:
      show_warning:
        message: "Enriching this 2-hour video will cost approximately $3.40"
        breakdown:
          <li>"Audio extraction: $0.00"</li>
          <li>"Transcription (120 min): $0.72"</li>
          <li>"Scene analysis (estimated 45 scenes): $1.35"</li>
          <li>"AI summary and description: $0.28"</li>
          <li>"Tag generation: $0.15"</li>
          <li>"Embeddings: $0.90"</li>
        options:
          <li>"Process all ($3.40)"</li>
          <li>"Transcription only ($0.72)"</li>
          <li>"Skip enrichment"</li>
          <li>"Don't warn me for this asset type" (remembers)</li></p>
<p>if estimated_cost <= threshold:
      process_without_warning: true</p>
<p>also_warn_for:
    <li>Bulk operations: "Re-enriching 234 videos will cost approximately $89.50"</li>
    <li>Tier upgrades: "Processing this video to Tier 3 will cost approximately $2.15"</li>
<pre><code class="language-">
</strong>Suggested Budgets:<strong>
</code></pre>yaml
Budget Recommendations:
  display_location: Settings, Budget section
  description: "Not sure what to budget? Here are typical ranges:"</p>
<p>profiles:
    light_user:
      description: "Occasional photos, few documents"
      typical_assets: "~50-100 per month"
      suggested_budget: "$5-10/month"
      typical_costs:
        <li>"Photos: ~$0.02 each"</li>
        <li>"Documents: ~$0.05 each"</li>
        <li>"Short audio (<5min): ~$0.05 each"</li></p>
<p>regular_user:
      description: "Daily photos, regular documents, some audio"
      typical_assets: "~200-500 per month"
      suggested_budget: "$15-30/month"
      typical_costs:
        <li>"Photos: ~$0.02 each"</li>
        <li>"Documents: ~$0.05 each"</li>
        <li>"Audio recordings: ~$0.10-0.50 each"</li>
        <li>"Occasional video: ~$1-3 each"</li></p>
<p>power_user:
      description: "Heavy media, life recording, lots of video"
      typical_assets: "~500-2000 per month"
      suggested_budget: "$50-100/month"
      typical_costs:
        <li>"Life recording: ~$0.50-2.00 per hour"</li>
        <li>"Video processing: ~$1-5 each"</li>
        <li>"Batch operations add up"</li></p>
<p>professional:
      description: "Business use, high volume, deep analysis"
      typical_assets: "~2000+ per month"
      suggested_budget: "$100-300/month"
      note: "Consider dedicated API tier pricing"</p>
<p>note: "These are estimates. Your actual costs depend on content types, enrichment depth, and provider pricing."
<pre><code class="language-">
</strong>Detailed Cost Tracking:<strong>
</code></pre>yaml
Cost Attribution:</p>
<p>per_asset_view:
    asset_id: "uuid"
    asset_name: "team_meeting_2026-01-28.mp4"
    total_cost: $2.47
    enrichments:
      <li>type: "audio_extraction"</li>
        provider: "local"
        cost: $0.00
      <li>type: "transcription"</li>
        provider: "openai_whisper"
        input_duration: 45.5 min
        cost: $0.27
      <li>type: "speaker_diarization"</li>
        provider: "openai_whisper"
        cost: $0.14
      <li>type: "scene_analysis"</li>
        provider: "openai_gpt4v"
        frames_analyzed: 28
        cost: $0.84
      <li>type: "ai_summary"</li>
        provider: "anthropic_claude"
        tokens_in: 8500
        tokens_out: 650
        cost: $0.32
      <li>type: "embeddings"</li>
        provider: "openai_ada"
        tokens: 12000
        cost: $0.01
      <li>type: "tag_generation"</li>
        provider: "openai_gpt4"
        cost: $0.12
    cached_savings: $0.45  # Would have cost more without cache hits</p>
<p>daily_view:
    date: "2026-01-28"
    total_spent: $4.23
    assets_processed: 34
    breakdown:
      by_type:
        images: $0.68 (32 assets)
        documents: $0.15 (2 assets)
        audio: $0.00 (0 assets)
        video: $3.40 (1 asset)
      by_enrichment:
        transcription: $0.27
        image_analysis: $0.68
        scene_analysis: $0.84
        summaries: $0.45
        embeddings: $0.12
        other: $1.87
      by_provider:
        openai: $3.45
        anthropic: $0.78
        local: $0.00
    cached_savings: $1.23</p>
<p>monthly_view:
    month: "January 2026"
    total_spent: $47.23
    budget: $50.00
    remaining: $2.77
    projected_end_of_month: $52.10 (over budget)</p>
<p>assets_processed: 342
    average_per_asset: $0.138</p>
<p>by_content_type:
      images: { count: 285, cost: $5.70, avg: $0.02 }
      documents: { count: 43, cost: $2.15, avg: $0.05 }
      audio: { count: 8, cost: $4.80, avg: $0.60 }
      video: { count: 6, cost: $34.58, avg: $5.76 }</p>
<p>by_enrichment_type:
      transcription: $12.45 (26%)
      image_analysis: $8.34 (18%)
      scene_analysis: $10.22 (22%)
      summaries: $6.78 (14%)
      embeddings: $4.22 (9%)
      tag_generation: $3.12 (7%)
      other: $2.10 (4%)</p>
<p>by_provider:
      openai: $35.00 (74%)
      anthropic: $10.23 (22%)
      google: $2.00 (4%)
      local: $0.00 (0%)</p>
<p>daily_trend:
      # Sparkline or chart showing daily spend
      <li>{ date: "Jan 1", cost: $1.20 }</li>
      <li>{ date: "Jan 2", cost: $0.85 }</li>
      # ... etc</p>
<p>cached_savings_total: $18.45
    cache_hit_rate: 28%</p>
<p>all_time_view:
    total_spent: $234.56
    since: "October 2025"
    months_active: 4
    average_monthly: $58.64
    total_assets: 1,847
    total_cached_savings: $67.23
<pre><code class="language-">
</strong>Cache Savings Display:<strong>
</code></pre>yaml
Cache Savings:
  visibility: Shown in cost reports and on individual assets</p>
<p>per_asset:
    message: "Saved $0.45 by using cached results"
    details:
      <li>"Embedding reused from similar content: $0.30"</li>
      <li>"Provider response cached from duplicate: $0.15"</li></p>
<p>monthly_summary:
    total_saved: $18.45
    cache_hit_rate: 28%
    message: "Caching saved you $18.45 this month (28% of requests served from cache)"</p>
<p>explanation:
    "When we've already processed similar content, we reuse the results instead
     of calling the AI provider again. This saves you money and speeds up processing."</p>
<p>breakdown:
    embedding_reuse: $8.23
    identical_content: $4.12
    similar_image_match: $3.45
    provider_response_cache: $2.65
<pre><code class="language-">
</strong>Budget Controls:<strong>
</code></pre>yaml
Budget Settings:
  monthly_budget:
    amount: $50.00
    alerts_at: [$25.00, $40.00, $45.00]  # 50%, 80%, 90%
    hard_stop_at: $50.00  # or null for no hard stop</p>
<p>per_asset_warning: $1.00  # warn before processing (user configurable)
  per_asset_hard_limit: $10.00  # never exceed per asset
  per_enrichment_limit: $5.00  # never exceed per enrichment type</p>
<p>alert_behavior:
    at_50_percent:
      notification: "You've used $25 of your $50 monthly budget"
      action: none</p>
<p>at_80_percent:
      notification: "You've used $40 of your $50 budget. $10 remaining."
      action: none</p>
<p>at_90_percent:
      notification: "You've used $45 of your $50 budget. Consider adjusting."
      action: Suggest reducing auto-processing tiers</p>
<p>at_100_percent:
      notification: "Monthly budget reached"
      action: Pause external processing (if hard stop enabled)
      options:
        <li>"Increase budget"</li>
        <li>"Continue with local processing only"</li>
        <li>"Wait until next month"</li>
        <li>"Process this one asset anyway"</li></p>
<p>when_budget_exceeded:
    <li>Pause non-critical enrichments</li>
    <li>Queue for next month (if user chooses)</li>
    <li>Continue with local-only processing</li>
    <li>Allow manual override per-asset</li>
    <li>Never lose data, just pause enrichment</li></p>
<p>Cost Optimization Tips:
  shown_when: Budget is tight or frequently exceeded
  suggestions:
    <li>"Videos are your biggest cost (72%). Consider Tier 0 default for videos."</li>
    <li>"You could save ~$12/month by using local transcription (quality tradeoff)."</li>
    <li>"23 duplicate images detected - deduplication could save ~$0.46/month"</li>
    <li>"Reducing image analysis to Tier 2 would save ~$3/month"</li>
<pre><code class="language-">
</strong>ROI Tracking:<strong>
</code></pre>yaml
Value Metrics Dashboard:
  description: "See what you're getting for your enrichment investment"</p>
<p>time_saved:
    total_estimated: "4.2 hours this month"
    breakdown:
      <li>"Manual tagging avoided: 2.3 hours (1,234 tags auto-applied)"</li>
      <li>"Transcription time saved: 1.2 hours (8 recordings)"</li>
      <li>"Document filing: 0.7 hours (43 documents auto-classified)"</li></p>
<p>automation_stats:
    action_items_extracted: 45
    documents_auto_filed: 123
    tags_auto_applied: 1,234
    summaries_generated: 67
    duplicates_caught: 12</p>
<p>quality_improvements:
    search_enhancement: "Content now searchable that wasn't before"
    related_discoveries: "34 related entries surfaced"
    life_steward_filings: "89% auto-filed correctly"</p>
<p>cost_efficiency:
    cost_per_action_item: "$0.02"
    cost_per_document_filed: "$0.05"
    cost_per_hour_saved: "$11.24"
    cost_per_searchable_minute: "$0.006"</p>
<p>comparison:
    vs_manual: "Equivalent manual work would take ~4.2 hours"
    vs_human_cost: "At $25/hr, that's $105 of work for $47.23"
    roi: "2.2x return on enrichment investment"
<pre><code class="language-">
<h3>External Data Source Integration</h3></p>
</strong>Philosophy: All integrations off by default, user opts in to what they want.<strong>
</code></pre>yaml
Settings Location: Privacy & Integrations â†’ External Data Sources
<p>Default State: All disabled
User Action Required: Enable each integration individually
Disclosure: Clear explanation of what data is sent where
<pre><code class="language-">
</strong>MVP External Integrations:<strong></p>
</code></pre>yaml
Music Recognition:
  enabled_by_default: false
  providers:
    <li>Shazam API (preferred for accuracy)</li>
    <li>ACRCloud (backup)</li>
    <li>MusicBrainz (free, for metadata enrichment)</li>
<p>extracted_data:
    <li>Song title, artist, album</li>
    <li>Release year, genre</li>
    <li>Lyrics (if available)</li>
    <li>Related songs/artists</li></p>
<p>integration:
    <li>Link to Spotify/Apple Music</li>
    <li>Add to "music listened to" tracking (if enabled)</li>
    <li>Tag with genre, mood, era</li>
    <li>Auto-create Consume Later entry (if enabled)</li></p>
<p>user_setting:
    description: "Identify songs in your audio and video content"
    data_sent: "Audio fingerprint (not full audio) sent to music recognition service"</p>
<p>Movie/TV Recognition:
  enabled_by_default: false
  providers:
    <li>TMDB API</li>
    <li>IMDB (via OMDb API)</li>
    <li>Gracenote (for video fingerprinting)</li></p>
<p>extracted_data:
    <li>Title, year, director, cast</li>
    <li>Genre, plot summary</li>
    <li>Episode info (for TV)</li>
    <li>Ratings, reviews</li></p>
<p>integration:
    <li>Link to streaming service</li>
    <li>Add to "watched" tracking (if enabled)</li>
    <li>Connect to Consume Later system</li>
    <li>Auto-create Consume Later entry (if enabled)</li></p>
<p>user_setting:
    description: "Identify movies and TV shows in your videos"
    data_sent: "Video/audio fingerprint sent to identification service"</p>
<p>Book Recognition:
  enabled_by_default: false
  providers:
    <li>Google Books API</li>
    <li>Open Library API</li>
    <li>ISBN lookup services</li></p>
<p>extracted_data:
    <li>Title, author, publisher</li>
    <li>ISBN, publication date</li>
    <li>Description, categories</li>
    <li>Page count, reading level</li></p>
<p>integration:
    <li>Track reading progress</li>
    <li>Link to Goodreads/library</li>
    <li>Add to reading list</li>
    <li>Auto-create Consume Later entry (if enabled)</li></p>
<p>user_setting:
    description: "Identify books from photos of covers or ISBN barcodes"
    data_sent: "Book cover image or ISBN sent to book database"</p>
<p>Geographic & Location Services:
  enabled_by_default: false
  providers:
    <li>OpenStreetMap / Nominatim (privacy-friendly)</li>
    <li>Google Maps API (more accurate)</li>
    <li>GeoNames</li></p>
<p>extracted_data:
    from_gps:
      <li>Reverse geocode to address</li>
      <li>Place name (restaurant, park, etc.)</li>
      <li>Country, city, neighborhood</li>
      <li>Time zone</li></p>
<p>from_content:
      <li>Landmark recognition â†’ location</li>
      <li>Address extraction from documents</li>
      <li>Business name â†’ location lookup</li></p>
<p>integration:
    <li>Auto-tag with location</li>
    <li>Group photos by place</li>
    <li>Travel timeline creation</li></p>
<p>user_setting:
    description: "Convert GPS coordinates to place names and identify landmarks"
    data_sent: "GPS coordinates or landmark images sent to location service"</p>
<p>Public Figure Recognition:
  enabled_by_default: false
  providers:
    <li>Wikipedia/Wikidata</li>
    <li>News databases</li></p>
<p>extracted_data:
    <li>Name, profession, bio</li>
    <li>Notable works, achievements</li>
    <li>Related people</li></p>
<p>integration:
    <li>Tag photos with identified celebrities/politicians</li>
    <li>Link to Wikipedia articles</li>
    <li>Context for news content</li></p>
<p>user_setting:
    description: "Identify celebrities, politicians, and public figures in photos"
    data_sent: "Face embeddings compared against public figure database"
    note: "Only matches against known public figures, not private individuals"</p>
<p>Business/Organization Lookup:
  enabled_by_default: false
  providers:
    <li>Clearbit</li>
    <li>Company databases</li>
    <li>Website verification services</li></p>
<p>extracted_data:
    <li>Company name, industry</li>
    <li>Website, social profiles</li>
    <li>Size, location</li>
    <li>Logo matching</li></p>
<p>integration:
    <li>Enrich business contacts</li>
    <li>Tag with industry/sector</li>
    <li>Link to company profiles</li></p>
<p>user_setting:
    description: "Look up information about companies and organizations"
    data_sent: "Company name or logo sent to business database"
<pre><code class="language-">
<hr></p>
<h3>Post-MVP: Extended External Data Source Integrations</h3>
</strong>All integrations follow the same pattern: off by default, user opts in, clear disclosure of data sent.<strong>
<h4>Content Identification Services</h4>
</code></pre>yaml
Podcast Recognition:
  providers:
    <li>Apple Podcasts API</li>
    <li>Spotify Podcast API</li>
    <li>Podchaser</li>
    <li>Listen Notes</li>
<p>extracted_data:
    <li>Show name, episode title</li>
    <li>Host(s), guest(s)</li>
    <li>Episode description, show notes</li>
    <li>Categories, duration</li>
    <li>Links mentioned in episode</li></p>
<p>integration:
    <li>Track listening history</li>
    <li>Link to show notes/transcript</li>
    <li>Auto-create Consume Later entry</li>
    <li>Connect to podcast app</li></p>
<p>user_setting:
    description: "Identify podcast episodes from audio"
    data_sent: "Audio fingerprint sent to podcast identification service"</p>
<p>News/Article Recognition:
  providers:
    <li>News API</li>
    <li>Media bias databases</li>
    <li>Fact-checking services (Snopes, PolitiFact)</li></p>
<p>extracted_data:
    <li>Publication, author</li>
    <li>Publication date</li>
    <li>Bias rating (if available)</li>
    <li>Fact-check status (if available)</li>
    <li>Related articles</li></p>
<p>integration:
    <li>Source credibility context</li>
    <li>Related coverage</li>
    <li>Fact-check warnings</li></p>
<p>user_setting:
    description: "Identify news sources and check for fact-checking information"
    data_sent: "Article URL or text excerpt sent to news databases"</p>
<p>Recipe Recognition:
  providers:
    <li>Spoonacular API</li>
    <li>Edamam</li>
    <li>Recipe databases</li></p>
<p>extracted_data:
    <li>Recipe name, cuisine type</li>
    <li>Ingredients list</li>
    <li>Nutritional information</li>
    <li>Cooking time, difficulty</li>
    <li>Similar recipes</li></p>
<p>integration:
    <li>Save recipes to collection</li>
    <li>Meal planning integration</li>
    <li>Shopping list generation</li>
    <li>Nutrition tracking</li></p>
<p>user_setting:
    description: "Identify recipes from food photos or screenshots"
    data_sent: "Food image sent to recipe recognition service"</p>
<p>Wine/Beer Recognition:
  providers:
    <li>Vivino API</li>
    <li>Untappd API</li>
    <li>Wine databases</li></p>
<p>extracted_data:
    <li>Name, producer, vintage</li>
    <li>Varietal, region</li>
    <li>Ratings, reviews</li>
    <li>Tasting notes</li>
    <li>Price range</li></p>
<p>integration:
    <li>Track what you've tried</li>
    <li>Personal ratings</li>
    <li>Wishlist/cellar management</li>
    <li>Food pairing suggestions</li></p>
<p>user_setting:
    description: "Identify wines and beers from label photos"
    data_sent: "Label image sent to beverage database"</p>
<p>Meme Recognition:
  providers:
    <li>Know Your Meme API</li>
    <li>Meme template databases</li></p>
<p>extracted_data:
    <li>Meme template name</li>
    <li>Origin, meaning</li>
    <li>Cultural context</li>
    <li>Related memes</li></p>
<p>integration:
    <li>Understand meme context</li>
    <li>Tag with meme type</li>
    <li>Cultural reference tracking</li></p>
<p>user_setting:
    description: "Identify meme templates and provide cultural context"
    data_sent: "Image sent to meme database"</p>
<p>Art Recognition:
  providers:
    <li>Google Arts & Culture</li>
    <li>WikiArt</li>
    <li>Museum APIs</li></p>
<p>extracted_data:
    <li>Artwork title, artist</li>
    <li>Year, medium, dimensions</li>
    <li>Museum/collection location</li>
    <li>Art movement, style</li>
    <li>Historical context</li></p>
<p>integration:
    <li>Museum visit tracking</li>
    <li>Art collection/favorites</li>
    <li>Artist exploration</li></p>
<p>user_setting:
    description: "Identify paintings, sculptures, and artworks"
    data_sent: "Artwork image sent to art database"
<pre><code class="language-">
<h4>Product/Object Identification Services</h4></p>
</code></pre>yaml
Barcode/UPC Lookup:
  providers:
    <li>UPCitemdb</li>
    <li>Barcode Lookup</li>
    <li>Open Food Facts (for food)</li>
<p>extracted_data:
    <li>Product name, brand</li>
    <li>Category, description</li>
    <li>Images, specifications</li>
    <li>Nutritional info (food)</li>
    <li>Current prices (optional)</li></p>
<p>integration:
    <li>Product inventory tracking</li>
    <li>Shopping list integration</li>
    <li>Price comparison</li>
    <li>Reorder reminders</li></p>
<p>user_setting:
    description: "Look up product information from barcodes"
    data_sent: "Barcode number sent to product database"</p>
<p>Plant Identification:
  providers:
    <li>PlantNet API</li>
    <li>iNaturalist</li>
    <li>Plant.id</li></p>
<p>extracted_data:
    <li>Species name (common and scientific)</li>
    <li>Plant family</li>
    <li>Care instructions</li>
    <li>Toxicity warnings (pets/children)</li>
    <li>Native region</li></p>
<p>integration:
    <li>Garden tracking</li>
    <li>Plant care reminders</li>
    <li>Nature journal</li>
    <li>Hiking/outdoor logging</li></p>
<p>user_setting:
    description: "Identify plants, flowers, and trees from photos"
    data_sent: "Plant image sent to identification service"</p>
<p>Food/Nutrition Analysis:
  providers:
    <li>Nutritionix</li>
    <li>CalorieNinjas</li>
    <li>FatSecret</li></p>
<p>extracted_data:
    <li>Food items identified</li>
    <li>Calories, macros</li>
    <li>Micronutrients</li>
    <li>Serving size estimate</li></p>
<p>integration:
    <li>Meal logging</li>
    <li>Nutrition tracking</li>
    <li>Diet goal progress</li>
    <li>Health app sync</li></p>
<p>user_setting:
    description: "Analyze food photos for nutritional information"
    data_sent: "Food image sent to nutrition analysis service"</p>
<p>Medication Identification:
  providers:
    <li>NIH Pill Identifier</li>
    <li>Drugs.com</li>
    <li>RxList</li></p>
<p>extracted_data:
    <li>Medication name</li>
    <li>Dosage, strength</li>
    <li>Manufacturer</li>
    <li>Common uses</li>
    <li>Warnings, interactions</li></p>
<p>integration:
    <li>Medication tracking</li>
    <li>Interaction warnings</li>
    <li>Refill reminders</li>
    <li>Health record</li></p>
<p>user_setting:
    description: "Identify pills and medications from photos"
    data_sent: "Pill image sent to medication database"
    warning: "For informational purposes only. Always verify with pharmacist."</p>
<p>Animal/Species Identification:
  providers:
    <li>iNaturalist</li>
    <li>Merlin Bird ID</li>
    <li>Google Lens species detection</li></p>
<p>extracted_data:
    <li>Species name</li>
    <li>Classification (family, genus)</li>
    <li>Habitat, range</li>
    <li>Conservation status</li>
    <li>Interesting facts</li></p>
<p>integration:
    <li>Nature journal</li>
    <li>Wildlife sighting log</li>
    <li>Birdwatching tracking</li>
    <li>Pet identification</li></p>
<p>user_setting:
    description: "Identify animals, birds, and insects from photos"
    data_sent: "Animal image sent to species identification service"</p>
<p>Vehicle Identification:
  providers:
    <li>CarNet API</li>
    <li>Vehicle databases</li></p>
<p>extracted_data:
    <li>Make, model, year</li>
    <li>Body style, trim level</li>
    <li>Estimated value</li>
    <li>Specifications</li></p>
<p>integration:
    <li>Vehicle tracking (owned)</li>
    <li>Car shopping research</li>
    <li>Accident documentation</li></p>
<p>user_setting:
    description: "Identify car make/model from photos"
    data_sent: "Vehicle image sent to car database"
<pre><code class="language-">
<h4>Location/Business Services</h4></p>
</code></pre>yaml
Restaurant/Business Lookup:
  providers:
    <li>Yelp API</li>
    <li>Google Places</li>
    <li>Foursquare</li>
<p>extracted_data:
    <li>Business name, type</li>
    <li>Address, hours</li>
    <li>Ratings, reviews</li>
    <li>Price range</li>
    <li>Menu (restaurants)</li>
    <li>Photos</li></p>
<p>integration:
    <li>Remember places visited</li>
    <li>Personal ratings/notes</li>
    <li>Recommendations</li>
    <li>Receipt matching</li></p>
<p>user_setting:
    description: "Look up restaurants and businesses from photos or locations"
    data_sent: "Location or business image sent to business database"</p>
<p>Event Identification:
  providers:
    <li>Eventbrite API</li>
    <li>Meetup API</li>
    <li>Ticketmaster</li>
    <li>Bandsintown</li></p>
<p>extracted_data:
    <li>Event name, type</li>
    <li>Date, time, venue</li>
    <li>Performers/speakers</li>
    <li>Ticket info</li></p>
<p>integration:
    <li>Event memory tagging</li>
    <li>Calendar linking</li>
    <li>Concert/event history</li>
    <li>Ticket stub archiving</li></p>
<p>user_setting:
    description: "Identify events from tickets, photos, or dates"
    data_sent: "Ticket image or event details sent to event database"</p>
<p>Travel/Destination Lookup:
  providers:
    <li>TripAdvisor</li>
    <li>Lonely Planet</li>
    <li>Atlas Obscura</li></p>
<p>extracted_data:
    <li>Destination info</li>
    <li>Attractions, activities</li>
    <li>Travel tips</li>
    <li>Best times to visit</li></p>
<p>integration:
    <li>Trip planning</li>
    <li>Travel memories</li>
    <li>Place recommendations</li></p>
<p>user_setting:
    description: "Get travel information for destinations in your photos"
    data_sent: "Location data sent to travel database"
<pre><code class="language-">
<h4>Reference/Knowledge Services</h4></p>
</code></pre>yaml
Wikipedia/Wikidata Enrichment:
  providers:
    <li>Wikipedia API</li>
    <li>Wikidata</li>
    <li>DBpedia</li>
<p>extracted_data:
    <li>Summary/overview</li>
    <li>Key facts</li>
    <li>Related topics</li>
    <li>Citations/sources</li></p>
<p>integration:
    <li>Context for any detected entity</li>
    <li>Quick facts overlay</li>
    <li>Learning/research support</li></p>
<p>user_setting:
    description: "Add Wikipedia context to detected people, places, and things"
    data_sent: "Entity names sent to Wikipedia"</p>
<p>Translation Services:
  providers:
    <li>DeepL</li>
    <li>Google Translate</li>
    <li>LibreTranslate (open source)</li></p>
<p>extracted_data:
    <li>Translated text</li>
    <li>Source language detection</li>
    <li>Alternative translations</li></p>
<p>integration:
    <li>Translate detected foreign text</li>
    <li>Multi-language document support</li>
    <li>Travel photo context</li></p>
<p>user_setting:
    description: "Translate foreign text detected in documents and images"
    data_sent: "Text sent to translation service"</p>
<p>Academic/Scientific Lookup:
  providers:
    <li>Google Scholar</li>
    <li>Semantic Scholar</li>
    <li>PubMed (medical)</li>
    <li>arXiv (preprints)</li></p>
<p>extracted_data:
    <li>Paper metadata</li>
    <li>Citations, impact</li>
    <li>Author info</li>
    <li>Related research</li></p>
<p>integration:
    <li>Research paper tracking</li>
    <li>Citation management</li>
    <li>Academic reading list</li></p>
<p>user_setting:
    description: "Look up academic papers and scientific references"
    data_sent: "Paper title or DOI sent to academic database"
<pre><code class="language-">
<h4>Post-MVP Integration Settings UI</h4></p>
</code></pre>yaml
External Integrations Settings Page:
<p>layout:
    categories:
      <li>"Content Identification" (Music, Movies, Podcasts, etc.)</li>
      <li>"Products & Objects" (Barcodes, Plants, Food, etc.)</li>
      <li>"Places & Businesses" (Restaurants, Events, Travel)</li>
      <li>"Reference & Knowledge" (Wikipedia, Translation, Academic)</li></p>
<p>per_integration:
    toggle: Enable/Disable
    description: What it does
    data_disclosure: What data is sent where
    sub_options:
      <li>Auto-create Consume Later entries: on/off</li>
      <li>Add to tracking history: on/off</li>
      <li>Provider preference (if multiple)</li></p>
<p>bulk_actions:
    <li>"Enable all content identification"</li>
    <li>"Enable all in category"</li>
    <li>"Disable all external integrations"</li></p>
<p>privacy_summary:
    "You have enabled 5 external integrations. Your data is sent to:
     Shazam, TMDB, Google Books, OpenStreetMap, Wikipedia"
<pre><code class="language-">
<h3>Cross-Asset Intelligence</h3></p>
</strong>Hierarchical Event Detection:<strong>
</code></pre>yaml
Event Hierarchy:
  concept: Events can contain sub-events, and span multiple days
<p>example_structure:
    "Beach Vacation 2026" (Jan 15-18):
      â”œâ”€â”€ "Tuesday Jan 15":
      â”‚   â”œâ”€â”€ "Drive to Beach" (8 photos, 1 video)
      â”‚   â”œâ”€â”€ "Beach Sunset" (23 photos)
      â”‚   â””â”€â”€ "Dinner at Seafood Place" (5 photos, receipt)
      â”œâ”€â”€ "Wednesday Jan 16":
      â”‚   â”œâ”€â”€ "Morning Beach Walk" (12 photos)
      â”‚   â”œâ”€â”€ "Kayaking Trip" (8 photos, 2 videos)
      â”‚   â””â”€â”€ "Evening Bonfire" (15 photos, 1 video)
      â”œâ”€â”€ "Thursday Jan 17":
      â”‚   â””â”€â”€ ...
      â””â”€â”€ "Friday Jan 18":
          â””â”€â”€ "Drive Home" (3 photos)</p>
<p>detection_signals:
    multi_day_event:
      <li>Continuous travel (away from home GPS)</li>
      <li>Similar participants across days</li>
      <li>Related locations (same region/destination)</li>
      <li>Calendar event spanning multiple days</li></p>
<p>single_day_grouping:
      <li>Same calendar day</li>
      <li>Groups into sub-events by time gaps, location changes</li></p>
<p>sub_event_detection:
      <li>Location change within day</li>
      <li>Activity change (beach â†’ restaurant)</li>
      <li>Time gaps > 1 hour</li>
      <li>Different participant groups</li></p>
<p>output:
    <li>Hierarchical event structure</li>
    <li>Each level has: title, summary, date range, assets, sub-events</li>
    <li>Can navigate up/down hierarchy</li>
    <li>Can flatten to single album if preferred</li></p>
<p>user_control:
    <li>Accept/modify suggested hierarchy</li>
    <li>Merge or split events</li>
    <li>Move assets between sub-events</li>
    <li>Change event titles</li>
    <li>Collapse/expand hierarchy</li>
<pre><code class="language-">
</strong>Story/Album Generation:<strong>
</code></pre>yaml
Auto-Album Detection:
  triggers:
    <li>Multiple photos from same timeframe</li>
    <li>Travel (different location than home)</li>
    <li>Special event (detected celebration, ceremony)</li>
    <li>People grouping (photos with specific person)</li></p>
<p>output:
    <li>Suggested album with cover photo</li>
    <li>Chronological arrangement</li>
    <li>Highlight selection</li>
    <li>Narrative summary</li></p>
<p>user_control:
    <li>Accept, modify, or dismiss suggestion</li>
    <li>Add/remove photos</li>
    <li>Change cover and title</li>
<pre><code class="language-">
</strong>Duplicate &amp; Near-Duplicate Detection:<strong>
</code></pre>yaml
Duplicate Types:
  exact_duplicate:
    <li>Same file hash</li>
    <li>Default action: Link together, notify user</li>
    <li>User setting: Auto-delete lower quality duplicate</li></p>
<p>near_duplicate_image:
    <li>Perceptual hash similarity >95%</li>
    <li>Default action: Group, link together, let user decide</li>
    <li>User setting: Auto-keep best quality, delete others</li></p>
<p>same_content_different_format:
    <li>PDF and DOCX of same document</li>
    <li>Action: Link together, note relationship</li></p>
<p>version_detection:
    <li>Document_v1.pdf, Document_v2.pdf</li>
    <li>Action: Create version history</li></p>
<p>burst_photos:
    <li>Multiple shots in rapid succession</li>
    <li>Default action: Group, suggest best shot</li>
    <li>User setting: Auto-keep best, delete others</li></p>
<p>User Settings:
  duplicate_handling:
    options:
      <li>"Link and let me decide" (default)</li>
      <li>"Auto-delete lower quality duplicates"</li>
      <li>"Auto-delete exact duplicates only"</li>
      <li>"Just link, never suggest deletion"</li></p>
<p>quality_determination:
    factors:
      <li>Resolution (higher = better)</li>
      <li>File size (for same resolution)</li>
      <li>Format (lossless > lossy)</li>
      <li>Sharpness detection</li>
      <li>Exposure quality</li>
    user_override: Can always recover from trash
<pre><code class="language-">
<hr></p>
<h3>Post-MVP: Extended Cross-Asset Intelligence</h3>
</strong>All cross-asset intelligence features follow the same pattern: detected automatically, surfaced as suggestions, user confirms or dismisses.<strong>
</code></pre>yaml
Temporal Correlation:
  description: "This photo was taken during this meeting"
  detection:
    <li>Timestamp of photo falls within duration of audio/video recording</li>
    <li>Photo GPS matches meeting location</li>
    <li>Photo content relates to meeting topics</li>
<p>output:
    <li>"This photo appears to be from your 'Team Planning' meeting"</li>
    <li>Suggest linking photo to meeting entry</li>
    <li>Show on meeting timeline</li></p>
<p>examples:
    <li>Photo of whiteboard â†’ linked to meeting where it was captured</li>
    <li>Screenshot â†’ linked to call where it was discussed</li>
    <li>Photo of document â†’ linked to meeting about that document</li></p>
<p>user_control:
    <li>Confirm or dismiss suggestion</li>
    <li>Manually link assets across time</li></p>
<p>Document Version Detection:
  description: "These appear to be versions of the same document"
  detection:
    <li>Similar filenames (proposal_v1.pdf, proposal_v2.pdf)</li>
    <li>Similar content with differences (diff analysis)</li>
    <li>Same title/subject, different dates</li>
    <li>Metadata indicates same original</li></p>
<p>output:
    <li>Create version history chain</li>
    <li>Show diff between versions</li>
    <li>Identify latest version</li>
    <li>Track who/when changes were made (if detectable)</li></p>
<p>examples:
    <li>Contract drafts through negotiation</li>
    <li>Resume versions over time</li>
    <li>Report iterations</li>
    <li>Code file versions (outside git)</li></p>
<p>user_control:
    <li>Confirm version relationship</li>
    <li>Reorder version sequence</li>
    <li>Break incorrect version links</li></p>
<p>Derivative Detection:
  description: "This image was derived from this original"
  detection:
    <li>Cropped version of larger image</li>
    <li>Filtered/edited version</li>
    <li>Screenshot of original</li>
    <li>Compressed version</li>
    <li>Format conversion (PNG â†’ JPG)</li></p>
<p>output:
    <li>Link derivative to original</li>
    <li>Show transformation chain</li>
    <li>Identify "original" master</li>
    <li>Track edit history</li></p>
<p>examples:
    <li>Instagram crop â†’ original photo</li>
    <li>Thumbnail â†’ full resolution</li>
    <li>Edited portrait â†’ raw file</li>
    <li>PDF export â†’ original document</li></p>
<p>user_control:
    <li>Confirm relationship</li>
    <li>Designate which is "master"</li>
    <li>Keep or delete derivatives</li></p>
<p>Content Continuity Detection:
  description: "These could be combined or are part of same sequence"
  detection:
    <li>Video clips from same recording session</li>
    <li>Burst photos of same moment</li>
    <li>Audio recordings that continue from each other</li>
    <li>Document pages that form complete document</li></p>
<p>output:
    <li>Suggest merging/combining</li>
    <li>Create sequence view</li>
    <li>Offer to stitch together</li></p>
<p>examples:
    <li>3 video clips from same event â†’ offer to combine</li>
    <li>47 burst photos â†’ offer best selection</li>
    <li>Split audio recording â†’ offer to merge</li>
    <li>Scanned pages â†’ offer to combine into single PDF</li></p>
<p>user_control:
    <li>Merge or keep separate</li>
    <li>Select which to keep from sequence</li>
    <li>Reorder sequence</li></p>
<p>Project Association:
  description: "This asset appears related to this project"
  detection:
    <li>Content mentions project name/keywords</li>
    <li>People involved match project participants</li>
    <li>Topic analysis matches project scope</li>
    <li>Temporal proximity to project activity</li>
    <li>Location matches project context</li></p>
<p>output:
    <li>Suggest linking to project</li>
    <li>Show confidence and reasoning</li>
    <li>Offer to auto-file in project</li></p>
<p>examples:
    <li>Photo of construction â†’ Kitchen Remodel project</li>
    <li>Meeting recording mentioning "Q2 launch" â†’ Product Launch project</li>
    <li>Receipt from Home Depot â†’ Kitchen Remodel project</li>
    <li>Email about contract â†’ Client Project</li></p>
<p>integration_with_life_steward:
    <li>Feeds into Life Steward's auto-filing</li>
    <li>Project association confidence affects filing confidence</li>
    <li>User confirmations improve future detection</li></p>
<p>user_control:
    <li>Confirm or dismiss association</li>
    <li>Correct project assignment</li>
    <li>"Never associate this type of content with projects"</li></p>
<p>People Co-occurrence Tracking:
  description: "These people frequently appear together"
  detection:
    <li>Face recognition across photos</li>
    <li>Voice recognition across audio</li>
    <li>Name mentions in same content</li></p>
<p>output:
    <li>"Sarah and John appear together in 47 photos"</li>
    <li>Relationship inference (colleagues, family, friends)</li>
    <li>Shared event history</li></p>
<p>privacy_note: "Requires face/voice recognition to be enabled"</p>
<p>user_control:
    <li>Enable/disable this feature separately</li>
    <li>View and delete co-occurrence data</li>
    <li>Correct relationship inferences</li></p>
<p>Conversation Threading:
  description: "This conversation is a follow-up to a previous one"
  detection:
    <li>Explicit references ("following up on yesterday's call")</li>
    <li>Same participants discussing same topic</li>
    <li>Temporal patterns (weekly 1:1s)</li>
    <li>Topic continuity across recordings</li></p>
<p>output:
    <li>Link related conversations</li>
    <li>Show conversation history/thread</li>
    <li>Track topic evolution over time</li>
    <li>"This is your 5th conversation about kitchen remodel"</li></p>
<p>examples:
    <li>Monday standup â†’ linked to previous standups</li>
    <li>Follow-up call â†’ linked to original call</li>
    <li>Negotiation calls â†’ threaded as single negotiation</li></p>
<p>user_control:
    <li>Confirm or break thread links</li>
    <li>Manually link conversations</li>
    <li>View full thread history</li></p>
<p>Reference Detection:
  description: "This asset references content in another asset"
  detection:
    <li>Photo of screen showing document</li>
    <li>Whiteboard photo discussed in meeting</li>
    <li>Document mentioned by name in audio</li>
    <li>Screenshot of conversation</li></p>
<p>output:
    <li>Create reference link</li>
    <li>"This whiteboard photo is referenced at 23:45 in Team Meeting"</li>
    <li>Cross-reference navigation</li></p>
<p>examples:
    <li>Meeting audio: "look at the diagram John drew" â†’ link to whiteboard photo</li>
    <li>Document: "see attached spreadsheet" â†’ link to spreadsheet</li>
    <li>Video: screen recording showing email â†’ link to email</li></p>
<p>user_control:
    <li>Confirm reference links</li>
    <li>Manually create references</li>
    <li>Navigate between referenced items</li></p>
<p>Source Tracking:
  description: "Where did this file originally come from?"
  detection:
    <li>Email attachment â†’ track sender, email</li>
    <li>Download â†’ track URL, date</li>
    <li>Camera â†’ track device, settings</li>
    <li>Screenshot â†’ track source app/URL</li>
    <li>Airdrop/share â†’ track sender</li>
    <li>Copy/paste â†’ track source app</li></p>
<p>output:
    <li>Provenance metadata on each asset</li>
    <li>"Received from john@example.com on Jan 15"</li>
    <li>"Downloaded from dropbox.com/shared/..."</li>
    <li>"Screenshot of Slack conversation"</li></p>
<p>examples:
    <li>PDF attachment â†’ linked to email it came from</li>
    <li>Downloaded image â†’ URL preserved</li>
    <li>Screenshot â†’ app and context captured</li></p>
<p>user_control:
    <li>View provenance info</li>
    <li>Edit/correct source</li>
    <li>Clear source tracking for privacy</li></p>
<p>Similar Content Clustering:
  description: "Group assets by content type or subject"
  detection:
    <li>Visual similarity (all food photos)</li>
    <li>Document type (all receipts)</li>
    <li>Subject matter (all photos of dog)</li>
    <li>Format/style (all screenshots)</li></p>
<p>output:
    <li>Smart folders/views by cluster</li>
    <li>"You have 234 food photos"</li>
    <li>"You have 89 receipts from 2025"</li>
    <li>Bulk organization options</li></p>
<p>clusters:
    <li>Receipts</li>
    <li>Screenshots</li>
    <li>Food photos</li>
    <li>Selfies</li>
    <li>Documents (by type)</li>
    <li>Nature/landscape</li>
    <li>People photos</li>
    <li>Memes</li>
    <li>Product photos</li></p>
<p>user_control:
    <li>View clusters</li>
    <li>Bulk tag/move/delete clusters</li>
    <li>Exclude items from clusters</li>
    <li>Create custom clusters</li></p>
<p>Contextual Grouping:
  description: "All assets related to a specific topic or project"
  detection:
    <li>Combines multiple signals:</li>
      <li>Explicit tags/filing</li>
      <li>Content analysis</li>
      <li>Temporal proximity</li>
      <li>People involved</li>
      <li>Location</li>
      <li>Project association</li></p>
<p>output:
    <li>"Everything related to Kitchen Remodel" view</li>
    <li>Cross-cuts all asset types</li>
    <li>Timeline of project-related content</li></p>
<p>examples:
    <li>Kitchen Remodel: photos, receipts, contracts, recordings, notes</li>
    <li>Wedding Planning: venue photos, vendor calls, guest list, inspiration</li>
    <li>Job Search: resumes, cover letters, interview recordings, offer letters</li></p>
<p>user_control:
    <li>View contextual groups</li>
    <li>Add/remove from groups</li>
    <li>Create manual contextual groups</li>
<pre><code class="language-">
<h3>Observability &amp; Monitoring</h3></p>
</strong>Philosophy: Full transparency - all metrics visible to users.<strong>
</strong>MVP Health Dashboard (User-Visible):<strong>
</code></pre>yaml
Dashboard Location: Settings â†’ Enrichment Status
<p>Current Status:
  overall_health: "Healthy" | "Degraded" | "Issues Detected"
  status_message: "All systems operating normally"</p>
<p>Queue Status:
  items_in_queue: 23
  by_tier:
    tier_0: 0 (immediate)
    tier_1: 5 (processing)
    tier_2: 12 (queued)
    tier_3: 6 (queued)
  oldest_item: "2 hours ago"
  estimated_clear_time: "~45 minutes"</p>
<p>Processing Activity:
  last_24_hours:
    assets_processed: 45
    enrichments_completed: 312
    average_time_per_asset: "23 seconds"
  currently_processing:
    <li>"team_meeting.mp4" - Transcription (45% complete)</li>
    <li>"vacation_photo_023.jpg" - Tag generation</li></p>
<p>Error Summary:
  last_24_hours:
    total_errors: 6
    error_rate: 0.02%
    by_type:
      provider_timeout: 3
      parse_error: 2
      rate_limit: 1
  recent_errors:
    <li>"image_045.jpg - OpenAI rate limited, will retry in 5 min"</li>
    <li>"document.pdf - Failed to parse, marked for review"</li></p>
<p>Provider Status:
  openai:
    status: "Healthy"
    latency: "234ms avg"
    last_success: "2 minutes ago"
  anthropic:
    status: "Healthy"
    latency: "189ms avg"
  local_whisper:
    status: "Not configured"</p>
<p>Cost Summary:
  today: $4.23
  this_week: $28.45
  this_month: $47.23
  budget: $50.00
  remaining: $2.77
  projected_end_of_month: $52.10 (over budget)
<pre><code class="language-">
</strong>Alerting (User-Configurable):<strong>
</code></pre>yaml
Alert Settings:
  location: Settings â†’ Notifications â†’ Enrichment Alerts</p>
<p>alert_types:
    processing_stuck:
      description: "Alert when enrichment queue is stuck"
      default: enabled
      threshold: "No progress for 30 minutes"</p>
<p>provider_issues:
      description: "Alert when AI provider is having issues"
      default: enabled
      threshold: "Provider errors for 10+ minutes"</p>
<p>budget_alerts:
      description: "Alert when approaching budget limit"
      default: enabled
      thresholds: [50%, 80%, 90%, 100%]</p>
<p>processing_complete:
      description: "Alert when large batch processing completes"
      default: enabled
      threshold: "Batch of 10+ items completes"</p>
<p>quality_concerns:
      description: "Alert when enrichment quality seems degraded"
      default: disabled
      threshold: "Confidence scores dropping significantly"</p>
<p>alert_channels:
    in_app: enabled (always)
    email:
      enabled: false (default)
      frequency: "immediate" | "daily_digest" | "weekly_digest"
    webhook:
      enabled: false
      url: null
      note: "For advanced users / integrations"
<pre><code class="language-">
</strong>Debug Mode (Power Users):<strong>
</code></pre>yaml
Debug Mode:
  location: Settings â†’ Advanced â†’ Debug Mode
  default: disabled</p>
<p>when_enabled:
    per_asset_details:
      shows:
        <li>Full enrichment reasoning chain</li>
        <li>Raw provider responses</li>
        <li>Confidence score calculations</li>
        <li>Why each tag was suggested (or not)</li>
        <li>Classification decision tree</li>
        <li>Processing time per enrichment type</li>
        <li>Token counts and costs per call</li>
        <li>Cache hit/miss details</li></p>
<p>example_debug_view:
      asset: "team_meeting.mp4"
      enrichment_log:
        <li>timestamp: "10:23:45"</li>
          action: "Started processing"
          details: "Asset queued at Tier 2"</p>
<li>timestamp: "10:23:46"</li>
          action: "Audio extraction"
          details: "Extracted 45:23 audio track, 48kHz, stereo"
          duration_ms: 1230
<li>timestamp: "10:23:48"</li>
          action: "Transcription started"
          provider: "openai_whisper"
          model: "whisper-1"
          audio_duration: "45:23"
          estimated_cost: "$0.27"
<li>timestamp: "10:25:12"</li>
          action: "Transcription complete"
          duration_ms: 84000
          actual_cost: "$0.27"
          word_count: 8456
          confidence: 0.94
          languages_detected: ["en"]
<li>timestamp: "10:25:13"</li>
          action: "Speaker diarization"
          details: "Detected 4 speakers"
          speaker_breakdown:
            speaker_1: "32% talk time, matched to 'Sarah Chen' (confidence: 0.91)"
            speaker_2: "28% talk time, matched to 'John Smith' (confidence: 0.87)"
            speaker_3: "25% talk time, unknown"
            speaker_4: "15% talk time, unknown"
<li>timestamp: "10:25:45"</li>
          action: "Action item extraction"
          provider: "anthropic_claude"
          model: "claude-3-sonnet"
          prompt_tokens: 8500
          completion_tokens: 450
          cost: "$0.12"
          items_extracted: 5
          reasoning: |
            Found 5 action items based on:
            <li>Explicit commitments ("I'll send that by...")</li>
            <li>Deadline mentions ("by Friday", "end of week")</li>
            <li>Assignment language ("John, can you...")</li>
            Rejected 3 potential items due to low confidence:
            <li>"think about the budget" (too vague, 0.34 confidence)</li>
            <li>"maybe we should..." (non-committal, 0.28 confidence)</li>
            <li>"someone should look into" (no assignee, 0.41 confidence)</li>
<li>timestamp: "10:26:02"</li>
          action: "Tag generation"
          tags_considered: 34
          tags_applied: 12
          reasoning:
            applied:
              <li>"meeting" (0.98) - detected meeting structure</li>
              <li>"product-planning" (0.91) - topic analysis</li>
              <li>"q2-roadmap" (0.87) - explicit mention</li>
            rejected:
              <li>"interview" (0.23) - meeting, not interview</li>
              <li>"presentation" (0.34) - discussion, not presentation</li>
<li>timestamp: "10:26:15"</li>
          action: "Life Steward data prepared"
          classification_suggestion:
            domain: "Business: Acme Inc"
            scope: "Product"
            project: "Q2 Roadmap"
            confidence: 0.89
            reasoning: "Topic matches active project, participants are project members"
<p>export_options:
      <li>"Export debug log as JSON"</li>
      <li>"Export debug log as text"</li>
      <li>"Copy to clipboard"</li></p>
<p>performance_impact:
    note: "Debug mode increases storage slightly (logs retained longer)"
    log_retention: "7 days with debug mode, 24 hours without"
<pre><code class="language-">
<hr></p>
<h3>Post-MVP: Advanced Admin Dashboard (Self-Hosted)</h3>
</code></pre>yaml
Full Admin Dashboard:
  description: "Comprehensive monitoring for self-hosted installations"
  audience: "System administrators, power users"
<p>system_health:
    cpu_usage: "23% (4 workers)"
    memory_usage: "4.2GB / 16GB"
    disk_usage: "234GB enrichment data"
    gpu_status: "NVIDIA RTX 3080 - 45% utilization"</p>
<p>worker_status:
    workers:
      <li>id: "worker-1"</li>
        status: "processing"
        current_job: "video_transcription"
        uptime: "4 days 12 hours"
        jobs_completed: 1234
      <li>id: "worker-2"</li>
        status: "idle"
        last_job: "2 minutes ago"
      # ... etc</p>
<p>queue_analytics:
    queue_depth_over_time: [chart]
    processing_rate_over_time: [chart]
    average_wait_time_by_tier: [chart]
    bottleneck_analysis:
      current_bottleneck: "Tier 2 transcription"
      recommendation: "Consider adding GPU worker for local Whisper"</p>
<p>provider_analytics:
    requests_by_provider: [chart]
    latency_by_provider: [chart]
    error_rate_by_provider: [chart]
    cost_by_provider: [chart]</p>
<p>quality_analytics:
    confidence_distribution: [histogram]
    user_correction_rate_over_time: [chart]
    accuracy_by_enrichment_type: [table]
    model_performance_comparison: [chart]</p>
<p>cost_analytics:
    daily_spend: [chart]
    cost_by_content_type: [pie chart]
    cost_per_enrichment_type: [breakdown]
    projected_monthly: [forecast]
    optimization_recommendations:
      <li>"Video processing is 72% of cost - consider local models"</li>
      <li>"High cache miss rate on embeddings - increase cache size"</li></p>
<p>error_analytics:
    error_rate_over_time: [chart]
    errors_by_type: [breakdown]
    recent_errors: [log view]
    error_patterns: "Rate limit errors spike at 9am - consider spreading load"</p>
<p>capacity_planning:
    current_throughput: "45 assets/hour"
    queue_growth_rate: "+12 assets/hour"
    time_to_queue_overflow: "Never (processing faster than intake)"
    recommendations:
      <li>"Current capacity sufficient for workload"</li>
      <li>"Consider scaling if intake exceeds 60 assets/hour"</li></p>
<p>configuration:
    worker_settings: [editable]
    provider_api_keys: [editable, masked]
    queue_settings: [editable]
    cache_settings: [editable]
    log_retention: [editable]</p>
<p>maintenance:
    clear_cache: [button]
    restart_workers: [button]
    reprocess_failed_jobs: [button]
    export_diagnostics: [button]
    database_stats: [view]
<pre><code class="language-">
<h3>User Control &amp; Customization</h3></p>
</strong>Enrichment Profiles:<strong>
</code></pre>yaml
Preset Profiles:
  minimal:
    <li>Basic metadata only</li>
    <li>No AI processing</li>
    <li>No external providers</li>
    <li>Free</li>
<p>standard:
    <li>Descriptions and summaries</li>
    <li>Tag suggestions</li>
    <li>Transcription for audio</li>
    <li>Cost: ~$0.05/asset average</li></p>
<p>comprehensive:
    <li>All enrichment types</li>
    <li>Deep analysis</li>
    <li>Cross-asset intelligence</li>
    <li>Cost: ~$0.20/asset average</li></p>
<p>custom:
    <li>User selects specific enrichments</li>
    <li>Per-asset-type settings</li>
    <li>Provider preferences</li>
<pre><code class="language-">
</strong>Per-Asset-Type Settings:<strong>
</code></pre>yaml
User Preferences:
  images:
    enable_description: true
    enable_ocr: true
    enable_face_detection: false  # privacy choice
    enable_object_detection: true
    auto_tag_threshold: 0.8</p>
<p>audio:
    enable_transcription: true
    enable_speaker_diarization: true
    enable_music_detection: true
    transcription_mode: "clean"  # vs "verbatim"</p>
<p>video:
    enable_scene_detection: true
    extract_audio: true
    thumbnail_count: 5
    skip_if_longer_than: 60  # minutes</p>
<p>documents:
    enable_summary: true
    enable_action_extraction: true
    enable_financial_extraction: true
    pii_handling: "redact"  # vs "flag" vs "ignore"
<pre><code class="language-">
</strong>Tag Integration with Onelist Taxonomy:<strong>
</code></pre>yaml
Philosophy:
  Tags generated by Asset Enrichment integrate with Onelist's broader
  tagging ecosystem, including agent-managed curated taxonomies.</p>
<p>Integration Points:
  tag_suggestion:
    <li>Agent suggests tags based on content analysis</li>
    <li>Suggestions filtered through user's existing taxonomy</li>
    <li>Prefers existing tags over creating new ones</li>
    <li>Respects tag hierarchies (suggests "golden retriever" â†’ also links to "dog" parent)</li></p>
<p>taxonomy_awareness:
    <li>Reads from user's curated tag taxonomies</li>
    <li>Librarian agent manages tag organization</li>
    <li>Enrichment agent suggests, taxonomy agents organize</li>
    <li>No separate "enrichment tags" - all tags are Onelist tags</li></p>
<p>learning_from_taxonomy:
    <li>If user has tag "Max" under "Pets" â†’ learn to apply to dog photos</li>
    <li>If user merges "meeting" and "meetings" â†’ respect going forward</li>
    <li>If user deletes a tag â†’ stop suggesting it</li></p>
<p>cross_agent_coordination:
    <li>Asset Enrichment: "This looks like a golden retriever" (suggests tag)</li>
    <li>Librarian: "User's taxonomy has 'Max' for this specific dog" (refines)</li>
    <li>Life Steward: "File under Personal > Pets > Max" (organizes)</li></p>
<p>See Also:
  <li>future_roadmap_core_taxonomy.md for taxonomy system design</li>
  <li>Librarian agent for tag curation and management</li>
<pre><code class="language-">
</strong>Bulk Operations:<strong>
</code></pre>yaml
Bulk Actions:
  re_enrich_all:
    <li>Scope: all assets, or filtered subset</li>
    <li>Options: specific enrichment types, newer model only</li>
    <li>Scheduling: immediate, overnight, weekend</li></p>
<p>delete_enrichments:
    <li>Remove all AI-generated data</li>
    <li>Keep only original asset</li>
    <li>Useful for privacy reset</li></p>
<p>export_enrichments:
    <li>Download all enrichment data</li>
    <li>Formats: JSON, CSV</li>
    <li>Include or exclude specific types</li></p>
<p>apply_new_settings:
    <li>Retroactively apply preference changes</li>
    <li>Example: "Now enable face detection for all photos"</li>
<pre><code class="language-">
</strong>Settings as Onelist Entries:<strong>
</code></pre>yaml
Settings Storage:
  concept: Enrichment settings stored as a Onelist entry
  benefits:
    <li>Backup: Settings backed up with regular Onelist backups</li>
    <li>Sync: Settings sync across devices</li>
    <li>Version history: Can restore previous settings</li>
    <li>Portability: Export/import by exporting the entry</li>
    <li>Sharing: Can share settings configurations</li></p>
<p>implementation:
    entry_type: "system_config"
    title: "Asset Enrichment Agent Settings"
    content: YAML/JSON of all settings
    metadata:
      config_type: "asset_enrichment_agent"
      version: "1.0"
      last_modified: timestamp</p>
<p>export_import:
    export:
      <li>Navigate to Settings â†’ Export Configuration</li>
      <li>Downloads as JSON file</li>
      <li>Or: Export the settings entry directly</li></p>
<p>import:
      <li>Navigate to Settings â†’ Import Configuration</li>
      <li>Upload JSON file</li>
      <li>Or: Create entry with config content</li>
      <li>Preview changes before applying</li>
      <li>Option to merge or replace</li></p>
<p>sharing:
    <li>"Here's my enrichment config" â†’ share entry</li>
    <li>Community configurations possible</li>
    <li>Privacy: Sensitive settings (API keys) never exported</li>
<pre><code class="language-">
<hr></p>
<h3>Post-MVP: Custom Enrichment Recipes</h3>
</code></pre>yaml
Enrichment Recipes:
  description: "User-defined enrichment workflows for specific content types"
<p>concept:
    <li>Users create custom "recipes" that define exactly what enrichments to run</li>
    <li>Recipes can be triggered by content type, tags, folders, or rules</li>
    <li>More granular than per-asset-type settings</li></p>
<p>example_recipes:
    work_screenshots:
      name: "Work Screenshots"
      trigger:
        conditions:
          <li>content_type: "image"</li>
          <li>detected_as: "screenshot"</li>
          <li>tagged_with: "work" OR source_folder: "Work Screenshots"</li>
      enrichments:
        <li>ocr: enabled</li>
        <li>tag_generation: enabled</li>
        <li>description: disabled  # don't need narrative descriptions</li>
        <li>object_detection: disabled</li>
        <li>face_detection: disabled</li>
      auto_actions:
        <li>add_tag: "work-screenshot"</li>
        <li>file_to: "Business / Reference"</li></p>
<p>family_photos:
      name: "Family Photos"
      trigger:
        conditions:
          <li>content_type: "image"</li>
          <li>face_detected: ["family_member"]  # requires face recognition</li>
      enrichments:
        <li>description: enabled</li>
        <li>face_recognition: enabled</li>
        <li>object_detection: enabled</li>
        <li>ocr: disabled</li>
        <li>location_lookup: enabled</li>
      auto_actions:
        <li>add_tag: "family"</li>
        <li>suggest_event_grouping: true</li></p>
<p>meeting_recordings:
      name: "Meeting Recordings"
      trigger:
        conditions:
          <li>content_type: "audio" OR "video"</li>
          <li>duration: "> 5 minutes"</li>
          <li>calendar_match: "meeting"  # correlates with calendar</li>
      enrichments:
        <li>transcription: enabled</li>
        <li>speaker_diarization: enabled</li>
        <li>action_item_extraction: enabled</li>
        <li>decision_extraction: enabled</li>
        <li>summary: enabled</li>
        <li>scene_detection: disabled  # for video</li>
      auto_actions:
        <li>extract_action_items_to_tasks: true</li>
        <li>link_to_calendar_event: true</li>
        <li>file_to_project: "infer_from_content"</li></p>
<p>receipts:
      name: "Receipts & Invoices"
      trigger:
        conditions:
          <li>content_type: "image" OR "pdf"</li>
          <li>detected_as: "receipt" OR "invoice"</li>
      enrichments:
        <li>ocr: enabled</li>
        <li>financial_extraction: enabled</li>
        <li>description: disabled</li>
        <li>tag_generation: minimal</li>
      auto_actions:
        <li>extract_amount: true</li>
        <li>extract_vendor: true</li>
        <li>extract_date: true</li>
        <li>add_tag: "receipt"</li>
        <li>file_to: "Personal / Finance"</li></p>
<p>recipe_builder_ui:
    <li>Visual builder for creating recipes</li>
    <li>Condition builder (AND/OR logic)</li>
    <li>Enrichment checkboxes</li>
    <li>Auto-action configuration</li>
    <li>Test recipe against sample assets</li>
    <li>Enable/disable recipes</li></p>
<p>recipe_priority:
    <li>Multiple recipes can match same asset</li>
    <li>Priority order determines which wins</li>
    <li>Or: Merge mode - combine enrichments from all matching recipes</li></p>
<p>recipe_sharing:
    <li>Export recipes as JSON</li>
    <li>Import shared recipes</li>
    <li>Community recipe library (future)</li></p>
<p>recipe_storage:
    <li>Stored as Onelist entries (like settings)</li>
    <li>Entry type: "enrichment_recipe"</li>
    <li>Version controlled</li>
    <li>Can be tagged and organized</li>
<pre><code class="language-">
<h3>Accessibility</h3></p>
</strong>Philosophy: Accessibility enrichments are prioritized - they have universal value.<strong>
</strong>Priority Tier for Accessibility:<strong>
</code></pre>yaml
Accessibility as Tier 1 Priority:
  concept: Accessibility enrichments run before other enrichments
  reason: They benefit everyone, not just users with disabilities
<p>always_enabled:
    <li>Alt text for images (can't be disabled)</li>
    <li>Transcripts for audio (can't be disabled)</li>
    <li>Captions for video (can't be disabled)</li>
    <li>Document structure extraction (can't be disabled)</li></p>
<p>processing_order:
    1. Accessibility enrichments (alt text, transcripts, captions)
    2. Core enrichments (descriptions, tags, summaries)
    3. Deep enrichments (analysis, cross-asset intelligence)</p>
<p>rationale:
    <li>Transcripts make audio searchable for everyone</li>
    <li>Alt text helps everyone understand images quickly</li>
    <li>Captions let anyone watch video without sound</li>
    <li>Structure helps everyone navigate documents</li>
<pre><code class="language-">
</strong>MVP Accessibility Features:<strong>
</code></pre>yaml
Images:
  alt_text:
    priority: Tier 1 (always runs first)
    output: Concise description suitable for screen readers
    format: "A golden retriever jumping to catch a red frisbee in a sunny park"
    length: 125 characters max (screen reader friendly)
    always_enabled: true</p>
<p>long_description:
    priority: Tier 1
    trigger: Complex images (charts, diagrams, infographics, dense scenes)
    output: Detailed description for full understanding
    format: Paragraph-length explanation
    example: |
      "Bar chart showing quarterly revenue from Q1-Q4 2025.
       Q1: $2.1M, Q2: $2.4M, Q3: $2.8M, Q4: $3.2M.
       Overall trend shows 52% growth year-over-year.
       Y-axis ranges from $0 to $4M in $1M increments."</p>
<p>color_information:
    priority: Tier 2
    output: Color context for color-blind users
    example: "Red warning banner at top of page"
    example: "Green checkmark indicating success"</p>
<p>Audio:
  transcript:
    priority: Tier 1 (always runs first)
    output: Full text transcript
    always_enabled: true</p>
<p>speaker_labels:
    priority: Tier 1
    output: Who said what
    format: "Sarah: [speech]\nJohn: [speech]"</p>
<p>sound_descriptions:
    priority: Tier 1
    output: Non-speech audio context
    examples:
      <li>"[laughter]"</li>
      <li>"[applause]"</li>
      <li>"[music playing]"</li>
      <li>"[phone ringing]"</li>
      <li>"[dog barking in background]"</li>
      <li>"[long pause]"</li>
      <li>"[crosstalk]"</li></p>
<p>timestamps:
    priority: Tier 1
    output: Navigation markers
    format: Paragraph-level timestamps for jumping to sections</p>
<p>Video:
  captions:
    priority: Tier 1 (always runs first)
    output: SRT/VTT caption files
    always_enabled: true
    includes:
      <li>Speech transcription</li>
      <li>Speaker labels</li>
      <li>Sound descriptions</li>
      <li>Timing synchronization</li></p>
<p>audio_descriptions:
    priority: Tier 2
    output: Description of visual elements for blind users
    format: Inserted during natural pauses
    example: "[Sarah walks to whiteboard and draws a flowchart]"
    note: More complex, may require manual review</p>
<p>chapter_markers:
    priority: Tier 1
    output: Navigation points for jumping through video
    format: Timestamp + description</p>
<p>visual_transcript:
    priority: Tier 1
    output: Transcript with visual context notes
    format: |
      "00:23 Sarah (standing at whiteboard): Let's look at the architecture...
       00:45 [Sarah draws three boxes connected by arrows]
       00:52 John (seated, raising hand): What about the database layer?"</p>
<p>Documents:
  heading_structure:
    priority: Tier 1 (always runs first)
    output: Preserved H1/H2/H3 hierarchy
    always_enabled: true
    enables: Screen reader navigation</p>
<p>reading_order:
    priority: Tier 1
    output: Correct sequence for multi-column, complex layouts
    important_for: PDFs, scanned documents</p>
<p>table_summaries:
    priority: Tier 1
    output: Plain language summary of table data
    example: "Table showing 5 employees with columns for name, department, and start date"</p>
<p>image_alt_in_docs:
    priority: Tier 1
    output: Alt text for images embedded in documents</p>
<p>form_field_descriptions:
    priority: Tier 2
    output: Labels and purposes for form fields
<pre><code class="language-">
</strong>Universal Benefits of Accessibility Enrichments:<strong>
</code></pre>yaml
Why Everyone Benefits:
  transcripts:
    <li>Search all audio content by text</li>
    <li>Skim meetings without listening</li>
    <li>Quote specific passages</li>
    <li>Works in quiet environments</li></p>
<p>captions:
    <li>Watch video without sound (public transit, sleeping baby)</li>
    <li>Better comprehension (accents, technical terms)</li>
    <li>Searchable video content</li></p>
<p>alt_text:
    <li>Quick image understanding without viewing</li>
    <li>Search images by description</li>
    <li>Context when images don't load</li></p>
<p>structure:
    <li>Jump to specific sections</li>
    <li>Better search results</li>
    <li>Faster navigation</li>
<pre><code class="language-">
<hr></p>
<h3>Post-MVP: Accessibility Mode</h3>
</code></pre>yaml
Accessibility Mode:
  description: "Enhanced focus on making all content fully accessible"
  location: Settings â†’ Accessibility
<p>when_enabled:
    prioritization:
      <li>Accessibility enrichments get highest priority</li>
      <li>Other enrichments only run after accessibility complete</li>
      <li>Budget allocation favors accessibility enrichments</li></p>
<p>enhanced_features:
      audio_descriptions_for_all_video:
        <li>Generate audio descriptions for all videos</li>
        <li>Not just when natural pauses exist</li>
        <li>May require more processing time</li></p>
<p>simplified_summaries:
        <li>Plain language summaries at lower reading level</li>
        <li>Key points in simple terms</li>
        <li>Avoid jargon and complex sentences</li></p>
<p>enhanced_image_descriptions:
        <li>More detailed alt text</li>
        <li>Include spatial relationships ("on the left", "in the background")</li>
        <li>Include emotional context ("smiling", "frustrated expression")</li></p>
<p>cognitive_support:
        <li>Content warnings for potentially distressing content</li>
        <li>Reading time estimates</li>
        <li>Complexity indicators</li></p>
<p>export_options:
      <li>Export all transcripts as accessible documents</li>
      <li>Export captions in multiple formats</li>
      <li>Generate accessible PDF versions of content</li></p>
<p>accessibility_dashboard:
    shows:
      <li>Percentage of content with accessibility enrichments</li>
      <li>Content missing alt text / transcripts / captions</li>
      <li>Accessibility coverage by content type</li></p>
<p>actions:
      <li>"Generate missing alt text for all images"</li>
      <li>"Generate missing transcripts for all audio"</li>
      <li>"Review auto-generated accessibility content"</li></p>
<p>compliance_helpers:
    wcag_alignment:
      <li>Notes on WCAG 2.1 compliance</li>
      <li>Recommendations for improvement</li>
      <li>Export accessibility audit report</li></p>
<p>note: "This is for personal use assistance, not legal compliance certification"
<pre><code class="language-">
</strong>Accessibility Integration with Screen Readers:<strong>
</code></pre>yaml
Screen Reader Optimization:
  all_enrichment_outputs:
    <li>Semantic HTML structure</li>
    <li>ARIA labels where appropriate</li>
    <li>Logical reading order</li>
    <li>Skip navigation links</li></p>
<p>enrichment_ui:
    <li>All controls keyboard accessible</li>
    <li>Focus management</li>
    <li>Announcements for async operations</li>
    <li>High contrast mode support</li></p>
<p>content_presentation:
    <li>Alt text displayed on hover/focus</li>
    <li>Transcript expandable/collapsible</li>
    <li>Caption toggle controls</li>
    <li>Accessible media player controls</li>
<pre><code class="language-">
<h3>Offline &amp; Degraded Mode</h3></p>
</strong>MVP: Queue Until Online<strong>
</code></pre>yaml
Philosophy:
  MVP focuses on online experience. Offline assets queue for processing
  when connectivity returns. Local AI options planned for post-MVP.
<p>When Offline:
  available_locally:
    <li>File type detection</li>
    <li>Basic metadata extraction (EXIF, file info)</li>
    <li>Hash calculation (for dedup)</li>
    <li>Thumbnail generation</li>
    <li>Previously cached enrichment results</li>
    <li>Viewing existing enrichments</li></p>
<p>queued_for_online:
    <li>All AI-powered enrichments</li>
    <li>External provider calls</li>
    <li>Vector embeddings</li>
    <li>Cross-asset analysis</li>
    <li>External data lookups</li></p>
<p>user_experience:
    <li>Assets added while offline show "Pending enrichment"</li>
    <li>Badge indicates items in queue</li>
    <li>Queue count visible in status bar</li>
    <li>No enrichment failures - just deferred</li></p>
<p>When Back Online:
  automatic_processing:
    <li>Queued items begin processing</li>
    <li>Priority: oldest first (FIFO) unless user-initiated</li>
    <li>User-initiated enrichments jump queue</li>
    <li>Background processing, non-blocking</li></p>
<p>notifications:
    <li>"You're back online. Processing X queued items."</li>
    <li>"Enrichment complete for items added while offline."</li></p>
<p>conflict_handling:
    <li>If asset was modified while offline, re-analyze</li>
    <li>If settings changed while offline, apply new settings</li>
    <li>Update Life Steward with new enrichment data</li>
<pre><code class="language-">
</strong>Flight Mode (Manual Offline):<strong>
</code></pre>yaml
Flight Mode:
  location: Quick settings menu + Settings â†’ Privacy
  icon: âœˆï¸ Airplane icon</p>
<p>purpose:
    <li>Manually pause all external API calls</li>
    <li>Use when: airplane, limited data, privacy concern, cost control</li>
    <li>Different from Panic Button (which is emergency stop)</li></p>
<p>when_enabled:
    <li>All external enrichment calls paused</li>
    <li>Queue builds up (same as offline)</li>
    <li>Local-only enrichments continue (metadata, thumbnails)</li>
    <li>Clear indicator in UI: "Flight Mode - External processing paused"</li>
    <li>Existing enrichments still viewable</li></p>
<p>user_actions:
    enable:
      <li>"Enable Flight Mode"</li>
      <li>Optional: "Enable for X hours" (auto-disable)</li>
      <li>Optional: "Enable until I turn off"</li></p>
<p>disable:
      <li>"Disable Flight Mode"</li>
      <li>Shows: "X items queued for processing"</li>
      <li>Begins processing queue</li></p>
<p>vs_panic_button:
    flight_mode:
      <li>Planned, intentional pause</li>
      <li>Queues new items normally</li>
      <li>Easy on/off toggle</li>
      <li>No urgency</li></p>
<p>panic_button:
      <li>Emergency stop</li>
      <li>Cancels in-flight requests</li>
      <li>Requires confirmation to resume</li>
      <li>For "oh no" moments</li>
<pre><code class="language-">
<hr></p>
<h3>Post-MVP: Local AI Processing</h3>
</code></pre>yaml
Local AI Options:
  description: "Run AI models locally for privacy, offline use, or cost savings"
  status: Post-MVP exploration
<p>potential_local_models:
    transcription:
      model: "Whisper.cpp / faster-whisper"
      quality: "Near cloud quality for English"
      requirements: "4GB+ RAM, better with GPU"
      use_case: "Offline transcription, privacy-sensitive audio"</p>
<p>image_understanding:
      model: "CLIP / BLIP-2 / LLaVA"
      quality: "Good for basic descriptions, less detailed than GPT-4V"
      requirements: "8GB+ RAM, GPU recommended"
      use_case: "Basic image descriptions, tagging"</p>
<p>text_analysis:
      model: "Ollama (Llama, Mistral, etc.)"
      quality: "Varies by model, generally good"
      requirements: "8GB+ RAM minimum, 16GB+ for larger models"
      use_case: "Summaries, action item extraction, analysis"</p>
<p>embeddings:
      model: "sentence-transformers / all-MiniLM"
      quality: "Good for semantic search"
      requirements: "2GB+ RAM"
      use_case: "Local vector search, similarity detection"</p>
<p>configuration:
    settings_location: Settings â†’ Advanced â†’ Local AI</p>
<p>options:
      local_ai_enabled: false (default)</p>
<p>when_to_use_local:
        options:
          <li>"Never (cloud only)"</li>
          <li>"When offline only"</li>
          <li>"Always prefer local"</li>
          <li>"Let me choose per enrichment type"</li>
        default: "When offline only"</p>
<p>auto_fallback:
        description: "Automatically use local when cloud unavailable"
        default: true (when local AI enabled)</p>
<p>quality_vs_cost:
        description: "Use local for cheaper processing even when online"
        default: false
        note: "Reduces quality but saves money"</p>
<p>per_enrichment_settings:
    transcription:
      options: ["Cloud only", "Local only", "Cloud preferred", "Local preferred"]
    image_description:
      options: same
    text_analysis:
      options: same
    embeddings:
      options: same</p>
<p>resource_management:
    gpu_usage:
      options: ["Don't use GPU", "Use GPU when available", "GPU only"]
      default: "Use GPU when available"</p>
<p>memory_limit:
      description: "Max RAM for local AI"
      default: "Auto (50% of available)"
      configurable: true</p>
<p>concurrent_local_jobs:
      description: "How many local AI jobs to run at once"
      default: 1
      range: 1-4</p>
<p>quality_comparison:
    show_user:
      <li>Side-by-side quality comparison (local vs cloud)</li>
      <li>Let user decide acceptable quality threshold</li>
      <li>"Local transcription is 95% as accurate as cloud for English"</li></p>
<p>installation:
    models_not_bundled: true
    download_on_demand:
      <li>User enables local AI</li>
      <li>System shows required models and sizes</li>
      <li>User confirms download (may be several GB)</li>
      <li>Download and setup automated</li></p>
<p>disk_usage:
      whisper_model: "~1.5GB"
      llava_model: "~4GB"
      ollama_base: "~4-8GB per model"
      embeddings_model: "~500MB"</p>
<p>offline_auto_activation:
    description: "Automatically switch to local models when offline"
    setting: "Auto-activate local AI when offline"
    default: true (when local AI is installed)</p>
<p>behavior:
      when_offline_detected:
        <li>Check if local models installed</li>
        <li>If yes: Process with local models</li>
        <li>If no: Queue for online processing</li>
        <li>Indicator: "Using local AI (offline mode)"</li></p>
<p>when_back_online:
        option_1: "Continue using local (cheaper)"
        option_2: "Switch back to cloud (better quality)"
        setting: "When back online, prefer: [Cloud / Local / Ask me]"
<pre><code class="language-">
<h3>Versioning &amp; Model Updates</h3></p>
</strong>Enrichment Versioning:<strong>
</code></pre>yaml
Version Tracking:
  per_enrichment:
    enrichment_id: "uuid"
    enrichment_type: "image_description"
    model_version: "gpt-4-vision-2024-01"
    agent_version: "1.2.3"
    created_at: "2026-01-28T10:30:00Z"
    confidence: 0.89
    is_current: true
    previous_version_id: "uuid" (if re-enriched)
<p>re_enrichment_triggers:
    <li>Major model version upgrade (e.g., GPT-4 â†’ GPT-5)</li>
    <li>User requests re-enrichment</li>
    <li>Quality improvement detected in new model</li>
    <li>Bug fix affecting this enrichment type</li>
    <li>User changes enrichment settings</li>
<pre><code class="language-">
</strong>Proactive Model Upgrade Notifications:<strong>
</code></pre>yaml
When Better Model Available:
  detection:
    <li>We track major model releases</li>
    <li>We benchmark new models against sample content</li>
    <li>We identify significant quality improvements</li></p>
<p>notification:
    message: |
      "A new AI model (GPT-5) is available that produces significantly
       better image descriptions. Your library has 1,234 images that
       could benefit from re-enrichment."</p>
<p>details_shown:
      <li>Quality improvement estimate ("~25% more detailed descriptions")</li>
      <li>Example comparison (before/after on sample from their library)</li>
      <li>Estimated cost to re-enrich ("~$24.68 using your API key")</li>
      <li>Estimated time ("~2 hours in background")</li></p>
<p>options:
      <li>"Re-enrich all images" (shows cost)</li>
      <li>"Re-enrich selected items"</li>
      <li>"Show me more examples first"</li>
      <li>"Remind me later"</li>
      <li>"Don't notify me about this model"</li></p>
<p>user_controls:
    settings:
      notify_about_model_upgrades: true (default)
      auto_re_enrich: false (never auto-spend user's API credits)
      notify_frequency: "major_upgrades_only" | "all_improvements"</p>
<p>note: "Re-enrichment uses your API keys - you control the cost"
<pre><code class="language-">
</strong>Old Enrichment Version Retention:<strong>
</code></pre>yaml
Retention Policy:
  when_re_enriched:
    <li>New enrichment becomes "current"</li>
    <li>Old enrichment marked as "previous_version"</li>
    <li>Old version retained for comparison/rollback</li></p>
<p>retention_period:
    default: 90 days after re-enrichment
    maximum: 1 year after re-enrichment
    user_configurable: true</p>
<p>settings:
    location: Settings â†’ Storage â†’ Enrichment History
    options:
      <li>"Keep old versions for: 30 / 60 / 90 / 180 / 365 days"</li>
      <li>"Delete old versions immediately after re-enrichment"</li>
      <li>"Keep forever" (not recommended - storage implications)</li>
    default: 90 days</p>
<p>what_happens_at_expiration:
    <li>Old enrichment version permanently deleted</li>
    <li>Current version unaffected</li>
    <li>Deletion logged for audit</li>
    <li>No user notification (expected behavior)</li></p>
<p>Implications of Deleting Old Enrichments:
  cannot_do_after_deletion:
    <li>Compare old vs new to verify improvement</li>
      "Was the new description actually better for this image?"</p>
<li>Revert to previous version if new is worse</li>
      "The old model understood my technical diagrams better"
<li>See how understanding evolved over time</li>
      "What did the AI think this was before?"
<li>Fall back if new model has bugs/issues</li>
      "New model hallucinates on this type of content"
<li>Audit trail for compliance (if needed)</li>
      "What was the original extraction on this date?"
<p>recommendation:
    <li>90 days is enough for most users to notice issues</li>
    <li>Power users may want longer for important content</li>
    <li>Storage cost is minimal for text enrichments</li>
    <li>Consider longer retention for expensive enrichments (video analysis)</li></p>
<p>user_guidance:
    message_in_settings: |
      "Old enrichment versions are kept for comparison and rollback
       after re-enrichment. After the retention period, only the
       current version is kept.</p>
<p>Keeping old versions uses additional storage but allows you to:
       â€¢ Compare old vs new results
       â€¢ Revert if the new version is worse for specific content
       â€¢ See how AI understanding has improved over time</p>
<p>90 days is recommended for most users."
<pre><code class="language-">
</strong>Migration Strategy:<strong>
</code></pre>yaml
Model Upgrade Process:
  1. New model released by provider
  2. We benchmark against diverse content samples
  3. If significant improvement detected:
     <li>Update our model version catalog</li>
     <li>Prepare comparison examples</li>
     <li>Queue notifications for users</li></p>
<p>4. User receives notification:
     <li>"GPT-5 is available with improved image understanding"</li>
     <li>Shows before/after examples from their library</li>
     <li>Shows estimated cost (their API keys)</li></p>
<p>5. User chooses:
     <li>"Re-enrich all" â†’ bulk job queued</li>
     <li>"Re-enrich selected" â†’ picker interface</li>
     <li>"Skip" â†’ no action, can revisit later</li></p>
<p>6. During re-enrichment:
     <li>Old version preserved (retention policy applies)</li>
     <li>New version generated</li>
     <li>Comparison available in debug mode</li>
     <li>Can abort mid-process</li></p>
<p>7. After re-enrichment:
     <li>New versions become current</li>
     <li>Old versions age out per retention policy</li>
     <li>Quality metrics updated</li>
<pre><code class="language-">
</strong>Rollback Capability:<strong>
</code></pre>yaml
Rollback (While Old Version Exists):
  per_asset:
    action: "Revert to previous enrichment"
    available_when: Previous version still within retention period
    effect:
      <li>Swaps current and previous</li>
      <li>Previous becomes current again</li>
      <li>"New" version moves to previous (restarts retention clock)</li></p>
<p>bulk_rollback:
    action: "Revert all re-enriched items from [date]"
    use_case: New model has systematic issues
    available_when: Within retention period
    confirmation_required: true</p>
<p>after_retention_expires:
    <li>Rollback not possible</li>
    <li>Only option is re-enrich again (costs API credits)</li>
    <li>Warning shown when retention approaching for recently rolled-back items</li>
<pre><code class="language-">
<h3>Legal &amp; Compliance</h3></p>
</strong>Philosophy: Inform and disclose, user takes responsibility for their content.<strong>
</strong>Illegal Content Policy:<strong>
</code></pre>yaml
Content We Refuse to Process:
  clearly_illegal:
    <li>Child Sexual Abuse Material (CSAM)</li>
    <li>Content depicting child exploitation</li>
    <li>Any content illegal to possess</li>
<p>detection:
    method: Industry-standard detection (PhotoDNA, similar)
    limitation: "AI detection is not perfect"</p>
<p>action_when_detected:
    <li>Processing immediately halted</li>
    <li>Content quarantined</li>
    <li>User notified</li>
    <li>May be reported per legal requirements</li></p>
<p>everything_else:
    policy: "Inform, don't block"
    <li>User's content, user's responsibility</li>
    <li>We detect and warn, we don't police</li></p>
<p>User Acknowledgment (Part of Agent Activation):
  disclosure: |
    "ILLEGAL CONTENT DETECTION LIMITATIONS</p>
<p>This system attempts to detect clearly illegal content (such as CSAM)
     and will refuse to process it. However:</p>
<p>â€¢ AI detection is NOT perfect
     â€¢ Some illegal content may not be detected
     â€¢ Some legal content may be incorrectly flagged
     â€¢ Detection capabilities vary by content type</p>
<p>YOU are responsible for ensuring the content you upload is legal
     in your jurisdiction. By enabling this agent, you acknowledge:</p>
<p>â€¢ You will not upload content that is illegal to possess
     â€¢ You understand AI detection has limitations
     â€¢ You assume all legal risk for the content you process
     â€¢ We may report detected illegal content as required by law"</p>
<p>acknowledgment_required: true
  checkbox: "I understand and accept these terms"
<pre><code class="language-">
</strong>AI Accuracy Disclaimers:<strong>
</code></pre>yaml
Disclaimers Displayed:
  on_all_enrichments:
    footer: "AI-generated â€¢ May contain errors"</p>
<p>on_specific_types:
    transcription:
      disclaimer: "AI-generated transcription. May contain errors, especially with accents, technical terms, or background noise. Verify critical information."</p>
<p>image_description:
      disclaimer: "AI-generated description. May misidentify objects, people, or context. Not suitable for legal or medical use."</p>
<p>action_items:
      disclaimer: "AI-extracted action items. Review for accuracy. Some items may be missed or incorrectly identified."</p>
<p>financial_extraction:
      disclaimer: "AI-extracted financial data. Always verify amounts and dates against original documents."</p>
<p>medical_content:
      disclaimer: "AI-generated analysis of medical content. NOT medical advice. Consult healthcare professionals for medical decisions."</p>
<p>legal_content:
      disclaimer: "AI-generated analysis of legal content. NOT legal advice. Consult qualified attorneys for legal matters."</p>
<p>user_settings:
    show_disclaimers: true (default)
    option: "Hide disclaimers on enrichments" (for experienced users)
    note: "Disclaimers still apply even if hidden"</p>
<p>Global Disclaimer (in Terms of Service):
  text: |
    "AI-GENERATED CONTENT DISCLAIMER</p>
<p>All enrichments, descriptions, transcriptions, summaries, extractions,
     and analyses generated by this system are produced by artificial
     intelligence and may contain errors, omissions, or inaccuracies.</p>
<p>This system is NOT a substitute for professional advice including
     but not limited to legal, medical, financial, or tax advice.</p>
<p>You are responsible for verifying the accuracy of all AI-generated
     content before relying on it for any purpose.</p>
<p>[Company] makes no warranties about the accuracy, completeness,
     or reliability of AI-generated content."
<pre><code class="language-">
</strong>Content Moderation (Non-Illegal):<strong>
</code></pre>yaml
Philosophy: Inform, don't block</p>
<p>Moderation Detection:
  types:
    <li>NSFW/adult content</li>
    <li>Violence/gore</li>
    <li>Hate speech</li>
    <li>Self-harm content</li>
    <li>Disturbing imagery</li></p>
<p>action: Detect and tag, but process if user chooses
  never: Block legal content from processing</p>
<p>User Controls:
  content_warnings:
    enabled: true (default)
    shows: "This image may contain [violence/nudity/etc]"
    user_can_disable: true</p>
<p>thumbnail_blurring:
    enabled: true (default)
    blurs: Potentially disturbing thumbnails until clicked
    user_can_disable: true</p>
<p>processing_warnings:
    for_sensitive_content:
      message: "This content appears to contain [X]. Process anyway?"
      options: ["Process", "Skip", "Always process this type"]</p>
<p>Note: "Your content, your choice. We inform, you decide."
<pre><code class="language-">
</strong>Copyright Detection:<strong>
</code></pre>yaml
Copyright Awareness:
  detection:
    <li>Music fingerprinting â†’ known tracks</li>
    <li>Image reverse search â†’ stock photos</li>
    <li>Text matching â†’ published works</li></p>
<p>handling:
    <li>Informational only (NOT legal advice)</li>
    <li>Note potential copyright: "This may be copyrighted material"</li>
    <li>Does NOT block processing</li>
    <li>Does NOT report to rights holders</li>
    <li>Link to licensing info if known</li></p>
<p>disclaimer:
    "Copyright detection is informational only and may be inaccurate.
     You are responsible for ensuring you have rights to the content
     you upload and process."</p>
<p>user_use_cases:
    legitimate:
      <li>"I own this music, organizing my library"</li>
      <li>"Fair use for research/education"</li>
      <li>"Licensed content I purchased"</li>
      <li>"My own copyrighted works"</li>
<pre><code class="language-">
</strong>Audit Trail (MVP - Basic):<strong>
</code></pre>yaml
Basic Logging:
  what_we_log:
    <li>Enrichment requests (type, timestamp)</li>
    <li>Provider used</li>
    <li>Success/failure</li>
    <li>User actions (accept, reject, modify)</li></p>
<p>retention: 90 days (default)</p>
<p>user_access:
    <li>View processing history</li>
    <li>Export logs</li>
    <li>Delete logs (with confirmation)</li></p>
<p>not_logged:
    <li>Full content of assets (just references)</li>
    <li>Full provider responses (summarized only)</li>
    <li>User behavior patterns (no analytics)</li>
<pre><code class="language-">
<hr></p>
<h3>Post-MVP: Compliance Mode for Regulated Industries</h3>
</code></pre>yaml
Compliance Mode:
  description: "Enhanced audit trails and controls for healthcare, legal, finance, etc."
  location: Settings â†’ Advanced â†’ Compliance Mode
  status: Post-MVP
<p>target_users:
    <li>Healthcare providers (HIPAA)</li>
    <li>Legal professionals (attorney-client privilege)</li>
    <li>Financial services (SEC, FINRA)</li>
    <li>Government contractors</li>
    <li>Anyone with regulatory requirements</li></p>
<p>when_enabled:
    enhanced_audit_trail:
      logs_include:
        <li>Full processing chain with timestamps</li>
        <li>Exact model versions used</li>
        <li>Hash of content processed</li>
        <li>All provider interactions</li>
        <li>User identity verification</li>
        <li>Access logs (who viewed what, when)</li>
        <li>Modification history</li>
        <li>Deletion records with certificates</li></p>
<p>retention:
        minimum: 7 years (configurable)
        maximum: Unlimited
        tamper_proof: Append-only log with integrity verification</p>
<p>export_formats:
        <li>JSON (machine readable)</li>
        <li>PDF (human readable audit report)</li>
        <li>CSV (spreadsheet compatible)</li>
        <li>Signed/certified exports available</li></p>
<p>access_controls:
      <li>Role-based access to enrichments</li>
      <li>View vs edit permissions</li>
      <li>Audit log access restrictions</li>
      <li>Multi-factor authentication required</li></p>
<p>data_handling:
      provider_restrictions:
        <li>Limit to HIPAA-compliant providers</li>
        <li>Limit to SOC2-certified providers</li>
        <li>Option: Local processing only</li></p>
<p>encryption:
        <li>Enhanced encryption at rest</li>
        <li>Encryption in transit (always, but emphasized)</li>
        <li>User-controlled encryption keys option</li></p>
<p>data_residency:
        <li>Specify geographic restrictions</li>
        <li>"Data must stay in US/EU/specific region"</li>
        <li>Provider selection respects residency</li></p>
<p>deletion_controls:
      <li>Deletion requires manager approval</li>
      <li>Deletion certificates generated</li>
      <li>Legal hold capability (prevent deletion)</li>
      <li>Scheduled retention-based deletion</li></p>
<p>reporting:
      compliance_dashboard:
        <li>Processing volume by type</li>
        <li>Provider usage breakdown</li>
        <li>Access patterns</li>
        <li>Anomaly detection</li>
        <li>Compliance score/checklist</li></p>
<p>automated_reports:
        <li>Monthly compliance summary</li>
        <li>Annual audit package</li>
        <li>On-demand compliance export</li></p>
<p>disclaimers:
    important: |
      "Compliance Mode provides enhanced logging and controls to support
       regulatory compliance, but does NOT guarantee compliance with any
       specific regulation (HIPAA, SOC2, GDPR, etc.).</p>
<p>You are responsible for ensuring your use of this system meets
       your regulatory obligations. Consult with compliance professionals
       and legal counsel regarding your specific requirements."</p>
<p>pricing_note:
    "Compliance mode may require additional storage for extended
     audit logs. Self-hosted users manage their own storage."
</code>``</p>
<h2>Open Questions</h2>
<p>1. </strong>Representation vs Metadata<strong>: When should extracted data be a representation vs metadata?
   <li>Proposal: User-visible content (transcripts, summaries) â†’ Representation</li>
   <li>Machine data (embeddings, scores, coordinates) â†’ Metadata</li></p>
<p>2. </strong>Tag Creation<strong>: Should the agent create new tags or only suggest existing ones?
   <li>Proposal: Suggest both, but require confirmation for new tags</li></p>
<p>3. </strong>Re-enrichment Triggers<strong>: When should we re-process assets?
   <li>Model version upgrades</li>
   <li>User request</li>
   <li>Failed enrichments</li>
   <li>New enrichment types added</li></p>
<p>4. </strong>Privacy Boundaries**: What data leaves the system?
   <li>User-configurable per provider</li>
   <li>Local processing option for sensitive content</li></p>
<hr>
<h2>Success Metrics</h2>
<li>Time from upload to fully enriched</li>
<li>Tag suggestion acceptance rate</li>
<li>Search result relevance improvement</li>
<li>User engagement with enriched content</li>
<li>Cost per asset enriched</li>
<li>Duplicate detection rate</li>
<hr>
<h2>Notes & Discussion</h2>
<em>This section will be updated as we discuss and refine the plan.</em>
<hr>
<h2>Related Documents</h2>
<li><a href="./future_roadmap_life_steward_agent.md">Life Steward Agent</a> - Primary consumer of enrichment data</li>
<li><a href="./asset_storage_plan.md">Asset Storage Plan</a></li>
<li><a href="./future_roadmap_core_taxonomy.md">Core Taxonomy System</a></li>
<li><a href="./ai_memory_evolution.md">AI Memory Evolution</a></li>
<li><a href="./phase_3_implementation_plan.md">Phase 3 Implementation</a></li>
<li><a href="./future_roadmap_consume_later.md">Consume Later</a> - Media consumption tracking</li>
<li><a href="./future_roadmap_limitless.md">Limitless Integration</a> - Life recording source</li>
</ul>
  </article>
</body>
</html>
